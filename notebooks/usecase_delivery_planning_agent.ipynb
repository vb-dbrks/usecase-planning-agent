{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9c28db-6a5d-4183-9497-abcdcd76338b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Usecase Delivery Planning Agent\n",
    "\n",
    "This notebook implements a RAG-based delivery planning agent using DSPy that helps users generate comprehensive project plans for data migration and delivery projects. The agent leverages indexed documents and follows a structured approach to gather requirements and generate actionable project plans.\n",
    "\n",
    "## Features:\n",
    "- **Intelligent Question Generation**: Automatically generates relevant questions based on project context\n",
    "- **Document Retrieval**: Uses vector search to find relevant information from indexed documents\n",
    "- **Structured Planning**: Generates comprehensive project plans with timelines, resources, and milestones\n",
    "- **Risk Assessment**: Identifies potential risks and mitigation strategies\n",
    "- **DSPy Optimization**: Uses DSPy for prompt optimization and performance improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fdb174-2dec-4da1-b66b-924272136c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5aca31a-d76c-40ef-b3e1-61c876a7b078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dspy-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (3.0.3)\n",
      "Requirement already satisfied: databricks-vectorsearch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (0.59)\n",
      "Requirement already satisfied: pydantic in /databricks/python3/lib/python3.12/site-packages (2.8.2)\n",
      "Requirement already satisfied: databricks-agents in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: dspy>=3.0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy-ai) (3.0.3)\n",
      "Requirement already satisfied: mlflow-skinny<4,>=2.11.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-vectorsearch) (3.3.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-vectorsearch) (5.29.5)\n",
      "Requirement already satisfied: requests>=2 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch) (2.32.2)\n",
      "Requirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-vectorsearch) (2.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: databricks-connect in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (17.1.2)\n",
      "Requirement already satisfied: dataclasses-json in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (0.6.7)\n",
      "Requirement already satisfied: databricks-sdk>=0.58.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents) (0.65.0)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (3.1.6)\n",
      "Requirement already satisfied: tenacity>=8.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (0.11.0)\n",
      "Requirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (2.5.0)\n",
      "Requirement already satisfied: whenever==0.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (0.7.3)\n",
      "Requirement already satisfied: boto3>1 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (1.34.69)\n",
      "Requirement already satisfied: botocore in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (1.34.69)\n",
      "Requirement already satisfied: litellm==1.75.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-agents) (1.75.9)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (3.12.15)\n",
      "Requirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (8.1.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (8.4.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (1.107.2)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (1.1.1)\n",
      "Requirement already satisfied: tokenizers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (0.22.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore->databricks-agents) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (2.35.0)\n",
      "Requirement already satisfied: langchain-openai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents) (0.3.33)\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from deprecation>=2->databricks-vectorsearch) (24.1)\n",
      "Requirement already satisfied: backoff>=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /databricks/python3/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (2025.9.1)\n",
      "Requirement already satisfied: orjson>=3.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (3.11.3)\n",
      "Requirement already satisfied: optuna>=3.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (4.5.0)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (0.1.6)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (0.50.1)\n",
      "Requirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (4.10.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (3.1.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (14.1.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (1.26.4)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (3.5.0)\n",
      "Requirement already satisfied: gepa==0.0.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from gepa[dspy]==0.0.7->dspy>=3.0.3->dspy-ai) (0.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from jinja2>=3.0.0->databricks-agents) (3.0.2)\n",
      "Requirement already satisfied: fastapi<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.116.1)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.1.37)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.27.0)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (6.0.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.5.1)\n",
      "Requirement already satisfied: uvicorn<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.35.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2024.6.2)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.65.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.65.0)\n",
      "Requirement already satisfied: grpcio-status>=1.67.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.71.2)\n",
      "Requirement already satisfied: grpcio>=1.67.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.74.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (0.10.9.9)\n",
      "Requirement already satisfied: pyarrow>=10.0.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (15.0.2)\n",
      "Requirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect->databricks-agents) (74.0.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect->databricks-agents) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dataclasses-json->databricks-agents) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from dataclasses-json->databricks-agents) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.20.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from anyio->dspy>=3.0.3->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.47.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.0.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (4.9)\n",
      "Requirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.75.9->databricks-agents) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm==1.75.9->databricks-agents) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (0.27.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (0.10.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.48b0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.16.5)\n",
      "Requirement already satisfied: colorlog in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (2.0.43)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect->databricks-agents) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents) (1.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.3.76)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from tokenizers->litellm==1.75.9->databricks-agents) (0.34.4)\n",
      "Requirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.3.10)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.14.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (5.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (1.1.10)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.4.27)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (1.33)\n",
      "Requirement already satisfied: mdurl~=0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.3->dspy-ai) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.4.8)\n",
      "Requirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (3.2.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.24.0)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Note: These versions are compatible with Databricks Runtime 16.4 LTS\n",
    "%pip install dspy-ai databricks-vectorsearch pydantic databricks-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b18e3bb-fd20-4a90-9c91-dad7842c833a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python to ensure packages are loaded\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009dc2a3-e70a-42b4-be5e-4aea2ebcf217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Vector Search Endpoint: use-case-planning-agent\n",
      "Vector Search Index: vbdemos.usecase_agent.migration_plan_pdfs\n",
      "Documents Table: vbdemos.usecase_agent.usecase_planning_agent_pdf_parsed\n",
      "Vector Index Table: vbdemos.usecase_agent.migration_plan_pdfs\n",
      "Planning Categories: ['Resource', 'Current Process Maturity', 'Customer Background', 'Scope']\n",
      "\n",
      "Note: Vector search index must be in format <catalog>.<schema>.<table>\n",
      "Vector index structure: path (string), text (string), __db_text_vector (array)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"use-case-planning-agent\"  # Your vector search endpoint\n",
    "VECTOR_SEARCH_INDEX_NAME = \"vbdemos.usecase_agent.migration_plan_pdfs\"  # Your existing vector search index\n",
    "CATALOG_NAME = \"vbdemos\"\n",
    "SCHEMA_NAME = \"usecase_agent\"\n",
    "DOCUMENTS_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.usecase_planning_agent_pdf_parsed\"  # Source table\n",
    "VECTOR_INDEX_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.migration_plan_pdfs\"  # Vector search index table\n",
    "\n",
    "# Project planning categories and questions\n",
    "PLANNING_CATEGORIES = {\n",
    "    \"Resource\": [\n",
    "        \"How many team members are there?\",\n",
    "        \"Are they using PS skills?\",\n",
    "        \"Are they using an SI?\",\n",
    "        \"Are the teams sufficiently skilled/trained?\",\n",
    "        \"Are resources shared with other projects?\",\n",
    "        \"Have resources done this work before?\",\n",
    "        \"Is there a product owner?\",\n",
    "        \"Are the DevOps/SecOps/Infra teams under this 'teams' control/purview?\",\n",
    "        \"Program manager?\",\n",
    "        \"Are the BAU teams that will ultimately manage the new system sufficiently trained?\",\n",
    "        \"Has an end-user adoption plan been created?\"\n",
    "    ],\n",
    "    \"Current Process Maturity\": [\n",
    "        \"Do they have a history of delays?\",\n",
    "        \"Do they have change management authority/equivalent?\",\n",
    "        \"Do they have an identified way of working - agile/waterfall?\"\n",
    "    ],\n",
    "    \"Customer Background\": [\n",
    "        \"Does the customer have a specific deadline/reason for that deadline?\",\n",
    "        \"Customer is already using cloud?\",\n",
    "        \"Customer has Databricks elsewhere?\",\n",
    "        \"Customer has security approval for this migration?\",\n",
    "        \"Are there any key connectors that are needed?\",\n",
    "        \"What are the key drivers of the migration?\",\n",
    "        \"Are there any legal compliance or requirements to consider?\"\n",
    "    ],\n",
    "    \"Scope\": [\n",
    "        \"Has a pilot or POC been conducted?\",\n",
    "        \"Does the customer have visibility of all the data and pipelines that need migration?\",\n",
    "        \"Is customer aware of where and who uses the data?\",\n",
    "        \"Is lift and shift or redesign preferred?\",\n",
    "        \"How many pipelines are to be migrated?\",\n",
    "        \"Relative complexity of pipelines?\",\n",
    "        \"Volume of data to be migrated?\",\n",
    "        \"How frequently is the data updated?\",\n",
    "        \"Is there a proposed UC design/infrastructure design?\",\n",
    "        \"Is PII handling included?\",\n",
    "        \"Does the migration include monitoring?\",\n",
    "        \"Does the migration include optimization?\",\n",
    "        \"Will it be run in parallel or phased move over?\",\n",
    "        \"Awareness of business critical pipelines/pipelines that cannot be down?\",\n",
    "        \"Do they have control over how they receive data?\",\n",
    "        \"Are additional data quality checks needing to be implemented?\",\n",
    "        \"Are there any key connectors that need to be migrated?\",\n",
    "        \"What level of testing is required and who will be doing this?\",\n",
    "        \"Are data consumers/systems that use the data known?\",\n",
    "        \"Does customer already have a plan?\",\n",
    "        \"What is the quality of the data?\",\n",
    "        \"Are the data pathways known?\",\n",
    "        \"Has a permissions model been agreed?\",\n",
    "        \"Is there a new data layout/has the UC catalog/schema structure been designed and agreed?\",\n",
    "        \"Is disaster recovery included in this migration?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Vector Search Endpoint: {VECTOR_SEARCH_ENDPOINT_NAME}\")\n",
    "print(f\"Vector Search Index: {VECTOR_SEARCH_INDEX_NAME}\")\n",
    "print(f\"Documents Table: {DOCUMENTS_TABLE}\")\n",
    "print(f\"Vector Index Table: {VECTOR_INDEX_TABLE}\")\n",
    "print(f\"Planning Categories: {list(PLANNING_CATEGORIES.keys())}\")\n",
    "print(\"\\nNote: Vector search index must be in format <catalog>.<schema>.<table>\")\n",
    "print(\"Vector index structure: path (string), text (string), __db_text_vector (array)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2531b9c8-49b4-4ba1-a5b7-e224f1bcfc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. DSPy Setup and Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25eb89f-4818-4d11-98c5-c679ae4b0d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy version: 3.0.3\n",
      "DSPy configured successfully with Databricks model\n",
      "DSPy and Vector Search configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"DeliveryPlanningAgent\").getOrCreate()\n",
    "\n",
    "# Configure DSPy with Databricks model\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() + '/serving-endpoints'\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=\"databricks/databricks-claude-sonnet-4\",\n",
    "    api_key=token,\n",
    "    api_base=url,\n",
    ")\n",
    "\n",
    "# Configure DSPy with Databricks model\n",
    "# Note: MLflow integration is handled automatically in Databricks Runtime 16.4 LTS\n",
    "# The warnings about MLflowCallback can be safely ignored as they don't affect functionality\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(f\"DSPy version: {dspy.__version__}\")\n",
    "print(\"DSPy configured successfully with Databricks model\")\n",
    "\n",
    "# Initialize Vector Search client\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "print(\"DSPy and Vector Search configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d9b1b4-a6ec-4020-87fb-a6482d888a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. DSPy Signatures and Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69613a6e-c9a7-408a-9dbf-5221cdd9d104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy signatures defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# DSPy Signatures for the delivery planning agent\n\nclass QuestionGenerator(dspy.Signature):\n    \"\"\"Generate relevant questions for migrating TO Databricks from existing data/analytics platforms.\"\"\"\n    project_context: str = dspy.InputField(desc=\"Description of the current data/analytics platform and objectives for migrating TO Databricks\")\n    category: str = dspy.InputField(desc=\"Planning category (Resource, Scope, Customer Background, etc.)\")\n    existing_answers: str = dspy.InputField(desc=\"Previously answered questions and responses\")\n    questions: str = dspy.OutputField(desc=\"List of 3-5 most relevant questions for this category specific to migrating TO Databricks\")\n\nclass DocumentRetriever(dspy.Signature):\n    \"\"\"Retrieve relevant documents based on questions and context.\"\"\"\n    question: str = dspy.InputField(desc=\"Specific question to find relevant information for\")\n    project_context: str = dspy.InputField(desc=\"Project context and background\")\n    retrieved_docs: str = dspy.OutputField(desc=\"Relevant document excerpts and information\")\n\nclass AnswerAnalyzer(dspy.Signature):\n    \"\"\"Analyze answers and extract key insights for project planning.\"\"\"\n    question: str = dspy.InputField(desc=\"The question that was asked\")\n    answer: str = dspy.InputField(desc=\"The answer provided by the user\")\n    relevant_docs: str = dspy.InputField(desc=\"Relevant documents retrieved from knowledge base\")\n    insights: str = dspy.OutputField(desc=\"Key insights, risks, and implications for project planning\")\n\nclass ProjectPlanGenerator(dspy.Signature):\n    \"\"\"Generate comprehensive migration plan for moving TO Databricks with structured table outputs.\"\"\"\n    project_context: str = dspy.InputField(desc=\"Overall project context and objectives for migrating TO Databricks\")\n    gathered_insights: str = dspy.InputField(desc=\"All insights gathered from questions and documents about current platform\")\n    timeline_requirements: str = dspy.InputField(desc=\"Timeline constraints and requirements for migrating TO Databricks\")\n    project_plan: str = dspy.OutputField(desc=\"Comprehensive migration plan for moving TO Databricks. Output as structured tables with clear headers for: 1) Migration Timeline, 2) Resource Requirements, 3) Migration Phases, 4) Risk Assessment. Use markdown table format with | separators.\")\n\nclass RiskAssessor(dspy.Signature):\n    \"\"\"Assess risks and provide mitigation strategies.\"\"\"\n    project_plan: str = dspy.InputField(desc=\"The proposed project plan\")\n    project_context: str = dspy.InputField(desc=\"Project context and constraints\")\n    risk_assessment: str = dspy.OutputField(desc=\"Identified risks, their likelihood, impact, and mitigation strategies\")\n\nprint(\"DSPy signatures defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9527bc5-da5f-424f-be13-558953641af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy modules defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# DSPy Modules for the delivery planning agent\n",
    "\n",
    "class QuestionGenerationModule(dspy.Module):\n",
    "    \"\"\"Module to generate relevant questions for each planning category.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(QuestionGenerator)\n",
    "    \n",
    "    def forward(self, project_context: str, category: str, existing_answers: str = \"\"):\n",
    "        return self.generate(\n",
    "            project_context=project_context,\n",
    "            category=category,\n",
    "            existing_answers=existing_answers\n",
    "        )\n",
    "\n",
    "class DocumentRetrievalModule(dspy.Module):\n",
    "    \"\"\"Module to retrieve relevant documents using vector search.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_search_endpoint: str, vector_search_index: str):\n",
    "        super().__init__()\n",
    "        self.vector_search_endpoint = vector_search_endpoint\n",
    "        self.vector_search_index = vector_search_index\n",
    "        self.retrieve = dspy.ChainOfThought(DocumentRetriever)\n",
    "    \n",
    "    def forward(self, question: str, project_context: str):\n",
    "        # Perform vector search\n",
    "        search_results = vsc.get_index(\n",
    "            endpoint_name=self.vector_search_endpoint,\n",
    "            index_name=self.vector_search_index\n",
    "        ).similarity_search(\n",
    "            query_text=question,\n",
    "            columns=[\"path\", \"text\"],  # Updated to match your vector index structure,\n",
    "            num_results=5\n",
    "        )\n",
    "        \n",
    "        # Format retrieved documents\n",
    "        # Handle the case where search_results might be a list or dict\n",
    "        if isinstance(search_results, list):\n",
    "            documents = search_results\n",
    "        else:\n",
    "            documents = search_results.get('result', {}).get('data_array', [])\n",
    "        \n",
    "        retrieved_docs = \"\\n\\n\".join([\n",
    "            f\"Source: {doc.get('path', 'Unknown') if isinstance(doc, dict) else 'Unknown'}\\nContent: {doc.get('text', '') if isinstance(doc, dict) else str(doc)}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        return self.retrieve(\n",
    "            question=question,\n",
    "            project_context=project_context,\n",
    "            retrieved_docs=retrieved_docs\n",
    "        )\n",
    "\n",
    "class AnswerAnalysisModule(dspy.Module):\n",
    "    \"\"\"Module to analyze answers and extract insights.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyze = dspy.ChainOfThought(AnswerAnalyzer)\n",
    "    \n",
    "    def forward(self, question: str, answer: str, relevant_docs: str):\n",
    "        return self.analyze(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            relevant_docs=relevant_docs\n",
    "        )\n",
    "\n",
    "class ProjectPlanGenerationModule(dspy.Module):\n",
    "    \"\"\"Module to generate comprehensive project plans.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_plan = dspy.ChainOfThought(ProjectPlanGenerator)\n",
    "    \n",
    "    def forward(self, project_context: str, gathered_insights: str, timeline_requirements: str):\n",
    "        return self.generate_plan(\n",
    "            project_context=project_context,\n",
    "            gathered_insights=gathered_insights,\n",
    "            timeline_requirements=timeline_requirements\n",
    "        )\n",
    "\n",
    "class RiskAssessmentModule(dspy.Module):\n",
    "    \"\"\"Module to assess risks and provide mitigation strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.assess_risks = dspy.ChainOfThought(RiskAssessor)\n",
    "    \n",
    "    def forward(self, project_plan: str, project_context: str):\n",
    "        return self.assess_risks(\n",
    "            project_plan=project_plan,\n",
    "            project_context=project_context\n",
    "        )\n",
    "\n",
    "print(\"DSPy modules defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdf6c4-560e-465b-9340-a3cc1047dc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Main Delivery Planning Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84881a6-9e82-4f0f-bb88-90750f0da9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery Planning Agent defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class DeliveryPlanningAgent(dspy.Module):\n    \"\"\"Main delivery planning agent that orchestrates the entire planning process.\"\"\"\n    \n    def __init__(self, vector_search_endpoint: str, vector_search_index: str):\n        super().__init__()\n        self.question_generator = QuestionGenerationModule()\n        self.document_retriever = DocumentRetrievalModule(vector_search_endpoint, vector_search_index)\n        self.answer_analyzer = AnswerAnalysisModule()\n        self.plan_generator = ProjectPlanGenerationModule()\n        self.risk_assessor = RiskAssessmentModule()\n        \n        # Store conversation state\n        self.conversation_history = []\n        self.gathered_insights = []\n        self.project_context = \"\"\n    \n    def start_planning_session(self, project_context: str, timeline_requirements: str = \"\"):\n        \"\"\"Start a new planning session.\"\"\"\n        self.project_context = project_context\n        self.conversation_history = []\n        self.gathered_insights = []\n        \n        print(\" Starting Delivery Planning Session\")\n        print(f\" Project Context: {project_context}\")\n        if timeline_requirements:\n            print(f\"\u23f0 Timeline Requirements: {timeline_requirements}\")\n        print(\"\\n\" + \"=\"*50)\n        \n        return self._generate_questions_for_category(\"Resource\")\n    \n    def _generate_questions_for_category(self, category: str):\n        \"\"\"Generate questions for a specific category.\"\"\"\n        existing_answers = self._format_existing_answers()\n        \n        result = self.question_generator(\n            project_context=self.project_context,\n            category=category,\n            existing_answers=existing_answers\n        )\n        \n        print(f\"\\n {category} Questions:\")\n        print(\"-\" * 30)\n        questions = result.questions.split('\\n')\n        for i, question in enumerate(questions, 1):\n            if question.strip():\n                print(f\"{i}. {question.strip()}\")\n        \n        return questions\n    \n    def answer_question(self, question: str, answer: str, category: str = \"\"):\n        \"\"\"Process a user's answer to a question.\"\"\"\n        print(f\"\\n Processing Answer:\")\n        print(f\"Q: {question}\")\n        print(f\"A: {answer}\")\n        \n        # Retrieve relevant documents\n        doc_result = self.document_retriever(question, self.project_context)\n        relevant_docs = doc_result.retrieved_docs\n        \n        # Analyze the answer\n        analysis_result = self.answer_analyzer(question, answer, relevant_docs)\n        insights = analysis_result.insights\n        \n        # Store in conversation history\n        self.conversation_history.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"category\": category,\n            \"insights\": insights,\n            \"relevant_docs\": relevant_docs\n        })\n        \n        self.gathered_insights.append(insights)\n        \n        print(f\"\\n Key Insights:\")\n        print(insights)\n        \n        return insights\n    \n    def generate_project_plan(self, timeline_requirements: str = \"\"):\n        \"\"\"Generate the final project plan based on all gathered information.\"\"\"\n        print(\"\\n Generating Project Plan...\")\n        print(\"=\"*50)\n        \n        # Combine all insights\n        all_insights = \"\\n\\n\".join(self.gathered_insights)\n        \n        # Generate project plan\n        plan_result = self.plan_generator(\n            project_context=self.project_context,\n            gathered_insights=all_insights,\n            timeline_requirements=timeline_requirements\n        )\n        \n        project_plan = plan_result.project_plan\n        \n        print(\" PROJECT PLAN:\")\n        print(\"=\"*50)\n        print(project_plan)\n        \n        # Assess risks\n        risk_result = self.risk_assessor(project_plan, self.project_context)\n        risk_assessment = risk_result.risk_assessment\n        \n        print(\"\\n  RISK ASSESSMENT:\")\n        print(\"=\"*50)\n        print(risk_assessment)\n        \n        return {\n            \"project_plan\": project_plan,\n            \"risk_assessment\": risk_assessment,\n            \"conversation_history\": self.conversation_history\n        }\n    \n    def _format_existing_answers(self):\n        \"\"\"Format existing answers for context.\"\"\"\n        if not self.conversation_history:\n            return \"No previous answers yet.\"\n        \n        formatted = []\n        for entry in self.conversation_history:\n            formatted.append(f\"Q: {entry['question']}\\nA: {entry['answer']}\")\n        \n        return \"\\n\\n\".join(formatted)\n    \n    def get_next_category_questions(self):\n        \"\"\"Get questions for the next planning category.\"\"\"\n        categories = list(PLANNING_CATEGORIES.keys())\n        completed_categories = set(entry.get('category', '') for entry in self.conversation_history)\n        \n        for category in categories:\n            if category not in completed_categories:\n                return self._generate_questions_for_category(category)\n        \n        return None  # All categories completed\n\nprint(\"Delivery Planning Agent defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc77ef9b-5c1a-4e73-b1e3-aabae793267c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Example Usage and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa088574-9375-4c26-8866-41ee75e39d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting Delivery Planning Session\n",
      "\ud83d\udccb Project Context: \n",
      "We are planning a data migration project for a large financial services company. \n",
      "They want to migrate their existing Oracle data warehouse to Databricks on Azure. \n",
      "The migration involves 50+ data pipelines, 2TB of data, and needs to be completed \n",
      "within 6 months. The company has some cloud experience but limited Databricks knowledge.\n",
      "\n",
      "\u23f0 Timeline Requirements: 6 months deadline, must be completed by end of Q2 2024\n",
      "\n",
      "==================================================\n",
      "\n",
      "\ud83d\udcdd Resource Questions:\n",
      "------------------------------\n",
      "1. 1. What is the current size and composition of your technical team, and how many team members have experience with Databricks, Azure, and data migration projects?\n",
      "3. 2. What is the allocated budget for this migration project, including costs for infrastructure, licensing, training, and potential external consulting services?\n",
      "5. 3. Are you planning to engage external consultants or system integrators for this migration, and if so, what specific expertise are you looking to supplement?\n",
      "7. 4. What are your current Azure infrastructure capabilities and what additional cloud resources (compute, storage, networking) will need to be provisioned for the Databricks environment?\n",
      "9. 5. How much time can your team dedicate to this migration project versus their ongoing operational responsibilities, and do you have plans for backfilling critical day-to-day tasks during the migration period?\n",
      "\n",
      "==================================================\n",
      "Example: Answer some questions to see the agent in action\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-52a09ccfc2d073910e77f272ac136c1a\"",
      "text/plain": [
       "Trace(trace_id=tr-52a09ccfc2d073910e77f272ac136c1a)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the delivery planning agent\n",
    "agent = DeliveryPlanningAgent(VECTOR_SEARCH_ENDPOINT_NAME, VECTOR_SEARCH_INDEX_NAME)\n",
    "\n",
    "# Example: Start a planning session\n",
    "project_context = \"\"\"\n",
    "We are planning a data migration project for a large financial services company. \n",
    "They want to migrate their existing Oracle data warehouse to Databricks on Azure. \n",
    "The migration involves 50+ data pipelines, 2TB of data, and needs to be completed \n",
    "within 6 months. The company has some cloud experience but limited Databricks knowledge.\n",
    "\"\"\"\n",
    "\n",
    "timeline_requirements = \"6 months deadline, must be completed by end of Q2 2024\"\n",
    "\n",
    "# Start the planning session\n",
    "questions = agent.start_planning_session(project_context, timeline_requirements)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Example: Answer some questions to see the agent in action\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7725da76-5d37-4c3b-80f6-b33ddc54d378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcdd Processing Resource Questions...\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: How many team members are there?\n",
      "A: We have 8 team members including 2 data engineers, 3 analysts, 2 developers, and 1 project manager\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Team Composition Analysis:**\n",
      "- **Total headcount:** 8 team members provides sufficient capacity for a 6-month, 50+ pipeline migration project\n",
      "- **Technical depth:** 7 technical resources (87.5%) vs 1 management resource indicates hands-on project approach\n",
      "- **Data engineering capacity:** 2 data engineers may be adequate but could become a bottleneck for 50+ pipeline migrations - monitor workload distribution\n",
      "- **Analyst-heavy structure:** 3 analysts (largest group) suggests strong focus on requirements gathering and data analysis, which is critical for Oracle-to-Databricks migration success\n",
      "- **Development support:** 2 developers provide good capacity for custom tooling and integration work\n",
      "- **Project management:** Single PM for 8-person team is standard but will require strong coordination skills given technical complexity\n",
      "\n",
      "**Risk Considerations:**\n",
      "- Data engineers may be overloaded with 25+ pipelines each if work isn't parallelizable\n",
      "- No dedicated QA/testing resources identified - quality assurance responsibilities need clarification\n",
      "- Client's limited Databricks experience may require additional knowledge transfer time from technical team\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Are the teams sufficiently skilled/trained?\n",
      "A: The data engineers have some cloud experience but no Databricks experience. The analysts are familiar with SQL but not Spark. We need training.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Critical Risk Identified:** Major skills gap threatens project timeline and quality. The team lacks Databricks and Spark expertise essential for a successful migration.\n",
      "\n",
      "**Immediate Actions Required:**\n",
      "- Extend project timeline by 2-3 months to accommodate learning curve\n",
      "- Budget for intensive Databricks/Spark training programs and certifications\n",
      "- Engage external Databricks consultants or certified partners for knowledge transfer\n",
      "- Implement a proof-of-concept phase to build practical experience before full migration\n",
      "\n",
      "**Resource Planning Implications:**\n",
      "- Training costs will be significant (estimate 15-20% of project budget)\n",
      "- Productivity will be reduced during initial learning phase\n",
      "- Consider hiring experienced Databricks engineers or contractors\n",
      "- Plan for mentoring relationships and knowledge sharing sessions\n",
      "\n",
      "**Timeline Adjustments:**\n",
      "- Add 4-6 weeks for comprehensive training before migration begins\n",
      "- Build in additional buffer time for learning-related delays\n",
      "- Implement phased approach starting with simpler pipelines\n",
      "- Schedule regular skill assessment checkpoints throughout project\n",
      "\n",
      "**Success Factors:**\n",
      "- Hands-on training with real project data is essential\n",
      "- Focus on practical migration scenarios rather than theoretical knowledge\n",
      "- Establish clear competency benchmarks before proceeding with critical pipelines\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Is there a product owner?\n",
      "A: Yes, we have a product owner from the business side who understands the data requirements\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Key Insights:**\n",
      "- **Positive Risk Mitigation**: Having a dedicated product owner with data requirements knowledge significantly reduces the risk of scope creep and misaligned deliverables during the Oracle to Databricks migration\n",
      "- **Business Alignment**: Business-side product owner ensures that technical migration decisions align with actual business needs and data usage patterns\n",
      "- **Documentation Gap**: Critical project role information is missing from formal documentation, which could create confusion about responsibilities and decision-making authority\n",
      "- **Requirements Clarity**: Product owner's understanding of data requirements will be essential for validating the 50+ pipeline migrations and ensuring data quality post-migration\n",
      "\n",
      "**Planning Implications:**\n",
      "- Leverage product owner expertise early in pipeline prioritization and validation phases\n",
      "- Ensure product owner is involved in UAT planning and data validation processes\n",
      "- Document the product owner role and responsibilities formally to avoid future ambiguity\n",
      "- Consider the product owner as a key stakeholder for change management and user adoption activities\n",
      "- Plan regular checkpoints with product owner to validate migration progress against business requirements\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Are the DevOps/SecOps/Infra teams under this 'teams' control/purview?\n",
      "A: No, they are separate teams with their own priorities and reporting lines\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Critical Project Risks:**\n",
      "- **Dependency Management**: Project success is heavily dependent on external teams with competing priorities, creating potential bottlenecks and delays\n",
      "- **Resource Contention**: Without direct control, securing necessary DevOps/SecOps/Infra resources may be challenging, especially given the 6-month timeline constraint\n",
      "- **Coordination Complexity**: Multiple independent teams require extensive coordination mechanisms and clear escalation paths\n",
      "\n",
      "**Key Planning Implications:**\n",
      "- **Early Engagement Required**: Immediate outreach to these teams is essential to secure commitments and understand their capacity constraints\n",
      "- **Formal Agreements Needed**: Establish SLAs, resource commitments, and priority agreements through proper governance channels\n",
      "- **Extended Timeline Risk**: The 6-month timeline may be optimistic given dependency on external teams with separate priorities\n",
      "- **Escalation Strategy**: Develop clear escalation paths to senior leadership for resolving resource conflicts and priority disputes\n",
      "\n",
      "**Recommended Actions:**\n",
      "- Map all required touchpoints with DevOps/SecOps/Infra teams early in project planning\n",
      "- Establish formal working agreements and communication protocols\n",
      "- Build buffer time into project timeline to account for external team dependencies\n",
      "- Consider dedicated liaison roles or embedded resources if possible\n",
      "\n",
      "==================================================\n",
      "Moving to next category...\n",
      "==================================================\n",
      "\n",
      "\ud83d\udcdd Current Process Maturity Questions:\n",
      "------------------------------\n",
      "1. 1. What is your current data governance framework and how are data quality, lineage, and metadata management handled in your existing Oracle environment?\n",
      "3. 2. How do you currently manage code deployments, testing, and releases for your data pipelines - do you have established CI/CD practices or is it more manual?\n",
      "5. 3. What is your current approach to monitoring, alerting, and incident response for data pipeline failures or performance issues?\n",
      "7. 4. How does your organization typically handle change management for major technology transitions - do you have established processes for training, communication, and user adoption?\n",
      "9. 5. What are your current data security and compliance processes, and how do you ensure regulatory requirements (financial services regulations) are met in your data operations?\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-e5b229b679bad3de0c59cae6c3847d91\", \"tr-a57d6ee6e8d00836cb4e00dabb9f1a0c\", \"tr-4eb66219ce387d5758bbe30d23307900\", \"tr-72450886f817bfe1a4683f852657fce3\", \"tr-16ee1fc86ec6acd0f8d27e51da444efb\", \"tr-9574af1ec473d0bfb94f3ec9ec6336f9\", \"tr-3cfb9ab92cd28a7f965054b472cbf75a\", \"tr-57d34f06921df3898e0f9206ba98a031\", \"tr-b887991231222dc41531c5d8aa62e4ed\"]",
      "text/plain": [
       "[Trace(trace_id=tr-e5b229b679bad3de0c59cae6c3847d91), Trace(trace_id=tr-a57d6ee6e8d00836cb4e00dabb9f1a0c), Trace(trace_id=tr-4eb66219ce387d5758bbe30d23307900), Trace(trace_id=tr-72450886f817bfe1a4683f852657fce3), Trace(trace_id=tr-16ee1fc86ec6acd0f8d27e51da444efb), Trace(trace_id=tr-9574af1ec473d0bfb94f3ec9ec6336f9), Trace(trace_id=tr-3cfb9ab92cd28a7f965054b472cbf75a), Trace(trace_id=tr-57d34f06921df3898e0f9206ba98a031), Trace(trace_id=tr-b887991231222dc41531c5d8aa62e4ed)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Simulate answering questions\n# In a real scenario, these would be user inputs\n\n# Answer Resource questions\nresource_answers = [\n    (\"How many team members are there?\", \"We have 8 team members including 2 data engineers, 3 analysts, 2 developers, and 1 project manager\"),\n    (\"Are the teams sufficiently skilled/trained?\", \"The data engineers have some cloud experience but no Databricks experience. The analysts are familiar with SQL but not Spark. We need training.\"),\n    (\"Is there a product owner?\", \"Yes, we have a product owner from the business side who understands the data requirements\"),\n    (\"Are the DevOps/SecOps/Infra teams under this 'teams' control/purview?\", \"No, they are separate teams with their own priorities and reporting lines\")\n]\n\nprint(\" Processing Resource Questions...\")\nfor question, answer in resource_answers:\n    agent.answer_question(question, answer, \"Resource\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Moving to next category...\")\nprint(\"=\"*50)\n\n# Get next category questions\nnext_questions = agent.get_next_category_questions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e3be5f-433f-4ce9-b5c0-f76b5793eccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcdd Processing Scope Questions...\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: How many pipelines are to be migrated?\n",
      "A: We have approximately 50 data pipelines that need to be migrated\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "Key insights for project planning:\n",
      "- **Scale Impact**: 50 pipelines represent a significant migration workload requiring systematic approach and phased execution\n",
      "- **Resource Planning**: Will need dedicated teams for pipeline analysis, conversion, testing, and validation across all 50 pipelines\n",
      "- **Timeline Considerations**: With 6-month deadline, this averages to ~8-9 pipelines per month, requiring parallel processing and efficient workflow management\n",
      "- **Risk Factors**: High volume increases complexity and potential for cascading failures; need robust testing strategy for each pipeline\n",
      "- **Dependencies**: Pipeline interdependencies must be mapped to determine migration sequence and avoid downstream impacts\n",
      "- **Quality Assurance**: Each pipeline will require individual validation, data reconciliation, and performance testing\n",
      "- **Change Management**: Significant scope affecting multiple business processes and stakeholders across the organization\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Volume of data to be migrated?\n",
      "A: The total data volume is about 2TB across multiple tables and schemas\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Key Planning Insights:**\n",
      "- **Migration Approach**: 2TB allows for both full and incremental migration strategies; recommend phased approach to minimize risk and downtime\n",
      "- **Timeline Impact**: With 6-month deadline, allocate 2-3 months for data migration activities including testing and validation\n",
      "- **Network Requirements**: Ensure adequate bandwidth (minimum 100 Mbps dedicated) for efficient data transfer; consider Azure ExpressRoute for large data movements\n",
      "- **Storage Costs**: Budget for temporary storage during migration - expect 3x storage overhead (source, staging, target)\n",
      "- **Testing Strategy**: 2TB volume requires automated validation tools; plan for 20-30% additional time for data quality verification\n",
      "- **Downtime Planning**: Size allows for weekend migration windows; estimate 24-48 hours for full cutover depending on network speed\n",
      "- **Resource Allocation**: Moderate volume suggests 2-3 data engineers can handle migration workload alongside pipeline conversion\n",
      "- **Risk Mitigation**: Implement checkpoint/restart capabilities for long-running transfers; plan rollback strategy with this data volume in mind\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Is lift and shift or redesign preferred?\n",
      "A: We prefer a hybrid approach - lift and shift for simple pipelines, redesign for complex ones\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Key Planning Insights:**\n",
      "- **Timeline Impact**: Hybrid approach supports the 6-month deadline by prioritizing quick wins through lift and shift while allowing time for strategic redesigns\n",
      "- **Resource Allocation**: Will need to identify and categorize pipelines by complexity early to allocate appropriate resources and expertise\n",
      "- **Risk Management**: Lower initial risk with lift and shift, but requires clear criteria for determining which pipelines qualify as \"simple\" vs \"complex\"\n",
      "- **Skill Development**: Team will need parallel training on Databricks optimization techniques while executing lift and shift migrations\n",
      "- **Technical Debt**: Accept some technical debt initially with lift and shift, but plan for future optimization phases\n",
      "- **Success Metrics**: Need different KPIs for lift and shift (migration speed, functional parity) vs redesign (performance improvement, cost optimization)\n",
      "- **Change Management**: Stakeholders should expect iterative improvements rather than immediate full optimization of all workloads\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Does the migration include monitoring?\n",
      "A: Yes, we need comprehensive monitoring and alerting for the new system\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Key Insights:**\n",
      "- **Multi-layered Monitoring Strategy Required**: The migration will need monitoring at infrastructure (Azure Monitor), platform (Databricks native), data quality (validation tools), and business process levels\n",
      "- **Compliance-Critical**: As a financial services migration, audit logging and regulatory compliance monitoring are mandatory, not optional\n",
      "- **Migration Risk Mitigation**: With 50+ pipelines to migrate in 6 months, real-time monitoring of migration progress, data validation, and error tracking will be essential to meet deadlines\n",
      "- **Performance Baseline Establishment**: Monitoring should be implemented early to establish performance baselines before migration begins\n",
      "- **Cost Management**: Comprehensive monitoring will help track resource usage and costs during the migration to avoid budget overruns\n",
      "\n",
      "**Risks:**\n",
      "- **Monitoring Overhead**: Extensive monitoring could impact system performance if not properly configured\n",
      "- **Alert Fatigue**: Too many alerts without proper prioritization could lead to missing critical issues\n",
      "- **Integration Complexity**: Coordinating monitoring across Oracle (source), migration tools, and Databricks (target) systems adds complexity\n",
      "\n",
      "**Planning Implications:**\n",
      "- Budget allocation needed for monitoring tools and potential third-party solutions\n",
      "- Dedicated resources required for monitoring setup and maintenance during migration\n",
      "- Early implementation of monitoring infrastructure before migration begins\n",
      "- Training requirements for teams on new monitoring tools and dashboards\n",
      "- Establish monitoring runbooks and escalation procedures for the migration period\n",
      "\n",
      "\ud83d\udcac Processing Answer:\n",
      "Q: Will it be run in parallel or phased move over?\n",
      "A: We plan to do a phased migration over 6 months, starting with non-critical pipelines\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "\n",
      "\ud83d\udd0d Key Insights:\n",
      "**Key Project Planning Insights:**\n",
      "\n",
      "**Risk Management:**\n",
      "- Low-risk start with non-critical pipelines provides safety net and learning opportunity\n",
      "- Phased approach reduces blast radius of potential failures\n",
      "- 6-month timeline aligns with industry best practices for this scale of migration\n",
      "\n",
      "**Resource Planning:**\n",
      "- Team will need gradual skill development across phases rather than immediate full expertise\n",
      "- Infrastructure costs can be managed incrementally rather than full upfront investment\n",
      "- Quality assurance efforts can be focused on smaller batches for better outcomes\n",
      "\n",
      "**Success Factors:**\n",
      "- Need to establish clear success criteria for each phase before proceeding\n",
      "- Documentation and pattern establishment in early phases critical for later success\n",
      "- Parallel execution within phases should be planned to optimize timeline while maintaining quality\n",
      "\n",
      "**Potential Risks:**\n",
      "- Dependencies between pipelines may complicate phased approach - need dependency mapping\n",
      "- Extended timeline increases risk of scope creep or changing requirements\n",
      "- Team may face pressure to accelerate timeline - need stakeholder alignment on phased benefits\n",
      "\n",
      "**Recommendations:**\n",
      "- Develop detailed wave planning with specific pipeline groupings\n",
      "- Establish rollback procedures for each phase\n",
      "- Plan for knowledge transfer and documentation between phases\n",
      "- Consider pilot phase success metrics before full commitment\n",
      "\n",
      "==================================================\n",
      "Generating Project Plan...\n",
      "==================================================\n",
      "\n",
      "\ud83d\udcca Generating Project Plan...\n",
      "==================================================\n",
      "\ud83d\udccb PROJECT PLAN:\n",
      "==================================================\n",
      "# Oracle to Databricks Migration Project Plan\n",
      "\n",
      "## Executive Summary\n",
      "**Project Duration**: 6 months (January - June 2024)  \n",
      "**Scope**: Migrate 50+ Oracle data pipelines and 2TB data to Databricks on Azure  \n",
      "**Team Size**: 8 resources (2 Data Engineers, 3 Analysts, 2 Developers, 1 PM)  \n",
      "**Approach**: Phased hybrid migration with intensive skill development\n",
      "\n",
      "## Phase 1: Foundation & Preparation (Weeks 1-6)\n",
      "\n",
      "### Week 1-2: Project Initiation\n",
      "- **Team Training Program Launch**\n",
      "  - Enroll all technical resources in Databricks/Spark certification programs\n",
      "  - Engage external Databricks consultant for 2-week intensive training\n",
      "  - Set up hands-on lab environment with sample Oracle data\n",
      "- **Stakeholder Alignment**\n",
      "  - Formalize agreements with DevOps/SecOps/Infrastructure teams\n",
      "  - Establish SLAs and resource commitments\n",
      "  - Document product owner role and responsibilities\n",
      "- **Environment Setup**\n",
      "  - Provision Azure Databricks workspace\n",
      "  - Set up development, testing, and production environments\n",
      "  - Configure network connectivity and security protocols\n",
      "\n",
      "### Week 3-4: Assessment & Planning\n",
      "- **Pipeline Discovery & Analysis**\n",
      "  - Catalog all 50+ pipelines with complexity scoring\n",
      "  - Map pipeline dependencies and data lineage\n",
      "  - Categorize pipelines for lift-and-shift vs redesign approach\n",
      "- **Infrastructure Planning**\n",
      "  - Size Databricks clusters for migration workload\n",
      "  - Plan data transfer strategy and network bandwidth requirements\n",
      "  - Design monitoring and logging architecture\n",
      "\n",
      "### Week 5-6: Proof of Concept\n",
      "- **Pilot Migration**\n",
      "  - Select 3-5 simple, non-critical pipelines for POC\n",
      "  - Execute end-to-end migration process\n",
      "  - Validate data quality and performance\n",
      "- **Process Refinement**\n",
      "  - Document migration patterns and best practices\n",
      "  - Refine tooling and automation scripts\n",
      "  - Establish quality gates and validation procedures\n",
      "\n",
      "## Phase 2: Wave 1 Migration (Weeks 7-12)\n",
      "\n",
      "### Target: 15 Simple Pipelines (Lift-and-Shift)\n",
      "- **Week 7-8: Batch 1** (5 pipelines)\n",
      "  - Execute migrations with full team support\n",
      "  - Intensive monitoring and validation\n",
      "  - Document lessons learned\n",
      "- **Week 9-10: Batch 2** (5 pipelines)\n",
      "  - Parallel execution by sub-teams\n",
      "  - Implement automated validation tools\n",
      "  - Performance baseline establishment\n",
      "- **Week 11-12: Batch 3** (5 pipelines)\n",
      "  - Optimize migration processes\n",
      "  - Begin user acceptance testing\n",
      "  - Prepare for production cutover\n",
      "\n",
      "### Key Activities:\n",
      "- Daily standups and progress tracking\n",
      "- Continuous data validation and reconciliation\n",
      "- Performance monitoring and optimization\n",
      "- Stakeholder communication and change management\n",
      "\n",
      "## Phase 3: Wave 2 Migration (Weeks 13-18)\n",
      "\n",
      "### Target: 20 Medium Complexity Pipelines\n",
      "- **Week 13-14: Analysis & Design**\n",
      "  - Detailed technical analysis of medium complexity pipelines\n",
      "  - Design optimized Databricks implementations\n",
      "  - Plan parallel execution streams\n",
      "- **Week 15-16: Migration Execution**\n",
      "  - Execute 10 pipelines in parallel streams\n",
      "  - Implement enhanced monitoring and alerting\n",
      "  - Conduct thorough testing and validation\n",
      "- **Week 17-18: Completion & Validation**\n",
      "  - Complete remaining 10 pipelines\n",
      "  - End-to-end system testing\n",
      "  - Performance tuning and optimization\n",
      "\n",
      "## Phase 4: Wave 3 Migration (Weeks 19-22)\n",
      "\n",
      "### Target: 15+ Complex Pipelines (Redesign)\n",
      "- **Week 19: Advanced Design**\n",
      "  - Redesign complex pipelines for Databricks optimization\n",
      "  - Implement advanced Spark features and optimizations\n",
      "  - Plan for enhanced data processing capabilities\n",
      "- **Week 20-21: Migration & Testing**\n",
      "  - Execute complex pipeline migrations\n",
      "  - Comprehensive integration testing\n",
      "  - Performance benchmarking against Oracle baseline\n",
      "- **Week 22: Final Validation**\n",
      "  - Complete data validation and reconciliation\n",
      "  - User acceptance testing completion\n",
      "  - Production readiness assessment\n",
      "\n",
      "## Phase 5: Production Cutover (Weeks 23-24)\n",
      "\n",
      "### Week 23: Pre-Production Activities\n",
      "- **Final Preparations**\n",
      "  - Complete all data synchronization\n",
      "  - Finalize cutover procedures and rollback plans\n",
      "  - Conduct dress rehearsal with all stakeholders\n",
      "- **Go-Live Preparation**\n",
      "  - Coordinate with business users and downstream systems\n",
      "  - Prepare monitoring dashboards and alert systems\n",
      "  - Brief support teams on new architecture\n",
      "\n",
      "### Week 24: Production Cutover\n",
      "- **Weekend Cutover Window**\n",
      "  - Execute production migration\n",
      "  - Monitor system performance and data integrity\n",
      "  - Validate all business processes\n",
      "- **Post-Cutover Support**\n",
      "  - 24/7 monitoring for first week\n",
      "  - Immediate issue resolution\n",
      "  - Performance optimization\n",
      "\n",
      "## Phase 6: Stabilization & Optimization (Weeks 25-26)\n",
      "\n",
      "### Week 25-26: Post-Migration Activities\n",
      "- **Performance Optimization**\n",
      "  - Fine-tune cluster configurations\n",
      "  - Optimize query performance\n",
      "  - Implement cost optimization measures\n",
      "- **Knowledge Transfer**\n",
      "  - Train operational support teams\n",
      "  - Document operational procedures\n",
      "  - Establish ongoing maintenance processes\n",
      "- **Project Closure**\n",
      "  - Conduct lessons learned sessions\n",
      "  - Document best practices and patterns\n",
      "  - Transition to business-as-usual operations\n",
      "\n",
      "## Resource Allocation\n",
      "\n",
      "### Core Team Responsibilities:\n",
      "- **Project Manager**: Overall coordination, stakeholder management, risk mitigation\n",
      "- **Data Engineers (2)**: Pipeline migration, technical architecture, performance optimization\n",
      "- **Analysts (3)**: Requirements validation, data quality testing, business process validation\n",
      "- **Developers (2)**: Tooling development, automation scripts, integration work\n",
      "- **Product Owner**: Requirements clarification, business validation, change management\n",
      "\n",
      "### External Resources:\n",
      "- **Databricks Consultant**: 2-week intensive training + ongoing advisory (Weeks 1-2, 5-6)\n",
      "- **DevOps Team**: Infrastructure setup and maintenance (Ongoing)\n",
      "- **Security Team**: Security configuration and compliance validation (Weeks 3-4, 23-24)\n",
      "\n",
      "## Risk Management\n",
      "\n",
      "### High-Priority Risks:\n",
      "1. **Skills Gap**: Mitigated through intensive training and external consulting\n",
      "2. **Timeline Pressure**: Managed through phased approach and parallel execution\n",
      "3. **External Dependencies**: Addressed through early engagement and formal SLAs\n",
      "4. **Data Quality Issues**: Prevented through comprehensive validation and testing\n",
      "\n",
      "### Contingency Plans:\n",
      "- **Timeline Extension**: 2-week buffer built into each phase\n",
      "- **Resource Augmentation**: Pre-approved budget for additional contractors\n",
      "- **Rollback Procedures**: Documented for each migration wave\n",
      "- **Escalation Paths**: Clear procedures for technical and business issues\n",
      "\n",
      "## Success Metrics\n",
      "\n",
      "### Technical KPIs:\n",
      "- Pipeline migration success rate: >95%\n",
      "- Data quality validation: 100% accuracy\n",
      "- Performance improvement: Maintain or improve Oracle baseline\n",
      "- System availability: >99.9% during migration\n",
      "\n",
      "### Business KPIs:\n",
      "- Zero business process disruption\n",
      "- User satisfaction score: >4.0/5.0\n",
      "- Cost optimization: 20% reduction in infrastructure costs\n",
      "- Compliance: 100% regulatory requirement adherence\n",
      "\n",
      "## Budget Considerations\n",
      "\n",
      "### Key Cost Components:\n",
      "- Azure Databricks licensing and compute costs\n",
      "- Training and certification programs\n",
      "- External consulting services\n",
      "- Temporary infrastructure during migration\n",
      "- Monitoring and validation tools\n",
      "\n",
      "### Cost Optimization:\n",
      "- Right-size clusters based on workload analysis\n",
      "- Implement auto-scaling and spot instances where appropriate\n",
      "- Optimize data storage with appropriate compression and partitioning\n",
      "- Monitor and adjust resource allocation throughout project\n",
      "\n",
      "This comprehensive plan addresses all identified risks while maintaining the aggressive 6-month timeline through careful phasing, parallel execution, and proactive risk mitigation strategies.\n",
      "\n",
      "\u26a0\ufe0f  RISK ASSESSMENT:\n",
      "==================================================\n",
      "## HIGH RISK ITEMS\n",
      "\n",
      "### 1. Skills Gap & Learning Curve (Probability: High, Impact: High)\n",
      "**Risk**: Team lacks Databricks/Spark expertise, potentially causing delays, poor implementations, and technical debt.\n",
      "**Mitigation Strategies**:\n",
      "- Extend consultant engagement beyond 2 weeks to 4-6 weeks with hands-on mentoring\n",
      "- Implement pair programming between consultants and internal team\n",
      "- Create mandatory certification requirements before pipeline migration begins\n",
      "- Establish technical review gates with external experts\n",
      "- Build 4-week buffer into Phase 1 for additional training if needed\n",
      "\n",
      "### 2. Aggressive Timeline with Limited Buffer (Probability: High, Impact: High)\n",
      "**Risk**: 6-month timeline leaves minimal room for unexpected issues, especially with 50+ pipelines.\n",
      "**Mitigation Strategies**:\n",
      "- Negotiate 8-month timeline with business stakeholders\n",
      "- Implement parallel execution streams earlier (Phase 2 instead of Phase 3)\n",
      "- Pre-identify 10-15 \"nice-to-have\" pipelines that can be deferred to Phase 2\n",
      "- Establish weekly executive steering committee for rapid decision-making\n",
      "- Prepare detailed rollback procedures for each phase\n",
      "\n",
      "### 3. Financial Services Regulatory Compliance (Probability: Medium, Impact: Critical)\n",
      "**Risk**: Data governance, audit trails, and regulatory requirements may not be properly addressed.\n",
      "**Mitigation Strategies**:\n",
      "- Engage compliance team from Week 1, not just security team\n",
      "- Implement comprehensive audit logging and data lineage tracking\n",
      "- Conduct regulatory impact assessment before Phase 1 completion\n",
      "- Establish data retention and archival procedures matching Oracle environment\n",
      "- Plan for regulatory approval processes that may extend timeline\n",
      "\n",
      "## MEDIUM RISK ITEMS\n",
      "\n",
      "### 4. External Dependencies & Resource Availability (Probability: Medium, Impact: High)\n",
      "**Risk**: DevOps, Security, and Infrastructure teams may not deliver on commitments.\n",
      "**Mitigation Strategies**:\n",
      "- Formalize SLAs with penalty clauses and escalation procedures\n",
      "- Identify backup vendors for critical infrastructure components\n",
      "- Cross-train internal team members on basic DevOps tasks\n",
      "- Establish weekly dependency review meetings with all external teams\n",
      "- Create detailed RACI matrix with named individuals, not just teams\n",
      "\n",
      "### 5. Data Quality & Validation Complexity (Probability: Medium, Impact: High)\n",
      "**Risk**: 2TB data migration with complex financial calculations may introduce data quality issues.\n",
      "**Mitigation Strategies**:\n",
      "- Implement automated data validation tools from Day 1\n",
      "- Create comprehensive test data sets representing edge cases\n",
      "- Establish parallel run period of 2-4 weeks before cutover\n",
      "- Implement real-time data reconciliation dashboards\n",
      "- Plan for manual validation of critical financial calculations\n",
      "\n",
      "### 6. Performance & Scalability Concerns (Probability: Medium, Impact: Medium)\n",
      "**Risk**: Databricks implementation may not meet Oracle performance benchmarks.\n",
      "**Mitigation Strategies**:\n",
      "- Conduct performance baseline testing in Phase 1 POC\n",
      "- Implement comprehensive monitoring from migration start\n",
      "- Plan for cluster optimization and auto-scaling configuration\n",
      "- Establish performance SLAs with rollback triggers\n",
      "- Budget for premium Databricks features if needed for performance\n",
      "\n",
      "## LOW-MEDIUM RISK ITEMS\n",
      "\n",
      "### 7. Change Management & User Adoption (Probability: Medium, Impact: Medium)\n",
      "**Risk**: Business users may resist new system or struggle with changes.\n",
      "**Mitigation Strategies**:\n",
      "- Implement comprehensive user training program starting Phase 2\n",
      "- Create user champions program with early access and feedback\n",
      "- Develop detailed user guides and video tutorials\n",
      "- Plan for extended support period post-migration\n",
      "- Establish user feedback loops and rapid issue resolution process\n",
      "\n",
      "### 8. Cost Overrun Risk (Probability: Medium, Impact: Medium)\n",
      "**Risk**: Azure costs may exceed budget due to inefficient resource usage or extended timeline.\n",
      "**Mitigation Strategies**:\n",
      "- Implement daily cost monitoring and alerting\n",
      "- Right-size clusters based on actual usage patterns, not estimates\n",
      "- Use spot instances for development and testing environments\n",
      "- Establish cost optimization reviews every 2 weeks\n",
      "- Negotiate volume discounts with Microsoft for extended usage\n",
      "\n",
      "## CONTINGENCY PLANNING\n",
      "\n",
      "### Technical Rollback Strategy:\n",
      "- Maintain Oracle environment in read-only mode for 30 days post-cutover\n",
      "- Implement automated rollback procedures for each migration wave\n",
      "- Establish 4-hour rollback window capability for critical issues\n",
      "\n",
      "### Resource Augmentation Plan:\n",
      "- Pre-approved budget for 2 additional Databricks specialists\n",
      "- Identified backup consulting firms for emergency support\n",
      "- Cross-training plan to reduce single points of failure\n",
      "\n",
      "### Timeline Extension Scenarios:\n",
      "- 2-month extension plan if major technical issues arise\n",
      "- Phased go-live option to deliver value incrementally\n",
      "- Minimum viable product (MVP) approach focusing on top 30 critical pipelines\n",
      "\n",
      "This risk assessment emphasizes the need for proactive management, especially around skills development, timeline management, and regulatory compliance in the financial services context.\n",
      "\n",
      "\u2705 Planning session completed!\n",
      "\ud83d\udcca Summary:\n",
      "- Total questions answered: 9\n",
      "- Categories covered: {'Resource', 'Scope'}\n",
      "- Key insights gathered: 9\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-ca0ee8e0cef57603991ced7f4a316f0c\", \"tr-a18c04f71c39c2c4380bd402a539dc71\", \"tr-0d32f8bc0de9bf06bd6239880b8b50f5\", \"tr-fb139b4d07eec24079a2e1122654b346\", \"tr-41af0774ed1afee715bbfb09d79e6cf6\", \"tr-e4ba617f5fbc7d5b15beae4c49a445a9\", \"tr-1089ad519f974e70b0c4a88f7db41536\", \"tr-6961f91e1c50507383d7e6db6a6f2e36\", \"tr-9b13cc7e1ff7b7a348bf4d5e0229c1e8\", \"tr-c76cd14815d762147f6aa8c1219ce0fb\"]",
      "text/plain": [
       "[Trace(trace_id=tr-ca0ee8e0cef57603991ced7f4a316f0c), Trace(trace_id=tr-a18c04f71c39c2c4380bd402a539dc71), Trace(trace_id=tr-0d32f8bc0de9bf06bd6239880b8b50f5), Trace(trace_id=tr-fb139b4d07eec24079a2e1122654b346), Trace(trace_id=tr-41af0774ed1afee715bbfb09d79e6cf6), Trace(trace_id=tr-e4ba617f5fbc7d5b15beae4c49a445a9), Trace(trace_id=tr-1089ad519f974e70b0c4a88f7db41536), Trace(trace_id=tr-6961f91e1c50507383d7e6db6a6f2e36), Trace(trace_id=tr-9b13cc7e1ff7b7a348bf4d5e0229c1e8), Trace(trace_id=tr-c76cd14815d762147f6aa8c1219ce0fb)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer Scope questions\nscope_answers = [\n    (\"How many pipelines are to be migrated?\", \"We have approximately 50 data pipelines that need to be migrated\"),\n    (\"Volume of data to be migrated?\", \"The total data volume is about 2TB across multiple tables and schemas\"),\n    (\"Is lift and shift or redesign preferred?\", \"We prefer a hybrid approach - lift and shift for simple pipelines, redesign for complex ones\"),\n    (\"Does the migration include monitoring?\", \"Yes, we need comprehensive monitoring and alerting for the new system\"),\n    (\"Will it be run in parallel or phased move over?\", \"We plan to do a phased migration over 6 months, starting with non-critical pipelines\")\n]\n\nprint(\" Processing Scope Questions...\")\nfor question, answer in scope_answers:\n    agent.answer_question(question, answer, \"Scope\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Generating Project Plan...\")\nprint(\"=\"*50)\n\n# Generate the final project plan\nfinal_plan = agent.generate_project_plan(timeline_requirements)\n\nprint(\"\\n Planning session completed!\")\nprint(\" Summary:\")\nprint(f\"- Total questions answered: {len(agent.conversation_history)}\")\nprint(f\"- Categories covered: {set(entry['category'] for entry in agent.conversation_history)}\")\nprint(f\"- Key insights gathered: {len(agent.gathered_insights)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752566dd-6d24-4570-a95d-2f281b519fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Interactive Planning Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c43b36-8379-408f-80c3-81c2c208ddb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive planning interface ready!\n",
      "To start an interactive session, uncomment and run: interactive_planning_session()\n"
     ]
    }
   ],
   "source": [
    "def interactive_planning_session():\n    \"\"\"Interactive function for users to run their own planning sessions.\"\"\"\n    \n    print(\" Welcome to the Delivery Planning Agent!\")\n    print(\"=\"*50)\n    \n    # Get project context from user\n    project_context = input(\"Please describe your project context: \")\n    timeline_requirements = input(\"Any specific timeline requirements? (press Enter to skip): \")\n    \n    # Initialize agent\n    agent = DeliveryPlanningAgent(VECTOR_SEARCH_ENDPOINT_NAME, VECTOR_SEARCH_INDEX_NAME)\n    \n    # Start planning session\n    questions = agent.start_planning_session(project_context, timeline_requirements)\n    \n    print(\"\\n Please answer the questions above. Type 'next' to move to the next category, or 'plan' to generate the project plan.\")\n    \n    while True:\n        user_input = input(\"\\nYour response (or 'next'/'plan'): \").strip()\n        \n        if user_input.lower() == 'next':\n            next_questions = agent.get_next_category_questions()\n            if next_questions is None:\n                print(\" All categories completed! Type 'plan' to generate your project plan.\")\n            continue\n        \n        elif user_input.lower() == 'plan':\n            final_plan = agent.generate_project_plan(timeline_requirements)\n            break\n        \n        else:\n            # This is a simplified version - in practice, you'd need to match the response to the current question\n            print(\"Please provide a more specific answer or use 'next'/'plan' commands.\")\n    \n    return final_plan\n\n# Uncomment the line below to run an interactive session\n# interactive_planning_session()\n\nprint(\"Interactive planning interface ready!\")\nprint(\"To start an interactive session, uncomment and run: interactive_planning_session()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0560fc43-1ad8-4ed9-ae38-9902ee9f28e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Welcome to the Delivery Planning Agent!\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Please describe your project context:  Oracle migration for a big pharmaceutical company, with 3 departments, 1.Fiannce 2. Sales 3. HR around 100 reports in each department with around roughly 200 Stored Procedures with ETL logic and 1000 tables need a project plan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Any specific timeline requirements? (press Enter to skip):  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting Delivery Planning Session\n",
      "\ud83d\udccb Project Context: Oracle migration for a big pharmaceutical company, with 3 departments, 1.Fiannce 2. Sales 3. HR around 100 reports in each department with around roughly 200 Stored Procedures with ETL logic and 1000 tables need a project plan\n",
      "\n",
      "==================================================\n",
      "\n",
      "\ud83d\udcdd Resource Questions:\n",
      "------------------------------\n",
      "1. 1. What is the current size and composition of your technical team, and how many team members have Oracle migration experience or database expertise?\n",
      "3. 2. Do you have dedicated database administrators (DBAs) and ETL developers available internally, or will you need to hire external consultants or contractors for this migration?\n",
      "5. 3. What is your preferred project timeline, and are there any critical business periods (like financial year-end, sales cycles, or HR processes) when system downtime must be avoided?\n",
      "7. 4. What is the approved budget range for this migration project, including potential costs for external resources, training, and migration tools?\n",
      "9. 5. Are there any specific Oracle migration tools or platforms you're already licensed for, and do you have relationships with preferred Oracle consulting partners or system integrators?\n",
      "\n",
      "\ud83d\udcdd Please answer the questions above. Type 'next' to move to the next category, or 'plan' to generate the project plan.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Your response (or 'next'/'plan'):  What is the current size and composition of your technical team, and how many team members have Oracle migration experience or database expertise? 10 Data Engineers, no migration expertise to migrate to DAtabricks. Dedcated DBAs are available, within 1 year i.e. by September 2026, 1Million USD budget, no tools for migrations"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide a more specific answer or use 'next'/'plan' commands.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Your response (or 'next'/'plan'):  plan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Generating Project Plan...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 20:45:45 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.12/site-packages/pydantic/main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Union[Choices, StreamingChoices]` but got `Choices` - serialized value may not be as expected\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udccb PROJECT PLAN:\n",
      "==================================================\n",
      "# Oracle Migration Project Plan - Pharmaceutical Company\n",
      "\n",
      "## Project Overview\n",
      "**Objective**: Migrate Oracle database infrastructure for Finance, Sales, and HR departments including 300 reports, 200 stored procedures, and 1000 tables.\n",
      "\n",
      "**Scope**: Complete database migration with minimal business disruption and full functionality preservation.\n",
      "\n",
      "## Phase 1: Assessment & Planning (Weeks 1-8)\n",
      "\n",
      "### 1.1 Current State Analysis\n",
      "- **Database Assessment**\n",
      "  - Inventory all 1000 tables with dependencies mapping\n",
      "  - Catalog 200 stored procedures and ETL logic\n",
      "  - Document 300 reports (100 per department)\n",
      "  - Analyze data volumes and growth patterns\n",
      "  - Identify custom code and third-party integrations\n",
      "\n",
      "- **Business Impact Analysis**\n",
      "  - Map critical business processes to database objects\n",
      "  - Identify peak usage periods and maintenance windows\n",
      "  - Document regulatory compliance requirements (FDA, GxP)\n",
      "  - Assess data retention and archival policies\n",
      "\n",
      "### 1.2 Target Architecture Design\n",
      "- Define target Oracle environment specifications\n",
      "- Design high availability and disaster recovery architecture\n",
      "- Plan security framework and access controls\n",
      "- Establish performance benchmarks and SLAs\n",
      "\n",
      "### 1.3 Migration Strategy\n",
      "- Choose migration approach (big bang vs. phased)\n",
      "- Define rollback procedures\n",
      "- Establish testing environments (DEV, QA, UAT, PROD)\n",
      "- Create detailed migration runbooks\n",
      "\n",
      "**Deliverables**: Assessment report, migration strategy document, project charter\n",
      "**Resources**: 2 Database Architects, 1 Business Analyst, 3 Department SMEs\n",
      "\n",
      "## Phase 2: Environment Setup & Tool Preparation (Weeks 9-16)\n",
      "\n",
      "### 2.1 Infrastructure Preparation\n",
      "- Provision target Oracle environments\n",
      "- Configure network connectivity and security\n",
      "- Set up monitoring and logging systems\n",
      "- Establish backup and recovery procedures\n",
      "\n",
      "### 2.2 Migration Tools Setup\n",
      "- Configure Oracle Data Pump utilities\n",
      "- Set up ETL tools for data transformation\n",
      "- Prepare report migration tools\n",
      "- Establish automated testing frameworks\n",
      "\n",
      "### 2.3 Team Training\n",
      "- Oracle migration best practices training\n",
      "- Tool-specific training sessions\n",
      "- Regulatory compliance training\n",
      "- Emergency response procedures\n",
      "\n",
      "**Deliverables**: Configured environments, trained team, migration tools ready\n",
      "**Resources**: 3 Infrastructure Engineers, 2 Database Administrators, 1 Training Coordinator\n",
      "\n",
      "## Phase 3: Development & Unit Testing (Weeks 17-28)\n",
      "\n",
      "### 3.1 Schema Migration\n",
      "- Migrate table structures and constraints\n",
      "- Convert stored procedures and functions\n",
      "- Migrate views and triggers\n",
      "- Update database links and synonyms\n",
      "\n",
      "### 3.2 Data Migration Development\n",
      "- Develop ETL processes for data transfer\n",
      "- Create data validation scripts\n",
      "- Build incremental update mechanisms\n",
      "- Implement data quality checks\n",
      "\n",
      "### 3.3 Report Migration\n",
      "- **Finance Department** (100 reports)\n",
      "  - Financial statements and regulatory reports\n",
      "  - Budget and forecasting reports\n",
      "  - Cost accounting and profitability analysis\n",
      "- **Sales Department** (100 reports)\n",
      "  - Sales performance dashboards\n",
      "  - Territory and product analysis\n",
      "  - Customer and market reports\n",
      "- **HR Department** (100 reports)\n",
      "  - Employee records and compliance reports\n",
      "  - Payroll and benefits reporting\n",
      "  - Performance and training reports\n",
      "\n",
      "### 3.4 Unit Testing\n",
      "- Test individual stored procedures\n",
      "- Validate data transformation logic\n",
      "- Verify report functionality\n",
      "- Performance testing of critical queries\n",
      "\n",
      "**Deliverables**: Migrated database objects, converted reports, unit test results\n",
      "**Resources**: 4 Database Developers, 3 Report Developers, 2 QA Engineers\n",
      "\n",
      "## Phase 4: Integration & System Testing (Weeks 29-36)\n",
      "\n",
      "### 4.1 Integration Testing\n",
      "- End-to-end process testing\n",
      "- Cross-department data flow validation\n",
      "- Third-party system integration testing\n",
      "- API and interface testing\n",
      "\n",
      "### 4.2 Performance Testing\n",
      "- Load testing with production-like volumes\n",
      "- Stress testing for peak usage scenarios\n",
      "- Optimization of slow-performing queries\n",
      "- Capacity planning validation\n",
      "\n",
      "### 4.3 Security Testing\n",
      "- Access control verification\n",
      "- Data encryption validation\n",
      "- Audit trail functionality testing\n",
      "- Compliance requirement verification\n",
      "\n",
      "**Deliverables**: Integration test results, performance benchmarks, security validation\n",
      "**Resources**: 3 QA Engineers, 2 Performance Specialists, 1 Security Analyst\n",
      "\n",
      "## Phase 5: User Acceptance Testing (Weeks 37-44)\n",
      "\n",
      "### 5.1 UAT Preparation\n",
      "- Prepare test data and scenarios\n",
      "- Train business users on testing procedures\n",
      "- Set up UAT environment\n",
      "- Create defect tracking processes\n",
      "\n",
      "### 5.2 Department-Specific UAT\n",
      "- **Finance UAT**: Financial reporting accuracy, regulatory compliance\n",
      "- **Sales UAT**: Sales data integrity, performance metrics validation\n",
      "- **HR UAT**: Employee data privacy, payroll accuracy\n",
      "\n",
      "### 5.3 UAT Execution & Defect Resolution\n",
      "- Execute business scenarios\n",
      "- Document and resolve defects\n",
      "- Obtain formal sign-off from departments\n",
      "- Conduct final acceptance testing\n",
      "\n",
      "**Deliverables**: UAT results, defect resolution log, business sign-off\n",
      "**Resources**: 6 Business Users (2 per department), 2 QA Engineers, 1 Project Manager\n",
      "\n",
      "## Phase 6: Production Migration (Weeks 45-48)\n",
      "\n",
      "### 6.1 Pre-Migration Activities\n",
      "- Final data synchronization\n",
      "- Communication to all stakeholders\n",
      "- Backup of current production system\n",
      "- Go/no-go decision meeting\n",
      "\n",
      "### 6.2 Migration Execution\n",
      "- Execute migration runbooks\n",
      "- Monitor migration progress\n",
      "- Validate data integrity\n",
      "- Perform smoke testing\n",
      "\n",
      "### 6.3 Post-Migration Activities\n",
      "- System performance monitoring\n",
      "- User support and issue resolution\n",
      "- Documentation updates\n",
      "- Lessons learned session\n",
      "\n",
      "**Deliverables**: Migrated production system, post-migration report\n",
      "**Resources**: 4 Database Administrators, 2 System Engineers, 24/7 support team\n",
      "\n",
      "## Phase 7: Stabilization & Support (Weeks 49-52)\n",
      "\n",
      "### 7.1 Hypercare Support\n",
      "- 24/7 monitoring and support\n",
      "- Rapid issue resolution\n",
      "- Performance optimization\n",
      "- User training and support\n",
      "\n",
      "### 7.2 Knowledge Transfer\n",
      "- Documentation handover to support teams\n",
      "- Training for operational staff\n",
      "- Establishment of ongoing maintenance procedures\n",
      "- Creation of troubleshooting guides\n",
      "\n",
      "**Deliverables**: Stable production system, support documentation, trained support team\n",
      "**Resources**: 2 Support Engineers, 1 Documentation Specialist\n",
      "\n",
      "## Risk Management\n",
      "\n",
      "### High-Risk Items\n",
      "1. **Data Loss/Corruption**: Implement comprehensive backup and validation procedures\n",
      "2. **Extended Downtime**: Plan for phased migration with minimal downtime windows\n",
      "3. **Regulatory Compliance**: Engage compliance team early and maintain audit trails\n",
      "4. **Performance Degradation**: Conduct thorough performance testing and optimization\n",
      "5. **User Resistance**: Implement change management and comprehensive training\n",
      "\n",
      "### Mitigation Strategies\n",
      "- Maintain parallel systems during transition\n",
      "- Implement automated rollback procedures\n",
      "- Establish 24/7 support during critical phases\n",
      "- Regular stakeholder communication and updates\n",
      "- Comprehensive testing at each phase\n",
      "\n",
      "## Resource Requirements\n",
      "\n",
      "### Core Team\n",
      "- 1 Project Manager\n",
      "- 2 Database Architects\n",
      "- 4 Database Developers\n",
      "- 3 Infrastructure Engineers\n",
      "- 3 Report Developers\n",
      "- 4 QA Engineers\n",
      "- 6 Business SMEs (2 per department)\n",
      "- 1 Change Management Specialist\n",
      "\n",
      "### Estimated Budget\n",
      "- Personnel: $2.5M - $3.5M\n",
      "- Infrastructure: $500K - $800K\n",
      "- Tools and Licenses: $200K - $400K\n",
      "- Contingency (15%): $480K - $705K\n",
      "- **Total**: $3.68M - $5.405M\n",
      "\n",
      "## Success Criteria\n",
      "1. 100% data integrity maintained\n",
      "2. All 300 reports functioning correctly\n",
      "3. Performance meets or exceeds current benchmarks\n",
      "4. Zero regulatory compliance issues\n",
      "5. User acceptance rate >95%\n",
      "6. Migration completed within planned timeline\n",
      "7. Post-migration support incidents <5% of baseline\n",
      "\n",
      "## Timeline Summary\n",
      "- **Total Duration**: 52 weeks (12 months)\n",
      "- **Critical Path**: Database migration and testing phases\n",
      "- **Key Milestones**: \n",
      "  - Week 8: Assessment complete\n",
      "  - Week 16: Environments ready\n",
      "  - Week 28: Development complete\n",
      "  - Week 36: System testing complete\n",
      "  - Week 44: UAT complete\n",
      "  - Week 48: Production migration complete\n",
      "  - Week 52: Project closure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 20:46:14 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.12/site-packages/pydantic/main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Union[Choices, StreamingChoices]` but got `Choices` - serialized value may not be as expected\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u26a0\ufe0f  RISK ASSESSMENT:\n",
      "==================================================\n",
      "**CRITICAL RISKS (High Impact, High Probability):**\n",
      "\n",
      "1. **Regulatory Compliance Failure**\n",
      "   - **Risk**: Non-compliance with FDA 21 CFR Part 11, GxP, or other pharmaceutical regulations\n",
      "   - **Impact**: Regulatory sanctions, audit failures, potential product recalls, legal liability\n",
      "   - **Likelihood**: Medium-High\n",
      "   - **Mitigation**: Engage regulatory compliance team from Phase 1, maintain complete audit trails, implement electronic signatures, conduct compliance-specific testing, obtain regulatory pre-approval for migration approach\n",
      "\n",
      "2. **Data Integrity Compromise**\n",
      "   - **Risk**: Data corruption, loss, or inconsistency during migration affecting patient safety data or financial records\n",
      "   - **Impact**: Patient safety risks, financial misstatements, regulatory violations\n",
      "   - **Likelihood**: Medium\n",
      "   - **Mitigation**: Implement checksums and data validation at every step, maintain parallel systems during transition, conduct multiple data integrity tests, establish automated rollback procedures\n",
      "\n",
      "3. **Extended Business Disruption**\n",
      "   - **Risk**: Migration takes longer than planned, causing prolonged system unavailability\n",
      "   - **Impact**: Inability to process critical business operations, revenue loss, compliance reporting delays\n",
      "   - **Likelihood**: High\n",
      "   - **Mitigation**: Implement phased migration approach, maintain legacy systems as backup, establish clear rollback criteria and procedures, plan migrations during low-activity periods\n",
      "\n",
      "**HIGH RISKS (High Impact, Medium Probability):**\n",
      "\n",
      "4. **Performance Degradation**\n",
      "   - **Risk**: New system performs slower than current system, affecting business operations\n",
      "   - **Impact**: Reduced productivity, user dissatisfaction, potential business process delays\n",
      "   - **Likelihood**: Medium\n",
      "   - **Mitigation**: Conduct extensive performance testing with production-like data volumes, optimize queries and indexes, implement performance monitoring, have database tuning specialists on standby\n",
      "\n",
      "5. **Security Breach During Migration**\n",
      "   - **Risk**: Sensitive pharmaceutical data exposed during migration process\n",
      "   - **Impact**: HIPAA violations, intellectual property theft, regulatory penalties, reputation damage\n",
      "   - **Likelihood**: Medium\n",
      "   - **Mitigation**: Implement end-to-end encryption, use secure data transfer protocols, conduct security assessments, limit access to migration team only, implement comprehensive logging\n",
      "\n",
      "6. **Critical Resource Unavailability**\n",
      "   - **Risk**: Key team members become unavailable during critical phases\n",
      "   - **Impact**: Project delays, knowledge gaps, quality issues\n",
      "   - **Likelihood**: Medium\n",
      "   - **Mitigation**: Cross-train team members, maintain detailed documentation, identify backup resources, implement knowledge transfer sessions\n",
      "\n",
      "**MEDIUM RISKS (Medium Impact, Medium Probability):**\n",
      "\n",
      "7. **Third-Party Integration Failures**\n",
      "   - **Risk**: External systems fail to integrate properly with migrated database\n",
      "   - **Impact**: Broken business processes, manual workarounds required\n",
      "   - **Likelihood**: Medium\n",
      "   - **Mitigation**: Early engagement with third-party vendors, comprehensive integration testing, maintain legacy interfaces temporarily\n",
      "\n",
      "8. **User Adoption Resistance**\n",
      "   - **Risk**: Business users resist new system or struggle with changes\n",
      "   - **Impact**: Reduced efficiency, increased support costs, potential workarounds\n",
      "   - **Likelihood**: Medium\n",
      "   - **Mitigation**: Implement comprehensive change management, provide extensive training, establish user champions, maintain user feedback channels\n",
      "\n",
      "**CONTINGENCY PLANNING:**\n",
      "- Maintain $705K contingency budget (15% of maximum estimate)\n",
      "- Establish emergency response team with 24/7 availability during critical phases\n",
      "- Prepare detailed rollback procedures for each migration phase\n",
      "- Maintain parallel systems for minimum 4 weeks post-migration\n",
      "- Create communication plan for stakeholder updates during issues\n",
      "- Establish escalation procedures for regulatory and compliance concerns\n",
      "\n",
      "**MONITORING AND CONTROL:**\n",
      "- Weekly risk assessment reviews during active phases\n",
      "- Real-time monitoring of system performance and data integrity\n",
      "- Regular compliance checkpoints with regulatory team\n",
      "- Continuous stakeholder communication on risk status\n",
      "- Monthly risk register updates with mitigation progress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_plan': '# Oracle Migration Project Plan - Pharmaceutical Company\\n\\n## Project Overview\\n**Objective**: Migrate Oracle database infrastructure for Finance, Sales, and HR departments including 300 reports, 200 stored procedures, and 1000 tables.\\n\\n**Scope**: Complete database migration with minimal business disruption and full functionality preservation.\\n\\n## Phase 1: Assessment & Planning (Weeks 1-8)\\n\\n### 1.1 Current State Analysis\\n- **Database Assessment**\\n  - Inventory all 1000 tables with dependencies mapping\\n  - Catalog 200 stored procedures and ETL logic\\n  - Document 300 reports (100 per department)\\n  - Analyze data volumes and growth patterns\\n  - Identify custom code and third-party integrations\\n\\n- **Business Impact Analysis**\\n  - Map critical business processes to database objects\\n  - Identify peak usage periods and maintenance windows\\n  - Document regulatory compliance requirements (FDA, GxP)\\n  - Assess data retention and archival policies\\n\\n### 1.2 Target Architecture Design\\n- Define target Oracle environment specifications\\n- Design high availability and disaster recovery architecture\\n- Plan security framework and access controls\\n- Establish performance benchmarks and SLAs\\n\\n### 1.3 Migration Strategy\\n- Choose migration approach (big bang vs. phased)\\n- Define rollback procedures\\n- Establish testing environments (DEV, QA, UAT, PROD)\\n- Create detailed migration runbooks\\n\\n**Deliverables**: Assessment report, migration strategy document, project charter\\n**Resources**: 2 Database Architects, 1 Business Analyst, 3 Department SMEs\\n\\n## Phase 2: Environment Setup & Tool Preparation (Weeks 9-16)\\n\\n### 2.1 Infrastructure Preparation\\n- Provision target Oracle environments\\n- Configure network connectivity and security\\n- Set up monitoring and logging systems\\n- Establish backup and recovery procedures\\n\\n### 2.2 Migration Tools Setup\\n- Configure Oracle Data Pump utilities\\n- Set up ETL tools for data transformation\\n- Prepare report migration tools\\n- Establish automated testing frameworks\\n\\n### 2.3 Team Training\\n- Oracle migration best practices training\\n- Tool-specific training sessions\\n- Regulatory compliance training\\n- Emergency response procedures\\n\\n**Deliverables**: Configured environments, trained team, migration tools ready\\n**Resources**: 3 Infrastructure Engineers, 2 Database Administrators, 1 Training Coordinator\\n\\n## Phase 3: Development & Unit Testing (Weeks 17-28)\\n\\n### 3.1 Schema Migration\\n- Migrate table structures and constraints\\n- Convert stored procedures and functions\\n- Migrate views and triggers\\n- Update database links and synonyms\\n\\n### 3.2 Data Migration Development\\n- Develop ETL processes for data transfer\\n- Create data validation scripts\\n- Build incremental update mechanisms\\n- Implement data quality checks\\n\\n### 3.3 Report Migration\\n- **Finance Department** (100 reports)\\n  - Financial statements and regulatory reports\\n  - Budget and forecasting reports\\n  - Cost accounting and profitability analysis\\n- **Sales Department** (100 reports)\\n  - Sales performance dashboards\\n  - Territory and product analysis\\n  - Customer and market reports\\n- **HR Department** (100 reports)\\n  - Employee records and compliance reports\\n  - Payroll and benefits reporting\\n  - Performance and training reports\\n\\n### 3.4 Unit Testing\\n- Test individual stored procedures\\n- Validate data transformation logic\\n- Verify report functionality\\n- Performance testing of critical queries\\n\\n**Deliverables**: Migrated database objects, converted reports, unit test results\\n**Resources**: 4 Database Developers, 3 Report Developers, 2 QA Engineers\\n\\n## Phase 4: Integration & System Testing (Weeks 29-36)\\n\\n### 4.1 Integration Testing\\n- End-to-end process testing\\n- Cross-department data flow validation\\n- Third-party system integration testing\\n- API and interface testing\\n\\n### 4.2 Performance Testing\\n- Load testing with production-like volumes\\n- Stress testing for peak usage scenarios\\n- Optimization of slow-performing queries\\n- Capacity planning validation\\n\\n### 4.3 Security Testing\\n- Access control verification\\n- Data encryption validation\\n- Audit trail functionality testing\\n- Compliance requirement verification\\n\\n**Deliverables**: Integration test results, performance benchmarks, security validation\\n**Resources**: 3 QA Engineers, 2 Performance Specialists, 1 Security Analyst\\n\\n## Phase 5: User Acceptance Testing (Weeks 37-44)\\n\\n### 5.1 UAT Preparation\\n- Prepare test data and scenarios\\n- Train business users on testing procedures\\n- Set up UAT environment\\n- Create defect tracking processes\\n\\n### 5.2 Department-Specific UAT\\n- **Finance UAT**: Financial reporting accuracy, regulatory compliance\\n- **Sales UAT**: Sales data integrity, performance metrics validation\\n- **HR UAT**: Employee data privacy, payroll accuracy\\n\\n### 5.3 UAT Execution & Defect Resolution\\n- Execute business scenarios\\n- Document and resolve defects\\n- Obtain formal sign-off from departments\\n- Conduct final acceptance testing\\n\\n**Deliverables**: UAT results, defect resolution log, business sign-off\\n**Resources**: 6 Business Users (2 per department), 2 QA Engineers, 1 Project Manager\\n\\n## Phase 6: Production Migration (Weeks 45-48)\\n\\n### 6.1 Pre-Migration Activities\\n- Final data synchronization\\n- Communication to all stakeholders\\n- Backup of current production system\\n- Go/no-go decision meeting\\n\\n### 6.2 Migration Execution\\n- Execute migration runbooks\\n- Monitor migration progress\\n- Validate data integrity\\n- Perform smoke testing\\n\\n### 6.3 Post-Migration Activities\\n- System performance monitoring\\n- User support and issue resolution\\n- Documentation updates\\n- Lessons learned session\\n\\n**Deliverables**: Migrated production system, post-migration report\\n**Resources**: 4 Database Administrators, 2 System Engineers, 24/7 support team\\n\\n## Phase 7: Stabilization & Support (Weeks 49-52)\\n\\n### 7.1 Hypercare Support\\n- 24/7 monitoring and support\\n- Rapid issue resolution\\n- Performance optimization\\n- User training and support\\n\\n### 7.2 Knowledge Transfer\\n- Documentation handover to support teams\\n- Training for operational staff\\n- Establishment of ongoing maintenance procedures\\n- Creation of troubleshooting guides\\n\\n**Deliverables**: Stable production system, support documentation, trained support team\\n**Resources**: 2 Support Engineers, 1 Documentation Specialist\\n\\n## Risk Management\\n\\n### High-Risk Items\\n1. **Data Loss/Corruption**: Implement comprehensive backup and validation procedures\\n2. **Extended Downtime**: Plan for phased migration with minimal downtime windows\\n3. **Regulatory Compliance**: Engage compliance team early and maintain audit trails\\n4. **Performance Degradation**: Conduct thorough performance testing and optimization\\n5. **User Resistance**: Implement change management and comprehensive training\\n\\n### Mitigation Strategies\\n- Maintain parallel systems during transition\\n- Implement automated rollback procedures\\n- Establish 24/7 support during critical phases\\n- Regular stakeholder communication and updates\\n- Comprehensive testing at each phase\\n\\n## Resource Requirements\\n\\n### Core Team\\n- 1 Project Manager\\n- 2 Database Architects\\n- 4 Database Developers\\n- 3 Infrastructure Engineers\\n- 3 Report Developers\\n- 4 QA Engineers\\n- 6 Business SMEs (2 per department)\\n- 1 Change Management Specialist\\n\\n### Estimated Budget\\n- Personnel: $2.5M - $3.5M\\n- Infrastructure: $500K - $800K\\n- Tools and Licenses: $200K - $400K\\n- Contingency (15%): $480K - $705K\\n- **Total**: $3.68M - $5.405M\\n\\n## Success Criteria\\n1. 100% data integrity maintained\\n2. All 300 reports functioning correctly\\n3. Performance meets or exceeds current benchmarks\\n4. Zero regulatory compliance issues\\n5. User acceptance rate >95%\\n6. Migration completed within planned timeline\\n7. Post-migration support incidents <5% of baseline\\n\\n## Timeline Summary\\n- **Total Duration**: 52 weeks (12 months)\\n- **Critical Path**: Database migration and testing phases\\n- **Key Milestones**: \\n  - Week 8: Assessment complete\\n  - Week 16: Environments ready\\n  - Week 28: Development complete\\n  - Week 36: System testing complete\\n  - Week 44: UAT complete\\n  - Week 48: Production migration complete\\n  - Week 52: Project closure',\n",
       " 'risk_assessment': '**CRITICAL RISKS (High Impact, High Probability):**\\n\\n1. **Regulatory Compliance Failure**\\n   - **Risk**: Non-compliance with FDA 21 CFR Part 11, GxP, or other pharmaceutical regulations\\n   - **Impact**: Regulatory sanctions, audit failures, potential product recalls, legal liability\\n   - **Likelihood**: Medium-High\\n   - **Mitigation**: Engage regulatory compliance team from Phase 1, maintain complete audit trails, implement electronic signatures, conduct compliance-specific testing, obtain regulatory pre-approval for migration approach\\n\\n2. **Data Integrity Compromise**\\n   - **Risk**: Data corruption, loss, or inconsistency during migration affecting patient safety data or financial records\\n   - **Impact**: Patient safety risks, financial misstatements, regulatory violations\\n   - **Likelihood**: Medium\\n   - **Mitigation**: Implement checksums and data validation at every step, maintain parallel systems during transition, conduct multiple data integrity tests, establish automated rollback procedures\\n\\n3. **Extended Business Disruption**\\n   - **Risk**: Migration takes longer than planned, causing prolonged system unavailability\\n   - **Impact**: Inability to process critical business operations, revenue loss, compliance reporting delays\\n   - **Likelihood**: High\\n   - **Mitigation**: Implement phased migration approach, maintain legacy systems as backup, establish clear rollback criteria and procedures, plan migrations during low-activity periods\\n\\n**HIGH RISKS (High Impact, Medium Probability):**\\n\\n4. **Performance Degradation**\\n   - **Risk**: New system performs slower than current system, affecting business operations\\n   - **Impact**: Reduced productivity, user dissatisfaction, potential business process delays\\n   - **Likelihood**: Medium\\n   - **Mitigation**: Conduct extensive performance testing with production-like data volumes, optimize queries and indexes, implement performance monitoring, have database tuning specialists on standby\\n\\n5. **Security Breach During Migration**\\n   - **Risk**: Sensitive pharmaceutical data exposed during migration process\\n   - **Impact**: HIPAA violations, intellectual property theft, regulatory penalties, reputation damage\\n   - **Likelihood**: Medium\\n   - **Mitigation**: Implement end-to-end encryption, use secure data transfer protocols, conduct security assessments, limit access to migration team only, implement comprehensive logging\\n\\n6. **Critical Resource Unavailability**\\n   - **Risk**: Key team members become unavailable during critical phases\\n   - **Impact**: Project delays, knowledge gaps, quality issues\\n   - **Likelihood**: Medium\\n   - **Mitigation**: Cross-train team members, maintain detailed documentation, identify backup resources, implement knowledge transfer sessions\\n\\n**MEDIUM RISKS (Medium Impact, Medium Probability):**\\n\\n7. **Third-Party Integration Failures**\\n   - **Risk**: External systems fail to integrate properly with migrated database\\n   - **Impact**: Broken business processes, manual workarounds required\\n   - **Likelihood**: Medium\\n   - **Mitigation**: Early engagement with third-party vendors, comprehensive integration testing, maintain legacy interfaces temporarily\\n\\n8. **User Adoption Resistance**\\n   - **Risk**: Business users resist new system or struggle with changes\\n   - **Impact**: Reduced efficiency, increased support costs, potential workarounds\\n   - **Likelihood**: Medium\\n   - **Mitigation**: Implement comprehensive change management, provide extensive training, establish user champions, maintain user feedback channels\\n\\n**CONTINGENCY PLANNING:**\\n- Maintain $705K contingency budget (15% of maximum estimate)\\n- Establish emergency response team with 24/7 availability during critical phases\\n- Prepare detailed rollback procedures for each migration phase\\n- Maintain parallel systems for minimum 4 weeks post-migration\\n- Create communication plan for stakeholder updates during issues\\n- Establish escalation procedures for regulatory and compliance concerns\\n\\n**MONITORING AND CONTROL:**\\n- Weekly risk assessment reviews during active phases\\n- Real-time monitoring of system performance and data integrity\\n- Regular compliance checkpoints with regulatory team\\n- Continuous stakeholder communication on risk status\\n- Monthly risk register updates with mitigation progress',\n",
       " 'conversation_history': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-6ff28d7c3504962c6ec008df78bc048d\", \"tr-09bdc2b2ef0d2f1b9c69d7a894e8e13a\", \"tr-0d4f931294e7f689439708087045edce\"]",
      "text/plain": [
       "[Trace(trace_id=tr-6ff28d7c3504962c6ec008df78bc048d), Trace(trace_id=tr-09bdc2b2ef0d2f1b9c69d7a894e8e13a), Trace(trace_id=tr-0d4f931294e7f689439708087045edce)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_planning_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fad7dbe-dc28-4508-b653-2f2c50c8f8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 20:53:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\ud83d\udd17 View Logged Model at: https://adb-984752964297111.11.azuredatabricks.net/ml/experiments/3013595711630001/models/m-7f4c40b1812a4289b6a7890f6f085478?o=984752964297111\n",
      "2025/09/13 20:53:37 WARNING mlflow.utils.requirements_utils: Failed to run predict on input_example, dependencies introduced in predict are not captured.\n",
      "KeyError('messages')Traceback (most recent call last):\n",
      "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages/mlflow/utils/_capture_modules.py\", line 166, in load_model_and_predict\n",
      "    model.predict(input_example, params=params)\n",
      "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages/mlflow/dspy/wrapper.py\", line 164, in predict\n",
      "    converted_inputs = self._get_model_input(inputs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-5605b0f9-6d0c-4b1d-9c46-7dcbcfc9760c/lib/python3.12/site-packages/mlflow/dspy/wrapper.py\", line 213, in _get_model_input\n",
      "    return inputs[\"messages\"]\n",
      "           ~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'messages'\n",
      "2025/09/13 20:53:40 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-7f4c40b1812a4289b6a7890f6f085478\n",
      "2025/09/13 20:53:40 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/09/13 20:53:40 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": {\n",
      "    \"question\": \"How many team mem.... Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: Failed to enforce schema of data '{'question': array('How many team members are there?', dtype='<U32'), 'category': array('Resource', dtype='<U8'), 'context': array('We have 5 team members', dtype='<U22')}' with schema '['messages': Array({content: string (required), role: string (required)}) (required)]'. Error: Model is missing inputs ['messages']. Note that there were extra inputs: ['question', 'context', 'category'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<head>\n",
       "  <link\n",
       "    rel=\"stylesheet\"\n",
       "    href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css\"\n",
       "  />\n",
       "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js\"></script>\n",
       "  <script>\n",
       "    hljs.highlightAll();\n",
       "  </script>\n",
       "  <style>\n",
       "    body {\n",
       "      margin: 0;\n",
       "      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n",
       "        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n",
       "        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n",
       "      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n",
       "      margin: 0;\n",
       "      font-weight: 400;\n",
       "      font-size: 13px;\n",
       "      line-height: 18px;\n",
       "      color: rgb(17, 23, 28);\n",
       "    }\n",
       "    code {\n",
       "      line-height: 18px;\n",
       "      font-size: 11px;\n",
       "      background: rgb(250, 250, 250) !important;\n",
       "    }\n",
       "    pre {\n",
       "      background: rgb(250, 250, 250);\n",
       "      margin: 0;\n",
       "      display: none;\n",
       "    }\n",
       "    pre.active {\n",
       "      display: unset;\n",
       "    }\n",
       "    button {\n",
       "      white-space: nowrap;\n",
       "      text-align: center;\n",
       "      position: relative;\n",
       "      cursor: pointer;\n",
       "      background: rgba(34, 114, 180, 0) !important;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      border-color: rgba(34, 114, 180, 0) !important;\n",
       "      padding: 4px 6px !important;\n",
       "      text-decoration: none !important;\n",
       "      line-height: 20px !important;\n",
       "      box-shadow: none !important;\n",
       "      height: 32px !important;\n",
       "      display: inline-flex !important;\n",
       "      -webkit-box-align: center !important;\n",
       "      align-items: center !important;\n",
       "      -webkit-box-pack: center !important;\n",
       "      justify-content: center !important;\n",
       "      vertical-align: middle !important;\n",
       "    }\n",
       "    p {\n",
       "      margin: 0;\n",
       "      padding: 0;\n",
       "    }\n",
       "    button:hover {\n",
       "      background: rgba(34, 114, 180, 0.08) !important;\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    button:active {\n",
       "      background: rgba(34, 114, 180, 0.16) !important;\n",
       "      color: rgb(4, 53, 93) !important;\n",
       "    }\n",
       "    h1 {\n",
       "      margin-top: 4px;\n",
       "      font-size: 22px;\n",
       "    }\n",
       "    .info {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      line-height: 16px;\n",
       "      color: rgb(95, 114, 129);\n",
       "    }\n",
       "    .tabs {\n",
       "      margin-top: 10px;\n",
       "      border-bottom: 1px solid rgb(209, 217, 225) !important;\n",
       "      display: flex;\n",
       "      line-height: 24px;\n",
       "    }\n",
       "    .tab {\n",
       "      font-size: 13px;\n",
       "      font-weight: 600 !important;\n",
       "      cursor: pointer;\n",
       "      margin: 0 24px 0 2px;\n",
       "      padding-left: 2px;\n",
       "    }\n",
       "    .tab:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .tab.active {\n",
       "      border-bottom: 3px solid rgb(34, 114, 180) !important;\n",
       "    }\n",
       "    .link {\n",
       "      margin-left: 12px;\n",
       "      display: inline-block;\n",
       "      text-decoration: none;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      font-size: 13px;\n",
       "      font-weight: 400;\n",
       "    }\n",
       "    .link:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .link-content {\n",
       "      display: flex;\n",
       "      gap: 6px;\n",
       "      align-items: center;\n",
       "    }\n",
       "    .caret-up {\n",
       "      transform: rotate(180deg);\n",
       "    }\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "  <div style=\"display: flex; align-items: center\">\n",
       "    The logged model is compatible with the Mosaic AI Agent Framework.\n",
       "    <button onclick=\"toggleCode()\">\n",
       "      See how to evaluate the model&nbsp;\n",
       "      <span\n",
       "        role=\"img\"\n",
       "        id=\"caret\"\n",
       "        aria-hidden=\"true\"\n",
       "        class=\"anticon css-6xix1i\"\n",
       "        style=\"font-size: 14px\"\n",
       "        ><svg\n",
       "          xmlns=\"http://www.w3.org/2000/svg\"\n",
       "          width=\"1em\"\n",
       "          height=\"1em\"\n",
       "          fill=\"none\"\n",
       "          viewBox=\"0 0 16 16\"\n",
       "          aria-hidden=\"true\"\n",
       "          focusable=\"false\"\n",
       "          class=\"\"\n",
       "        >\n",
       "          <path\n",
       "            fill=\"currentColor\"\n",
       "            fill-rule=\"evenodd\"\n",
       "            d=\"M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z\"\n",
       "            clip-rule=\"evenodd\"\n",
       "          ></path>\n",
       "        </svg>\n",
       "      </span>\n",
       "    </button>\n",
       "  </div>\n",
       "  <div id=\"code\" style=\"display: none\">\n",
       "    <h1>\n",
       "      Agent evaluation\n",
       "      <a\n",
       "        class=\"link\"\n",
       "        href=\"https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook\"\n",
       "        target=\"_blank\"\n",
       "      >\n",
       "        <span class=\"link-content\">\n",
       "          Learn more\n",
       "          <span role=\"img\" aria-hidden=\"true\" class=\"anticon css-6xix1i\"\n",
       "            ><svg\n",
       "              xmlns=\"http://www.w3.org/2000/svg\"\n",
       "              width=\"1em\"\n",
       "              height=\"1em\"\n",
       "              fill=\"none\"\n",
       "              viewBox=\"0 0 16 16\"\n",
       "              aria-hidden=\"true\"\n",
       "              focusable=\"false\"\n",
       "              class=\"\"\n",
       "            >\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"\n",
       "              ></path>\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"\n",
       "              ></path></svg></span></span\n",
       "      ></a>\n",
       "    </h1>\n",
       "    <p class=\"info\">\n",
       "      Copy the following code snippet in a notebook cell (right click \u2192 copy)\n",
       "    </p>\n",
       "    <div class=\"tabs\">\n",
       "      <div class=\"tab active\" onclick=\"tabClicked(0)\">Using synthetic data</div>\n",
       "      <div class=\"tab\" onclick=\"tabClicked(1)\">Using your own dataset</div>\n",
       "    </div>\n",
       "    <div style=\"height: 472px\">\n",
       "      <pre\n",
       "        class=\"active\"\n",
       "      ><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "from databricks.agents.evals import generate_evals_df\n",
       "import mlflow\n",
       "\n",
       "agent_description = &quot;A chatbot that answers questions about Databricks.&quot;\n",
       "question_guidelines = &quot;&quot;&quot;\n",
       "# User personas\n",
       "- A developer new to the Databricks platform\n",
       "# Example questions\n",
       "- What API lets me parallelize operations over rows of a delta table?\n",
       "&quot;&quot;&quot;\n",
       "# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\n",
       "docs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\n",
       "evals = generate_evals_df(\n",
       "    docs=docs,\n",
       "    num_evals=25,\n",
       "    agent_description=agent_description,\n",
       "    question_guidelines=question_guidelines,\n",
       ")\n",
       "eval_result = mlflow.evaluate(data=evals, model=&quot;models:/m-7f4c40b1812a4289b6a7890f6f085478&quot;, model_type=&quot;databricks-agent&quot;)\n",
       "</code></pre>\n",
       "\n",
       "      <pre><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "import pandas as pd\n",
       "import mlflow\n",
       "\n",
       "evals = [\n",
       "    {\n",
       "        &quot;request&quot;: {\n",
       "            &quot;messages&quot;: [\n",
       "                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n",
       "            ],\n",
       "        },\n",
       "        # Optional, needed for judging correctness.\n",
       "        &quot;expected_facts&quot;: [\n",
       "            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n",
       "        ],\n",
       "    }\n",
       "]\n",
       "eval_result = mlflow.evaluate(\n",
       "    data=pd.DataFrame.from_records(evals), model=&quot;models:/m-7f4c40b1812a4289b6a7890f6f085478&quot;, model_type=&quot;databricks-agent&quot;\n",
       ")\n",
       "</code></pre>\n",
       "    </div>\n",
       "  </div>\n",
       "  <script>\n",
       "    var codeShown = false;\n",
       "    function clip(el) {\n",
       "      var range = document.createRange();\n",
       "      range.selectNodeContents(el);\n",
       "      var sel = window.getSelection();\n",
       "      sel.removeAllRanges();\n",
       "      sel.addRange(range);\n",
       "    }\n",
       "\n",
       "    function toggleCode() {\n",
       "      if (codeShown) {\n",
       "        document.getElementById(\"code\").style.display = \"none\";\n",
       "        codeShown = false;\n",
       "      } else {\n",
       "        document.getElementById(\"code\").style.display = \"block\";\n",
       "        clip(document.querySelector(\"pre.active\"));\n",
       "        codeShown = true;\n",
       "      }\n",
       "      document.getElementById(\"caret\").classList.toggle(\"caret-up\");\n",
       "    }\n",
       "\n",
       "    function tabClicked(tabIndex) {\n",
       "      document.querySelectorAll(\".tab\").forEach((tab, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          tab.classList.add(\"active\");\n",
       "        } else {\n",
       "          tab.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      document.querySelectorAll(\"pre\").forEach((pre, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          pre.classList.add(\"active\");\n",
       "        } else {\n",
       "          pre.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      clip(document.querySelector(\"pre.active\"));\n",
       "    }\n",
       "  </script>\n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Agent logged to MLflow: models:/m-7f4c40b1812a4289b6a7890f6f085478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'vbdemos.usecase_agent.delivery-planning-agent' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f67d54aee514fc38db51333cc9839c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78b29eb3a034c2a891e4929073b95c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Agent registered in Unity Catalog: vbdemos.usecase_agent.delivery-planning-agent\n",
      "   Version: 3\n",
      "\u2705 Agent ready for deployment!\n",
      "\ud83d\udcca Model URI: models:/m-7f4c40b1812a4289b6a7890f6f085478\n",
      "\ud83c\udff7\ufe0f  Registered as: vbdemos.usecase_agent.delivery-planning-agent\n",
      "\ud83d\udcc8 Next step: Deploy via Databricks Model Serving UI or API\n",
      "\ud83d\udd17 Use the model URI to create a serving endpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ud83d\udd17 Created version '3' of model 'vbdemos.usecase_agent.delivery-planning-agent': https://adb-984752964297111.11.azuredatabricks.net/explore/data/models/vbdemos/usecase_agent/delivery-planning-agent/version/3?o=984752964297111\n"
     ]
    }
   ],
   "source": [
    "# Deploy Agent using DSPy MLflow Integration\nimport mlflow\nimport mlflow.dspy\nfrom mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n\n# Set registry URI for Unity Catalog\nmlflow.set_registry_uri(\"databricks-uc\")\n\n# Input example for the model\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"How many team members are there for the Databricks migration? Category: Resource. Context: We have 5 team members.\"\n        }\n    ]\n}\n\n# Create a DSPy-compatible wrapper for the agent\nclass DeliveryPlanningAgentDSPy(dspy.Module):\n    \"\"\"DSPy-compatible wrapper for the Delivery Planning Agent.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.agent = None\n        \n    def forward(self, question, category=\"General\", context=\"\"):\n        \"\"\"Forward method for DSPy.\"\"\"\n        try:\n            if not self.agent:\n                # Initialize the agent if not already done\n                self.agent = DeliveryPlanningAgent(\n                    vector_search_endpoint=VECTOR_SEARCH_ENDPOINT_NAME,\n                    vector_search_index=VECTOR_SEARCH_INDEX_NAME\n                )\n            \n            result = self.agent.answer_question(question, context, category)\n            return dspy.Prediction(answer=result[\"answer\"])\n        except Exception as e:\n            return dspy.Prediction(answer=f\"Error: {str(e)}\")\n\n# Create the DSPy model instance\ndspy_agent = DeliveryPlanningAgentDSPy()\n\n# Log the agent using DSPy MLflow integration\nwith mlflow.start_run() as run:\n    logged_agent_info = mlflow.dspy.log_model(\n        dspy_model=dspy_agent,\n        artifact_path=\"delivery-planning-agent\",\n        input_example=input_example,\n        task=\"llm/v1/chat\",\n        model_config={\n            \"vector_search_endpoint\": VECTOR_SEARCH_ENDPOINT_NAME,\n            \"vector_search_index\": VECTOR_SEARCH_INDEX_NAME\n        },\n        resources=[\n            DatabricksVectorSearchIndex(index_name=VECTOR_SEARCH_INDEX_NAME),\n            DatabricksServingEndpoint(endpoint_name=\"databricks-claude-sonnet-4\")\n        ]\n    )\n    \n    print(f\" Agent logged to MLflow: {logged_agent_info.model_uri}\")\n\n# Register the agent in Unity Catalog\nmodel_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.delivery-planning-agent\"\nuc_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=model_name)\n\nprint(f\" Agent registered in Unity Catalog: {uc_model_info.name}\")\nprint(f\"   Version: {uc_model_info.version}\")\n\nprint(f\" Agent ready for deployment!\")\nprint(f\" Model URI: {logged_agent_info.model_uri}\")\nprint(f\"  Registered as: {uc_model_info.name}\")\nprint(f\" Next step: Deploy via Databricks Model Serving UI or API\")\nprint(f\" Use the model URI to create a serving endpoint\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "usecase_delivery_planning_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}