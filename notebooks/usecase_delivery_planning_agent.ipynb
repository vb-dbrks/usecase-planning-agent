{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9c28db-6a5d-4183-9497-abcdcd76338b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Usecase Delivery Planning Agent\n",
    "\n",
    "This notebook implements a RAG-based delivery planning agent using DSPy that helps users generate comprehensive project plans for data migration and delivery projects. The agent leverages indexed documents and follows a structured approach to gather requirements and generate actionable project plans.\n",
    "\n",
    "## Features:\n",
    "- **Intelligent Question Generation**: Automatically generates relevant questions based on project context\n",
    "- **Document Retrieval**: Uses vector search to find relevant information from indexed documents\n",
    "- **Structured Planning**: Generates comprehensive project plans with timelines, resources, and milestones\n",
    "- **Risk Assessment**: Identifies potential risks and mitigation strategies\n",
    "- **DSPy Optimization**: Uses DSPy for prompt optimization and performance improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fdb174-2dec-4da1-b66b-924272136c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5aca31a-d76c-40ef-b3e1-61c876a7b078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dspy-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (3.0.3)\nRequirement already satisfied: databricks-vectorsearch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (0.59)\nRequirement already satisfied: pydantic in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (2.11.9)\nRequirement already satisfied: databricks-agents in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (1.5.0)\nRequirement already satisfied: dspy>=3.0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy-ai) (3.0.3)\nRequirement already satisfied: mlflow-skinny<4,>=2.11.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-vectorsearch) (3.3.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-vectorsearch) (5.29.5)\nRequirement already satisfied: requests>=2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-vectorsearch) (2.32.3)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-vectorsearch) (2.1.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from pydantic) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from pydantic) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from pydantic) (0.4.1)\nRequirement already satisfied: databricks-connect in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-agents) (17.2.0)\nRequirement already satisfied: dataclasses-json in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (0.6.7)\nRequirement already satisfied: databricks-sdk>=0.58.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents) (0.65.0)\nRequirement already satisfied: jinja2>=3.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-agents) (3.1.6)\nRequirement already satisfied: tenacity>=8.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (9.1.2)\nRequirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (0.11.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (4.67.1)\nRequirement already satisfied: urllib3>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (2.5.0)\nRequirement already satisfied: whenever==0.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (0.7.3)\nRequirement already satisfied: boto3>1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (1.40.31)\nRequirement already satisfied: botocore in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (1.40.31)\nRequirement already satisfied: litellm==1.75.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-agents) (1.75.9)\nRequirement already satisfied: aiohttp>=3.10 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (3.12.15)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (8.1.7)\nRequirement already satisfied: httpx>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (0.28.1)\nRequirement already satisfied: importlib-metadata>=6.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (8.4.0)\nRequirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (4.23.0)\nRequirement already satisfied: openai>=1.99.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (1.107.3)\nRequirement already satisfied: python-dotenv>=0.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (1.1.1)\nRequirement already satisfied: tokenizers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (0.22.0)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents) (1.0.1)\nRequirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from boto3>1->databricks-agents) (0.14.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from botocore->databricks-agents) (2.8.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (2.35.0)\nRequirement already satisfied: langchain-openai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents) (0.3.33)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from deprecation>=2->databricks-vectorsearch) (24.1)\nRequirement already satisfied: backoff>=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (2.2.1)\nRequirement already satisfied: joblib~=1.3 in /databricks/python3/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (1.4.2)\nRequirement already satisfied: regex>=2023.10.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (2025.9.1)\nRequirement already satisfied: orjson>=3.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (3.11.3)\nRequirement already satisfied: optuna>=3.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (4.5.0)\nRequirement already satisfied: magicattr>=0.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (0.1.6)\nRequirement already satisfied: diskcache>=5.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (5.6.3)\nRequirement already satisfied: json-repair>=0.30.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (0.50.1)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (4.10.0)\nRequirement already satisfied: asyncer==0.0.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (0.0.8)\nRequirement already satisfied: cachetools>=5.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (5.5.2)\nRequirement already satisfied: cloudpickle>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (3.1.1)\nRequirement already satisfied: rich>=13.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (14.1.0)\nRequirement already satisfied: numpy>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (1.26.4)\nRequirement already satisfied: xxhash>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dspy>=3.0.3->dspy-ai) (3.5.0)\nRequirement already satisfied: gepa==0.0.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from gepa[dspy]==0.0.7->dspy>=3.0.3->dspy-ai) (0.0.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from jinja2>=3.0.0->databricks-agents) (3.0.2)\nRequirement already satisfied: fastapi<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.1.37)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.27.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.27.0)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (6.0.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.5.1)\nRequirement already satisfied: uvicorn<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.35.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2024.6.2)\nRequirement already satisfied: googleapis-common-protos>=1.65.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.67.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.71.2)\nRequirement already satisfied: grpcio>=1.67.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.74.0)\nRequirement already satisfied: pandas>=1.0.5 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (2.3.2)\nRequirement already satisfied: py4j==0.10.9.9 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (0.10.9.9)\nRequirement already satisfied: pyarrow>=10.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (20.0.0)\nRequirement already satisfied: setuptools>=68.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (80.9.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect->databricks-agents) (1.16.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dataclasses-json->databricks-agents) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from dataclasses-json->databricks-agents) (0.9.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.20.1)\nRequirement already satisfied: sniffio>=1.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from anyio->dspy>=3.0.3->dspy-ai) (1.3.1)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.47.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.0.11)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (4.9)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.75.9->databricks-agents) (0.16.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm==1.75.9->databricks-agents) (3.17.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (0.27.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (0.11.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.48b0)\nRequirement already satisfied: alembic>=1.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.16.5)\nRequirement already satisfied: colorlog in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (2.0.43)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect->databricks-agents) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect->databricks-agents) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.7.1->dspy>=3.0.3->dspy-ai) (2.15.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents) (1.0.0)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.3.76)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from tokenizers->litellm==1.75.9->databricks-agents) (0.34.6)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (1.3.10)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (5.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (3.15.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (2025.9.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (1.1.10)\nRequirement already satisfied: langsmith>=0.3.45 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.4.28)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (1.33)\nRequirement already satisfied: mdurl~=0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.3->dspy-ai) (0.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.4.8)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.3->dspy-ai) (3.2.4)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (3.0.0)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.25.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Note: These versions are compatible with Databricks Runtime 16.4 LTS\n",
    "%pip install dspy-ai databricks-vectorsearch pydantic databricks-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b18e3bb-fd20-4a90-9c91-dad7842c833a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python to ensure packages are loaded\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009dc2a3-e70a-42b4-be5e-4aea2ebcf217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\nVector Search Endpoint: usecase-agent\nVector Search Index: vbdemos.usecase_agent.migration_plan_pdfs\nDocuments Table: vbdemos.usecase_agent.usecase_planning_agent_pdf_parsed\nVector Index Table: vbdemos.usecase_agent.migration_plan_pdfs\nPlanning Categories: ['Resource', 'Current Process Maturity', 'Customer Background', 'Scope']\n\nNote: Vector search index must be in format <catalog>.<schema>.<table>\nVector index structure: path (string), text (string), __db_text_vector (array)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"usecase-agent\" #\"use-case-planning-agent\"  # Your vector search endpoint\n",
    "VECTOR_SEARCH_INDEX_NAME = \"vbdemos.usecase_agent.migration_plan_pdfs\" #\"vbdemos.usecase_agent.migration_plan_pdfs\"  # Your existing vector search index\n",
    "CATALOG_NAME = \"vbdemos\"\n",
    "SCHEMA_NAME = \"usecase_agent\"\n",
    "DOCUMENTS_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.usecase_planning_agent_pdf_parsed\"  # Source table\n",
    "VECTOR_INDEX_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.migration_plan_pdfs\"  # Vector search index table\n",
    "\n",
    "# Project planning categories and questions\n",
    "PLANNING_CATEGORIES = {\n",
    "    \"Resource\": [\n",
    "        \"How many team members are there?\",\n",
    "        \"Are they using PS skills?\",\n",
    "        \"Are they using an SI?\",\n",
    "        \"Are the teams sufficiently skilled/trained?\",\n",
    "        \"Are resources shared with other projects?\",\n",
    "        \"Have resources done this work before?\",\n",
    "        \"Is there a product owner?\",\n",
    "        \"Are the DevOps/SecOps/Infra teams under this 'teams' control/purview?\",\n",
    "        \"Program manager?\",\n",
    "        \"Are the BAU teams that will ultimately manage the new system sufficiently trained?\",\n",
    "        \"Has an end-user adoption plan been created?\"\n",
    "    ],\n",
    "    \"Current Process Maturity\": [\n",
    "        \"Do they have a history of delays?\",\n",
    "        \"Do they have change management authority/equivalent?\",\n",
    "        \"Do they have an identified way of working - agile/waterfall?\"\n",
    "    ],\n",
    "    \"Customer Background\": [\n",
    "        \"Does the customer have a specific deadline/reason for that deadline?\",\n",
    "        \"Customer is already using cloud?\",\n",
    "        \"Customer has Databricks elsewhere?\",\n",
    "        \"Customer has security approval for this migration?\",\n",
    "        \"Are there any key connectors that are needed?\",\n",
    "        \"What are the key drivers of the migration?\",\n",
    "        \"Are there any legal compliance or requirements to consider?\"\n",
    "    ],\n",
    "    \"Scope\": [\n",
    "        \"Has a pilot or POC been conducted?\",\n",
    "        \"Does the customer have visibility of all the data and pipelines that need migration?\",\n",
    "        \"Is customer aware of where and who uses the data?\",\n",
    "        \"Is lift and shift or redesign preferred?\",\n",
    "        \"How many pipelines are to be migrated?\",\n",
    "        \"Relative complexity of pipelines?\",\n",
    "        \"Volume of data to be migrated?\",\n",
    "        \"How frequently is the data updated?\",\n",
    "        \"Is there a proposed UC design/infrastructure design?\",\n",
    "        \"Is PII handling included?\",\n",
    "        \"Does the migration include monitoring?\",\n",
    "        \"Does the migration include optimization?\",\n",
    "        \"Will it be run in parallel or phased move over?\",\n",
    "        \"Awareness of business critical pipelines/pipelines that cannot be down?\",\n",
    "        \"Do they have control over how they receive data?\",\n",
    "        \"Are additional data quality checks needing to be implemented?\",\n",
    "        \"Are there any key connectors that need to be migrated?\",\n",
    "        \"What level of testing is required and who will be doing this?\",\n",
    "        \"Are data consumers/systems that use the data known?\",\n",
    "        \"Does customer already have a plan?\",\n",
    "        \"What is the quality of the data?\",\n",
    "        \"Are the data pathways known?\",\n",
    "        \"Has a permissions model been agreed?\",\n",
    "        \"Is there a new data layout/has the UC catalog/schema structure been designed and agreed?\",\n",
    "        \"Is disaster recovery included in this migration?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Vector Search Endpoint: {VECTOR_SEARCH_ENDPOINT_NAME}\")\n",
    "print(f\"Vector Search Index: {VECTOR_SEARCH_INDEX_NAME}\")\n",
    "print(f\"Documents Table: {DOCUMENTS_TABLE}\")\n",
    "print(f\"Vector Index Table: {VECTOR_INDEX_TABLE}\")\n",
    "print(f\"Planning Categories: {list(PLANNING_CATEGORIES.keys())}\")\n",
    "print(\"\\nNote: Vector search index must be in format <catalog>.<schema>.<table>\")\n",
    "print(\"Vector index structure: path (string), text (string), __db_text_vector (array)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2531b9c8-49b4-4ba1-a5b7-e224f1bcfc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. DSPy Setup and Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25eb89f-4818-4d11-98c5-c679ae4b0d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy version: 3.0.3\nDSPy configured successfully with Databricks model\nDSPy and Vector Search configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"DeliveryPlanningAgent\").getOrCreate()\n",
    "\n",
    "# Configure DSPy with Databricks model\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() + '/serving-endpoints'\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=\"databricks/databricks-claude-sonnet-4\",\n",
    "    api_key=token,\n",
    "    api_base=url,\n",
    ")\n",
    "\n",
    "# Configure DSPy with Databricks model\n",
    "# Note: MLflow integration is handled automatically in Databricks Runtime 16.4 LTS\n",
    "# The warnings about MLflowCallback can be safely ignored as they don't affect functionality\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(f\"DSPy version: {dspy.__version__}\")\n",
    "print(\"DSPy configured successfully with Databricks model\")\n",
    "\n",
    "# Initialize Vector Search client\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "print(\"DSPy and Vector Search configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d9b1b4-a6ec-4020-87fb-a6482d888a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. DSPy Signatures and Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69613a6e-c9a7-408a-9dbf-5221cdd9d104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy signatures defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# DSPy Signatures for the delivery planning agent\n",
    "\n",
    "class QuestionGenerator(dspy.Signature):\n",
    "    \"\"\"Generate relevant questions for migrating TO Databricks from existing data/analytics platforms.\"\"\"\n",
    "    project_context: str = dspy.InputField(desc=\"Description of the current data/analytics platform and objectives for migrating TO Databricks\")\n",
    "    category: str = dspy.InputField(desc=\"Planning category (Resource, Scope, Customer Background, etc.)\")\n",
    "    existing_answers: str = dspy.InputField(desc=\"Previously answered questions and responses\")\n",
    "    questions: str = dspy.OutputField(desc=\"List of 3-5 most relevant questions for this category specific to migrating TO Databricks\")\n",
    "\n",
    "class DocumentRetriever(dspy.Signature):\n",
    "    \"\"\"Retrieve relevant documents based on questions and context.\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"Specific question to find relevant information for\")\n",
    "    project_context: str = dspy.InputField(desc=\"Project context and background\")\n",
    "    retrieved_docs: str = dspy.OutputField(desc=\"Relevant document excerpts and information\")\n",
    "\n",
    "class AnswerAnalyzer(dspy.Signature):\n",
    "    \"\"\"Analyze answers and extract key insights for project planning.\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"The question that was asked\")\n",
    "    answer: str = dspy.InputField(desc=\"The answer provided by the user\")\n",
    "    relevant_docs: str = dspy.InputField(desc=\"Relevant documents retrieved from knowledge base\")\n",
    "    insights: str = dspy.OutputField(desc=\"Key insights, risks, and implications for project planning\")\n",
    "\n",
    "class ProjectPlanGenerator(dspy.Signature):\n",
    "    \"\"\"Generate comprehensive migration plan for moving TO Databricks with structured table outputs.\"\"\"\n",
    "    project_context: str = dspy.InputField(desc=\"Overall project context and objectives for migrating TO Databricks\")\n",
    "    gathered_insights: str = dspy.InputField(desc=\"All insights gathered from questions and documents about current platform\")\n",
    "    timeline_requirements: str = dspy.InputField(desc=\"Timeline constraints and requirements for migrating TO Databricks\")\n",
    "    project_plan: str = dspy.OutputField(desc=\"Comprehensive migration plan for moving TO Databricks. Output as structured tables with clear headers for: 1) Migration Timeline, 2) Resource Requirements, 3) Migration Phases, 4) Risk Assessment. Use markdown table format with | separators.\")\n",
    "\n",
    "class RiskAssessor(dspy.Signature):\n",
    "    \"\"\"Assess risks and provide mitigation strategies.\"\"\"\n",
    "    project_plan: str = dspy.InputField(desc=\"The proposed project plan\")\n",
    "    project_context: str = dspy.InputField(desc=\"Project context and constraints\")\n",
    "    risk_assessment: str = dspy.OutputField(desc=\"Identified risks, their likelihood, impact, and mitigation strategies\")\n",
    "\n",
    "print(\"DSPy signatures defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9527bc5-da5f-424f-be13-558953641af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy modules defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# DSPy Modules for the delivery planning agent\n",
    "\n",
    "class QuestionGenerationModule(dspy.Module):\n",
    "    \"\"\"Module to generate relevant questions for each planning category.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(QuestionGenerator)\n",
    "    \n",
    "    def forward(self, project_context: str, category: str, existing_answers: str = \"\"):\n",
    "        return self.generate(\n",
    "            project_context=project_context,\n",
    "            category=category,\n",
    "            existing_answers=existing_answers\n",
    "        )\n",
    "\n",
    "class DocumentRetrievalModule(dspy.Module):\n",
    "    \"\"\"Module to retrieve relevant documents using vector search.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_search_endpoint: str, vector_search_index: str):\n",
    "        super().__init__()\n",
    "        self.vector_search_endpoint = vector_search_endpoint\n",
    "        self.vector_search_index = vector_search_index\n",
    "        self.retrieve = dspy.ChainOfThought(DocumentRetriever)\n",
    "    \n",
    "    def forward(self, question: str, project_context: str):\n",
    "        # Perform vector search\n",
    "        search_results = vsc.get_index(\n",
    "            endpoint_name=self.vector_search_endpoint,\n",
    "            index_name=self.vector_search_index\n",
    "        ).similarity_search(\n",
    "            query_text=question,\n",
    "            columns=[\"path\", \"text\"],  # Updated to match your vector index structure,\n",
    "            num_results=5\n",
    "        )\n",
    "        \n",
    "        # Format retrieved documents\n",
    "        # Handle the case where search_results might be a list or dict\n",
    "        if isinstance(search_results, list):\n",
    "            documents = search_results\n",
    "        else:\n",
    "            documents = search_results.get('result', {}).get('data_array', [])\n",
    "        \n",
    "        retrieved_docs = \"\\n\\n\".join([\n",
    "            f\"Source: {doc.get('path', 'Unknown') if isinstance(doc, dict) else 'Unknown'}\\nContent: {doc.get('text', '') if isinstance(doc, dict) else str(doc)}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        return self.retrieve(\n",
    "            question=question,\n",
    "            project_context=project_context,\n",
    "            retrieved_docs=retrieved_docs\n",
    "        )\n",
    "\n",
    "class AnswerAnalysisModule(dspy.Module):\n",
    "    \"\"\"Module to analyze answers and extract insights.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyze = dspy.ChainOfThought(AnswerAnalyzer)\n",
    "    \n",
    "    def forward(self, question: str, answer: str, relevant_docs: str):\n",
    "        return self.analyze(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            relevant_docs=relevant_docs\n",
    "        )\n",
    "\n",
    "class ProjectPlanGenerationModule(dspy.Module):\n",
    "    \"\"\"Module to generate comprehensive project plans.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_plan = dspy.ChainOfThought(ProjectPlanGenerator)\n",
    "    \n",
    "    def forward(self, project_context: str, gathered_insights: str, timeline_requirements: str):\n",
    "        return self.generate_plan(\n",
    "            project_context=project_context,\n",
    "            gathered_insights=gathered_insights,\n",
    "            timeline_requirements=timeline_requirements\n",
    "        )\n",
    "\n",
    "class RiskAssessmentModule(dspy.Module):\n",
    "    \"\"\"Module to assess risks and provide mitigation strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.assess_risks = dspy.ChainOfThought(RiskAssessor)\n",
    "    \n",
    "    def forward(self, project_plan: str, project_context: str):\n",
    "        return self.assess_risks(\n",
    "            project_plan=project_plan,\n",
    "            project_context=project_context\n",
    "        )\n",
    "\n",
    "print(\"DSPy modules defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdf6c4-560e-465b-9340-a3cc1047dc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Main Delivery Planning Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84881a6-9e82-4f0f-bb88-90750f0da9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery Planning Agent defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class DeliveryPlanningAgent(dspy.Module):\n",
    "    \"\"\"Main delivery planning agent that orchestrates the entire planning process.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_search_endpoint: str, vector_search_index: str):\n",
    "        super().__init__()\n",
    "        self.question_generator = QuestionGenerationModule()\n",
    "        self.document_retriever = DocumentRetrievalModule(vector_search_endpoint, vector_search_index)\n",
    "        self.answer_analyzer = AnswerAnalysisModule()\n",
    "        self.plan_generator = ProjectPlanGenerationModule()\n",
    "        self.risk_assessor = RiskAssessmentModule()\n",
    "        \n",
    "        # Store conversation state\n",
    "        self.conversation_history = []\n",
    "        self.gathered_insights = []\n",
    "        self.project_context = \"\"\n",
    "    \n",
    "    def start_planning_session(self, project_context: str, timeline_requirements: str = \"\"):\n",
    "        \"\"\"Start a new planning session.\"\"\"\n",
    "        self.project_context = project_context\n",
    "        self.conversation_history = []\n",
    "        self.gathered_insights = []\n",
    "        \n",
    "        print(\" Starting Delivery Planning Session\")\n",
    "        print(f\" Project Context: {project_context}\")\n",
    "        if timeline_requirements:\n",
    "            print(f\"⏰ Timeline Requirements: {timeline_requirements}\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        \n",
    "        return self._generate_questions_for_category(\"Resource\")\n",
    "    \n",
    "    def _generate_questions_for_category(self, category: str):\n",
    "        \"\"\"Generate questions for a specific category.\"\"\"\n",
    "        existing_answers = self._format_existing_answers()\n",
    "        \n",
    "        result = self.question_generator(\n",
    "            project_context=self.project_context,\n",
    "            category=category,\n",
    "            existing_answers=existing_answers\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n {category} Questions:\")\n",
    "        print(\"-\" * 30)\n",
    "        questions = result.questions.split('\\n')\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            if question.strip():\n",
    "                print(f\"{i}. {question.strip()}\")\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def answer_question(self, question: str, answer: str, category: str = \"\"):\n",
    "        \"\"\"Process a user's answer to a question.\"\"\"\n",
    "        print(f\"\\n Processing Answer:\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        doc_result = self.document_retriever(question, self.project_context)\n",
    "        relevant_docs = doc_result.retrieved_docs\n",
    "        \n",
    "        # Analyze the answer\n",
    "        analysis_result = self.answer_analyzer(question, answer, relevant_docs)\n",
    "        insights = analysis_result.insights\n",
    "        \n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"category\": category,\n",
    "            \"insights\": insights,\n",
    "            \"relevant_docs\": relevant_docs\n",
    "        })\n",
    "        \n",
    "        self.gathered_insights.append(insights)\n",
    "        \n",
    "        print(f\"\\n Key Insights:\")\n",
    "        print(insights)\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def generate_project_plan(self, timeline_requirements: str = \"\"):\n",
    "        \"\"\"Generate the final project plan based on all gathered information.\"\"\"\n",
    "        print(\"\\n Generating Project Plan...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Combine all insights\n",
    "        all_insights = \"\\n\\n\".join(self.gathered_insights)\n",
    "        \n",
    "        # Generate project plan\n",
    "        plan_result = self.plan_generator(\n",
    "            project_context=self.project_context,\n",
    "            gathered_insights=all_insights,\n",
    "            timeline_requirements=timeline_requirements\n",
    "        )\n",
    "        \n",
    "        project_plan = plan_result.project_plan\n",
    "        \n",
    "        print(\" PROJECT PLAN:\")\n",
    "        print(\"=\"*50)\n",
    "        print(project_plan)\n",
    "        \n",
    "        # Assess risks\n",
    "        risk_result = self.risk_assessor(project_plan, self.project_context)\n",
    "        risk_assessment = risk_result.risk_assessment\n",
    "        \n",
    "        print(\"\\n  RISK ASSESSMENT:\")\n",
    "        print(\"=\"*50)\n",
    "        print(risk_assessment)\n",
    "        \n",
    "        return {\n",
    "            \"project_plan\": project_plan,\n",
    "            \"risk_assessment\": risk_assessment,\n",
    "            \"conversation_history\": self.conversation_history\n",
    "        }\n",
    "    \n",
    "    def _format_existing_answers(self):\n",
    "        \"\"\"Format existing answers for context.\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"No previous answers yet.\"\n",
    "        \n",
    "        formatted = []\n",
    "        for entry in self.conversation_history:\n",
    "            formatted.append(f\"Q: {entry['question']}\\nA: {entry['answer']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted)\n",
    "    \n",
    "    def get_next_category_questions(self):\n",
    "        \"\"\"Get questions for the next planning category.\"\"\"\n",
    "        categories = list(PLANNING_CATEGORIES.keys())\n",
    "        completed_categories = set(entry.get('category', '') for entry in self.conversation_history)\n",
    "        \n",
    "        for category in categories:\n",
    "            if category not in completed_categories:\n",
    "                return self._generate_questions_for_category(category)\n",
    "        \n",
    "        return None  # All categories completed\n",
    "\n",
    "print(\"Delivery Planning Agent defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc77ef9b-5c1a-4e73-b1e3-aabae793267c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Example Usage and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa088574-9375-4c26-8866-41ee75e39d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Delivery Planning Session\n Project Context: \nWe are planning a data migration project for a large financial services company. \nThey want to migrate their existing Oracle data warehouse to Databricks on Azure. \nThe migration involves 50+ data pipelines, 2TB of data, and needs to be completed \nwithin 6 months. The company has some cloud experience but limited Databricks knowledge.\n\n⏰ Timeline Requirements: 6 months deadline, must be completed by end of Q2 2024\n\n==================================================\n\n Resource Questions:\n------------------------------\n1. 1. What is the current size and composition of your data engineering team, and how many team members can be dedicated to this migration project full-time?\n3. 2. Do you have existing Azure cloud infrastructure and Databricks workspace provisioned, or will this need to be set up as part of the project scope?\n5. 3. What is your allocated budget for this migration project, including licensing, training, external consulting, and potential infrastructure costs?\n7. 4. Are you planning to engage external Databricks consultants or system integrators, or do you prefer to build internal capabilities through training and upskilling?\n9. 5. What are your current Oracle database administration and data warehouse expertise levels within the team, and who will be responsible for knowledge transfer during the migration?\n\n==================================================\nExample: Answer some questions to see the agent in action\n==================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-6cd632e54663cc3a2688fedf541a9ae0\"",
      "text/plain": [
       "Trace(trace_id=tr-6cd632e54663cc3a2688fedf541a9ae0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the delivery planning agent\n",
    "agent = DeliveryPlanningAgent(VECTOR_SEARCH_ENDPOINT_NAME, VECTOR_SEARCH_INDEX_NAME)\n",
    "\n",
    "# Example: Start a planning session\n",
    "project_context = \"\"\"\n",
    "We are planning a data migration project for a large financial services company. \n",
    "They want to migrate their existing Oracle data warehouse to Databricks on Azure. \n",
    "The migration involves 50+ data pipelines, 2TB of data, and needs to be completed \n",
    "within 6 months. The company has some cloud experience but limited Databricks knowledge.\n",
    "\"\"\"\n",
    "\n",
    "timeline_requirements = \"6 months deadline, must be completed by end of Q2 2024\"\n",
    "\n",
    "# Start the planning session\n",
    "questions = agent.start_planning_session(project_context, timeline_requirements)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Example: Answer some questions to see the agent in action\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7725da76-5d37-4c3b-80f6-b33ddc54d378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Resource Questions...\n\n Processing Answer:\nQ: How many team members are there?\nA: We have 8 team members including 2 data engineers, 3 analysts, 2 developers, and 1 project manager\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:26:57 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\nKey insights for project planning:\n- **Team Size**: 8-member team provides sufficient capacity for a complex data migration project\n- **Role Distribution**: Good balance with emphasis on analysis (3 analysts) which is critical for data validation and testing phases\n- **Technical Capacity**: 4 technical resources (2 data engineers + 2 developers) should handle pipeline development and migration tasks\n- **Project Management**: Single PM may need support given project complexity (50+ pipelines, 2TB data, 6-month timeline)\n- **Potential Risks**: Limited Databricks expertise mentioned in project context may require additional training or external support\n- **Resource Allocation**: Consider if current staffing levels are adequate for 6-month timeline with 50+ pipeline migrations\n- **Skill Gap Consideration**: May need to assess team's Databricks proficiency and plan for knowledge transfer or training\n\n Processing Answer:\nQ: Are the teams sufficiently skilled/trained?\nA: The data engineers have some cloud experience but no Databricks experience. The analysts are familiar with SQL but not Spark. We need training.\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:27:10 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:27:17 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Critical Skill Gap Identified**: Major deficiency in Databricks and Spark expertise creates high project risk. **Immediate Training Required**: Comprehensive upskilling program needed before migration begins - estimate 2-3 months for basic competency. **Timeline Impact**: Current 6-month timeline likely unrealistic without external expertise or extended training period. **Risk Mitigation Options**: (1) Engage Databricks professional services/certified partners, (2) Hire experienced Databricks developers, (3) Implement phased approach with extensive training, (4) Consider proof-of-concept to build skills before full migration. **Budget Implications**: Additional costs for training, certifications, external consultants, or new hires must be factored into project budget. **Success Factors**: Project success heavily dependent on either rapid skill acquisition or supplementing team with external Databricks expertise. Recommend immediate action on training plan and consideration of hybrid internal/external team approach.\n\n Processing Answer:\nQ: Is there a product owner?\nA: Yes, we have a product owner from the business side who understands the data requirements\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:27:22 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:27:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Key Insights:**\n- **Positive Risk Mitigation:** Having a dedicated product owner with business domain knowledge significantly reduces the risk of misaligned requirements and scope creep during the data migration\n- **Business-IT Bridge:** The product owner's understanding of data requirements suggests good potential for translating business needs into technical specifications\n- **Documentation Gap:** The absence of role information in project documentation indicates a need to formalize team structure and responsibilities\n- **Stakeholder Engagement:** Business-side product owner presence suggests active business engagement, which is critical for a 6-month migration timeline\n\n**Planning Implications:**\n- Leverage the product owner's domain knowledge early in requirements gathering and validation phases\n- Ensure clear communication channels between the product owner and technical teams\n- Document all team roles and responsibilities to avoid confusion during execution\n- Establish regular checkpoints with the product owner to validate migration progress against business expectations\n- Consider the product owner as a key decision-maker for prioritizing the 50+ data pipelines migration sequence\n\n Processing Answer:\nQ: Are the DevOps/SecOps/Infra teams under this 'teams' control/purview?\nA: No, they are separate teams with their own priorities and reporting lines\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:27:41 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:27:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Critical Risk Identified**: Organizational silos present major project delivery risk. Key implications:\n\n**Dependencies & Bottlenecks**: Project success depends on teams outside direct control, creating potential scheduling conflicts and resource availability issues during critical migration phases.\n\n**Governance Gap**: Need to establish formal escalation paths and service level agreements with DevOps/SecOps/Infra teams to ensure project requirements are prioritized appropriately.\n\n**Timeline Risk**: 6-month timeline may be jeopardized if these teams have competing priorities or insufficient capacity to support migration activities.\n\n**Mitigation Strategies Required**:\n- Engage these teams early in project planning to secure commitments\n- Establish clear communication channels and regular touchpoints\n- Consider involving executive sponsors to ensure cross-team alignment\n- Build buffer time into project schedule for potential delays from dependencies\n- Document all requirements and handoffs clearly to minimize miscommunication\n\n**Resource Planning**: Budget may need to include external contractors if internal teams cannot provide adequate support within required timeframes.\n\n==================================================\nMoving to next category...\n==================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:28:00 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Current Process Maturity Questions:\n------------------------------\n1. 1. What is your current data development lifecycle process (development, testing, deployment) and do you have established CI/CD practices for your Oracle data warehouse pipelines?\n3. 2. How do you currently handle data quality monitoring, validation, and error handling in your existing Oracle environment?\n5. 3. What data governance processes do you have in place today (data lineage tracking, metadata management, access controls, compliance procedures)?\n7. 4. How do you currently manage environment promotion (dev/test/prod) and what change management processes exist for data pipeline modifications?\n9. 5. What monitoring and alerting capabilities do you have for your current data pipelines, and how do you handle incident response and troubleshooting?\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-25e2aabcd20e7a2f9302e96769d9bbdd\", \"tr-4e0683d2706ad1382ad4d88ae59c02eb\", \"tr-529a0848574e61ac1e32f422f03028e3\", \"tr-1b3f3d61260690c59fd5473424f530b5\", \"tr-9539abff3de554682630498b7c491595\", \"tr-4e2e741b207babc1ebb0d0b7448b56d0\", \"tr-9e55c39414ba18d6553bb006d7697309\", \"tr-b5f7c79b0d26dc353223034679bb90a2\", \"tr-850f5deb8ac95278cc2cc7b7fc8f62da\"]",
      "text/plain": [
       "[Trace(trace_id=tr-25e2aabcd20e7a2f9302e96769d9bbdd), Trace(trace_id=tr-4e0683d2706ad1382ad4d88ae59c02eb), Trace(trace_id=tr-529a0848574e61ac1e32f422f03028e3), Trace(trace_id=tr-1b3f3d61260690c59fd5473424f530b5), Trace(trace_id=tr-9539abff3de554682630498b7c491595), Trace(trace_id=tr-4e2e741b207babc1ebb0d0b7448b56d0), Trace(trace_id=tr-9e55c39414ba18d6553bb006d7697309), Trace(trace_id=tr-b5f7c79b0d26dc353223034679bb90a2), Trace(trace_id=tr-850f5deb8ac95278cc2cc7b7fc8f62da)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Simulate answering questions\n",
    "# In a real scenario, these would be user inputs\n",
    "\n",
    "# Answer Resource questions\n",
    "resource_answers = [\n",
    "    (\"How many team members are there?\", \"We have 8 team members including 2 data engineers, 3 analysts, 2 developers, and 1 project manager\"),\n",
    "    (\"Are the teams sufficiently skilled/trained?\", \"The data engineers have some cloud experience but no Databricks experience. The analysts are familiar with SQL but not Spark. We need training.\"),\n",
    "    (\"Is there a product owner?\", \"Yes, we have a product owner from the business side who understands the data requirements\"),\n",
    "    (\"Are the DevOps/SecOps/Infra teams under this 'teams' control/purview?\", \"No, they are separate teams with their own priorities and reporting lines\")\n",
    "]\n",
    "\n",
    "print(\" Processing Resource Questions...\")\n",
    "for question, answer in resource_answers:\n",
    "    agent.answer_question(question, answer, \"Resource\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Moving to next category...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get next category questions\n",
    "next_questions = agent.get_next_category_questions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e3be5f-433f-4ce9-b5c0-f76b5793eccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Scope Questions...\n\n Processing Answer:\nQ: How many pipelines are to be migrated?\nA: We have approximately 50 data pipelines that need to be migrated\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:28:08 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:28:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\nKey insights for project planning:\n- **Scale Impact**: 50 pipelines represent a significant migration workload requiring systematic approach and phased execution\n- **Resource Planning**: Will need dedicated teams for pipeline analysis, conversion, testing, and validation across all 50 pipelines\n- **Timeline Considerations**: With 6-month deadline, this averages to ~8-9 pipelines per month, requiring parallel processing and efficient workflow management\n- **Risk Factors**: High volume increases complexity and potential for cascading failures; need robust testing strategy for each pipeline\n- **Dependencies**: Pipeline interdependencies must be mapped to determine migration sequence and avoid downstream impacts\n- **Quality Assurance**: Each pipeline will require individual validation, data reconciliation, and performance testing\n- **Change Management**: Significant scope affecting multiple business processes and stakeholders across the organization\n\n Processing Answer:\nQ: Volume of data to be migrated?\nA: The total data volume is about 2TB across multiple tables and schemas\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:28:24 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:28:30 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Key Planning Insights:**\n- **Migration Approach**: 2TB allows for both full and incremental migration strategies; recommend phased approach to minimize business disruption\n- **Timeline Impact**: With 6-month constraint, allocate 2-3 months for data migration including testing phases\n- **Resource Requirements**: Moderate volume requires dedicated migration tools (Azure Data Factory, Databricks Auto Loader) and sufficient network bandwidth (minimum 1Gbps recommended)\n- **Risk Considerations**: Data validation becomes critical with this volume - implement automated data quality checks and reconciliation processes\n- **Infrastructure Planning**: Ensure adequate Azure storage provisioning and Databricks cluster sizing to handle 2TB efficiently\n- **Testing Strategy**: Plan for parallel testing environments to validate data integrity without impacting migration timeline\n- **Rollback Planning**: 2TB volume requires robust backup and rollback procedures in case of migration issues\n\n Processing Answer:\nQ: Is lift and shift or redesign preferred?\nA: We prefer a hybrid approach - lift and shift for simple pipelines, redesign for complex ones\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:28:42 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:28:55 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Key Project Planning Insights:**\n\n**Risk Mitigation:**\n- Hybrid approach reduces overall project risk by avoiding all-or-nothing strategy\n- Lift and shift provides quick wins and maintains business continuity\n- Selective redesign allows for strategic modernization without overwhelming the team\n\n**Resource Allocation Implications:**\n- Need to categorize pipelines by complexity and business criticality early\n- Require different skill sets: basic SQL conversion vs. advanced Databricks optimization\n- Training needs can be phased - basic Spark SQL first, advanced features later\n\n**Timeline Considerations:**\n- Front-load simple migrations to demonstrate early progress\n- Reserve redesign efforts for later phases when team has gained experience\n- Build buffer time for learning curve and unexpected complexities\n\n**Success Metrics:**\n- Track both migration velocity (lift and shift) and optimization gains (redesign)\n- Measure performance improvements on redesigned pipelines\n- Monitor business continuity during transition\n\n**Post-Migration Strategy:**\n- Plan for continuous improvement beyond 6-month window\n- Establish framework for evaluating which lifted pipelines should be redesigned later\n- Create knowledge transfer processes to build internal Databricks expertise\n\n Processing Answer:\nQ: Does the migration include monitoring?\nA: Yes, we need comprehensive monitoring and alerting for the new system\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:29:08 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:29:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Critical Success Factors:**\n- Monitoring is essential for managing the complexity of migrating 50+ pipelines within the 6-month deadline\n- Multi-layered approach needed: native Databricks monitoring, Azure integration, and specialized data quality tools\n- Real-time visibility into migration progress will be crucial for staying on schedule\n\n**Risk Mitigation:**\n- Comprehensive monitoring reduces risk of data loss or corruption during migration\n- Early detection of pipeline failures prevents cascading issues across dependent systems\n- Audit logging ensures regulatory compliance throughout the migration process\n\n**Resource Planning Implications:**\n- Budget allocation needed for monitoring tools (Monte Carlo, Great Expectations, etc.)\n- DevOps resources required to set up Azure Monitor integration and custom dashboards\n- Training needed for team members on new monitoring tools and processes\n\n**Timeline Considerations:**\n- Monitoring infrastructure should be established early in the migration timeline\n- Data validation and reconciliation processes must be implemented before production cutover\n- Executive reporting dashboards needed for stakeholder visibility into migration progress\n\n**Compliance Requirements:**\n- Financial services regulatory requirements mandate comprehensive audit trails\n- Data lineage tracking through Unity Catalog supports compliance reporting\n- Security monitoring ensures data protection standards are maintained during migration\n\n Processing Answer:\nQ: Will it be run in parallel or phased move over?\nA: We plan to do a phased migration over 6 months, starting with non-critical pipelines\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:29:28 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n2025/09/16 09:29:43 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Key Insights:\n**Key Insights for Project Planning:**\n\n**Positive Strategic Alignment:**\n- Phased approach reduces overall project risk and allows for iterative learning\n- 6-month timeline provides structured milestones for progress tracking and stakeholder communication\n- Starting with non-critical pipelines minimizes business disruption during initial learning phase\n\n**Critical Planning Considerations:**\n- **Timeline Risk**: 6 months for 50+ pipelines averages 8-10 pipelines per month, requiring significant parallel execution and resource allocation\n- **Learning Curve Impact**: Limited Databricks experience may slow initial phases but accelerate later phases as team expertise grows\n- **Resource Planning**: Need to balance team members across multiple parallel workstreams while maintaining knowledge transfer\n\n**Recommended Phase Structure Adjustments:**\n- Consider hybrid approach: Start with 2-3 non-critical pipelines for technical learning, then move to critical pipelines for business validation\n- Plan for 3-5 parallel migration streams within each phase to meet timeline objectives\n- Allocate 20-30% buffer time in early phases for unexpected technical challenges\n\n**Risk Mitigation Requirements:**\n- Establish clear rollback procedures for each phase\n- Implement comprehensive testing protocols given financial services regulatory environment\n- Plan for potential timeline extensions if critical issues emerge during non-critical pipeline migrations\n\n==================================================\nGenerating Project Plan...\n==================================================\n\n Generating Project Plan...\n==================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:30:15 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROJECT PLAN:\n==================================================\n## Migration Timeline\n\n| Phase | Duration | Timeline | Key Activities | Success Criteria |\n|-------|----------|----------|----------------|------------------|\n| **Phase 0: Foundation** | 4 weeks | Weeks 1-4 | Team training, tool setup, external expertise onboarding, cross-team governance establishment | Databricks environment ready, team certified, governance framework active |\n| **Phase 1: Pilot Migration** | 6 weeks | Weeks 5-10 | Migrate 5-8 non-critical pipelines, establish monitoring, validate processes | Successful migration of pilot pipelines, monitoring operational, processes validated |\n| **Phase 2: Critical Systems** | 8 weeks | Weeks 11-18 | Migrate 20-25 critical pipelines, implement compliance controls, performance optimization | Critical business functions operational, compliance validated, performance benchmarks met |\n| **Phase 3: Remaining Systems** | 6 weeks | Weeks 19-24 | Complete remaining 15-20 pipelines, full data validation, cutover preparation | All pipelines migrated, data integrity confirmed, cutover procedures tested |\n| **Phase 4: Go-Live & Stabilization** | 2 weeks | Weeks 25-26 | Production cutover, hypercare support, issue resolution | Production stable, Oracle systems decommissioned, stakeholder sign-off |\n\n## Resource Requirements\n\n| Resource Type | Internal | External | Total FTE | Key Responsibilities |\n|---------------|----------|----------|-----------|---------------------|\n| **Project Management** | 1 PM | 1 Senior PM | 2.0 | Timeline management, stakeholder coordination, risk mitigation |\n| **Databricks Specialists** | 0 | 3 Consultants | 3.0 | Architecture design, complex pipeline migration, team mentoring |\n| **Data Engineers** | 2 | 1 Senior | 3.0 | Pipeline development, data validation, performance tuning |\n| **Data Analysts** | 3 | 0 | 3.0 | Requirements analysis, data reconciliation, business validation |\n| **Developers** | 2 | 0 | 2.0 | Application integration, API development, testing automation |\n| **DevOps/Infrastructure** | 1 | 1 Specialist | 2.0 | Azure setup, monitoring implementation, security configuration |\n| **Business Product Owner** | 1 | 0 | 1.0 | Requirements validation, business acceptance, change management |\n| **Total Resources** | 10 | 5 | **15.0** | **Full project team capacity** |\n\n## Migration Phases\n\n| Phase | Pipelines | Approach | Key Focus Areas | Dependencies |\n|-------|-----------|----------|-----------------|--------------|\n| **Foundation Setup** | 0 | Infrastructure | Databricks workspace, Unity Catalog, monitoring tools, team training | Azure provisioning, security approvals, external consultant onboarding |\n| **Pilot Migration** | 5-8 | Lift & Shift | Process validation, tool effectiveness, team skill building | Foundation complete, non-critical pipeline identification |\n| **Critical Systems** | 20-25 | Hybrid | Business continuity, compliance validation, performance optimization | Pilot success, stakeholder approval, regulatory sign-off |\n| **Remaining Systems** | 15-20 | Optimized | Efficiency gains, advanced features, knowledge transfer | Critical systems stable, team expertise matured |\n| **Production Cutover** | All 50+ | Full Migration | Final validation, Oracle decommission, hypercare support | All phases complete, business acceptance, rollback procedures ready |\n\n## Risk Assessment\n\n| Risk Category | Risk Level | Impact | Mitigation Strategy | Contingency Plan |\n|---------------|------------|--------|-------------------|------------------|\n| **Skill Gap** | HIGH | Timeline delay, quality issues | Immediate external consultant engagement, intensive training program | Extend timeline by 2-3 months, increase external support |\n| **Organizational Silos** | HIGH | Resource bottlenecks, delays | Executive sponsorship, formal SLAs, early engagement | Escalation procedures, external contractor backup |\n| **Timeline Pressure** | MEDIUM | Quality compromise, team burnout | Phased approach, parallel workstreams, buffer time | Scope reduction, timeline extension, additional resources |\n| **Data Integrity** | HIGH | Business disruption, compliance issues | Comprehensive testing, automated validation, parallel runs | Rollback procedures, extended parallel operations |\n| **Regulatory Compliance** | HIGH | Legal/financial penalties | Early compliance validation, audit trail implementation | Compliance specialist engagement, regulatory consultation |\n| **Technical Complexity** | MEDIUM | Performance issues, rework | Proof of concepts, performance testing, expert review | Architecture redesign, additional optimization phases |\n| **Change Management** | MEDIUM | User adoption issues, resistance | Training programs, communication plan, stakeholder engagement | Extended support period, additional training resources |\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:30:36 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n  RISK ASSESSMENT:\n==================================================\n**CRITICAL RISKS (Immediate Action Required):**\n\n1. **Regulatory Compliance Risk - VERY HIGH**\n   - **Likelihood:** High | **Impact:** Severe\n   - **Details:** Financial services face stringent regulations (SOX, Basel III, GDPR, etc.). Data lineage, audit trails, and compliance controls must be maintained throughout migration.\n   - **Mitigation:** Engage compliance team from Week 1, implement comprehensive audit logging, establish regulatory sandbox environment, obtain pre-approval for architecture changes.\n   - **Contingency:** Dedicated compliance workstream, external regulatory consultant, extended parallel operations until full compliance validation.\n\n2. **Data Integrity & Business Continuity Risk - VERY HIGH**\n   - **Likelihood:** Medium-High | **Impact:** Severe\n   - **Details:** 2TB of financial data with 50+ pipelines creates multiple failure points. Any data corruption or loss could impact trading, reporting, and regulatory submissions.\n   - **Mitigation:** Implement automated data validation frameworks, maintain parallel Oracle systems during transition, establish real-time data reconciliation processes.\n   - **Contingency:** Immediate rollback procedures, extended parallel operations, emergency data recovery protocols.\n\n3. **Timeline Compression Risk - HIGH**\n   - **Likelihood:** High | **Impact:** High\n   - **Details:** 6-month timeline for 50+ pipelines is aggressive, especially with team skill gaps and regulatory requirements.\n   - **Mitigation:** Increase external consultant ratio to 60/40, implement parallel migration streams, pre-approve timeline extensions.\n   - **Contingency:** Scope reduction to critical pipelines only, 3-month timeline extension, emergency resource augmentation.\n\n**SIGNIFICANT RISKS (Close Monitoring Required):**\n\n4. **Skill Gap & Knowledge Transfer Risk - HIGH**\n   - **Likelihood:** High | **Impact:** Medium-High\n   - **Details:** Limited Databricks experience could lead to suboptimal architecture, performance issues, and maintenance challenges.\n   - **Mitigation:** Extend consultant engagement through stabilization phase, implement formal knowledge transfer sessions, create comprehensive documentation.\n   - **Contingency:** Long-term consultant retention, additional training budget, phased knowledge transfer approach.\n\n5. **Performance & Scalability Risk - MEDIUM-HIGH**\n   - **Likelihood:** Medium | **Impact:** High\n   - **Details:** Financial systems require sub-second response times for trading and real-time reporting. Databricks performance must match or exceed Oracle.\n   - **Mitigation:** Conduct performance benchmarking in Phase 1, implement auto-scaling configurations, optimize data partitioning strategies.\n   - **Contingency:** Architecture redesign, additional compute resources, performance optimization specialists.\n\n6. **Security & Access Control Risk - MEDIUM-HIGH**\n   - **Likelihood:** Medium | **Impact:** High\n   - **Details:** Financial data requires sophisticated access controls, encryption, and monitoring. Azure/Databricks security model differs significantly from Oracle.\n   - **Mitigation:** Implement Unity Catalog with fine-grained permissions, establish comprehensive monitoring, conduct security audits.\n   - **Contingency:** Security specialist engagement, additional security tooling, extended security validation period.\n\n**MODERATE RISKS (Standard Mitigation):**\n\n7. **Vendor Lock-in & Cost Escalation Risk - MEDIUM**\n   - **Likelihood:** Medium | **Impact:** Medium\n   - **Details:** Databricks and Azure costs may escalate beyond projections, creating budget overruns.\n   - **Mitigation:** Implement cost monitoring dashboards, establish usage governance policies, negotiate volume discounts.\n   - **Contingency:** Multi-cloud strategy evaluation, cost optimization reviews, budget reallocation procedures.\n\n8. **Change Management & User Adoption Risk - MEDIUM**\n   - **Likelihood:** Medium | **Impact:** Medium\n   - **Details:** End users and analysts may resist new tools and processes, impacting productivity.\n   - **Mitigation:** Early user engagement, comprehensive training programs, champion user identification.\n   - **Contingency:** Extended support period, additional training resources, gradual transition approach.\n\n**RECOMMENDED RISK MITIGATION ACTIONS:**\n- Establish weekly risk review meetings with executive sponsorship\n- Create dedicated compliance and security workstreams\n- Implement automated monitoring and alerting for all risk categories\n- Develop detailed rollback procedures for each migration phase\n- Establish emergency response protocols for critical system failures\n- Negotiate flexible consultant agreements for rapid scaling\n\n Planning session completed!\n Summary:\n- Total questions answered: 9\n- Categories covered: {'Resource', 'Scope'}\n- Key insights gathered: 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-c8208a074b95786d88e38d75ea2db04c\", \"tr-f1d6ecc0fb4a42050c7e30b698877d5c\", \"tr-f18c11c9c2cf3bbdf9253691403ab16f\", \"tr-afc216375233aaac4cceebda14bc5314\", \"tr-1954b394b5b407196857865a9a1eb2f4\", \"tr-c0ac3a7d6449aa9123c17f3997989fe8\", \"tr-6981523c71805f3a88e17d8126a84f1c\", \"tr-c7e29f784cf826c314fdd6914ce37bfe\", \"tr-12fbf73634c5ab7be0c038a83ec8703d\", \"tr-170493683ae15c873e27a32554f76302\"]",
      "text/plain": [
       "[Trace(trace_id=tr-c8208a074b95786d88e38d75ea2db04c), Trace(trace_id=tr-f1d6ecc0fb4a42050c7e30b698877d5c), Trace(trace_id=tr-f18c11c9c2cf3bbdf9253691403ab16f), Trace(trace_id=tr-afc216375233aaac4cceebda14bc5314), Trace(trace_id=tr-1954b394b5b407196857865a9a1eb2f4), Trace(trace_id=tr-c0ac3a7d6449aa9123c17f3997989fe8), Trace(trace_id=tr-6981523c71805f3a88e17d8126a84f1c), Trace(trace_id=tr-c7e29f784cf826c314fdd6914ce37bfe), Trace(trace_id=tr-12fbf73634c5ab7be0c038a83ec8703d), Trace(trace_id=tr-170493683ae15c873e27a32554f76302)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer Scope questions\n",
    "scope_answers = [\n",
    "    (\"How many pipelines are to be migrated?\", \"We have approximately 50 data pipelines that need to be migrated\"),\n",
    "    (\"Volume of data to be migrated?\", \"The total data volume is about 2TB across multiple tables and schemas\"),\n",
    "    (\"Is lift and shift or redesign preferred?\", \"We prefer a hybrid approach - lift and shift for simple pipelines, redesign for complex ones\"),\n",
    "    (\"Does the migration include monitoring?\", \"Yes, we need comprehensive monitoring and alerting for the new system\"),\n",
    "    (\"Will it be run in parallel or phased move over?\", \"We plan to do a phased migration over 6 months, starting with non-critical pipelines\")\n",
    "]\n",
    "\n",
    "print(\" Processing Scope Questions...\")\n",
    "for question, answer in scope_answers:\n",
    "    agent.answer_question(question, answer, \"Scope\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generating Project Plan...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate the final project plan\n",
    "final_plan = agent.generate_project_plan(timeline_requirements)\n",
    "\n",
    "print(\"\\n Planning session completed!\")\n",
    "print(\" Summary:\")\n",
    "print(f\"- Total questions answered: {len(agent.conversation_history)}\")\n",
    "print(f\"- Categories covered: {set(entry['category'] for entry in agent.conversation_history)}\")\n",
    "print(f\"- Key insights gathered: {len(agent.gathered_insights)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752566dd-6d24-4570-a95d-2f281b519fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Interactive Planning Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c43b36-8379-408f-80c3-81c2c208ddb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive planning interface ready!\nTo start an interactive session, uncomment and run: interactive_planning_session()\n"
     ]
    }
   ],
   "source": [
    "def interactive_planning_session():\n",
    "    \"\"\"Interactive function for users to run their own planning sessions.\"\"\"\n",
    "    \n",
    "    print(\" Welcome to the Delivery Planning Agent!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get project context from user\n",
    "    project_context = input(\"Please describe your project context: \")\n",
    "    timeline_requirements = input(\"Any specific timeline requirements? (press Enter to skip): \")\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = DeliveryPlanningAgent(VECTOR_SEARCH_ENDPOINT_NAME, VECTOR_SEARCH_INDEX_NAME)\n",
    "    \n",
    "    # Start planning session\n",
    "    questions = agent.start_planning_session(project_context, timeline_requirements)\n",
    "    \n",
    "    print(\"\\n Please answer the questions above. Type 'next' to move to the next category, or 'plan' to generate the project plan.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYour response (or 'next'/'plan'): \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'next':\n",
    "            next_questions = agent.get_next_category_questions()\n",
    "            if next_questions is None:\n",
    "                print(\" All categories completed! Type 'plan' to generate your project plan.\")\n",
    "            continue\n",
    "        \n",
    "        elif user_input.lower() == 'plan':\n",
    "            final_plan = agent.generate_project_plan(timeline_requirements)\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            # This is a simplified version - in practice, you'd need to match the response to the current question\n",
    "            print(\"Please provide a more specific answer or use 'next'/'plan' commands.\")\n",
    "    \n",
    "    return final_plan\n",
    "\n",
    "# Uncomment the line below to run an interactive session\n",
    "# interactive_planning_session()\n",
    "\n",
    "print(\"Interactive planning interface ready!\")\n",
    "print(\"To start an interactive session, uncomment and run: interactive_planning_session()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0560fc43-1ad8-4ed9-ae38-9902ee9f28e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to the Delivery Planning Agent!\n==================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Please describe your project context:  CENTRICA ARE doing a hive to uc migration"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Any specific timeline requirements? (press Enter to skip):  6 months"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Delivery Planning Session\n Project Context: CENTRICA ARE doing a hive to uc migration\n⏰ Timeline Requirements: 6 months\n\n==================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:36:08 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Resource Questions:\n------------------------------\n1. 1. What is the current scale of your Hive environment (number of databases, tables, data volume in TB/PB, and daily query volume) that needs to be migrated to Unity Catalog?\n3. 2. What technical expertise does your team currently have with Databricks, Unity Catalog, and Spark, and what training or additional resources will be needed to support the migration?\n5. 3. What is your allocated budget and timeline for this Hive to Unity Catalog migration project, including any constraints or dependencies?\n7. 4. What are your current compute and storage resources for the Hive environment, and how do you plan to provision Databricks clusters and storage for the migrated workloads?\n9. 5. Who are the key stakeholders and project team members assigned to this migration, and what is their availability and role in the project execution?\n\n Please answer the questions above. Type 'next' to move to the next category, or 'plan' to generate the project plan.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\n",
       "Your response (or 'next'/'plan'):  plan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Generating Project Plan...\n==================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:37:18 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROJECT PLAN:\n==================================================\n## Migration Timeline\n\n| Phase | Duration | Start Week | End Week | Key Deliverables |\n|-------|----------|------------|----------|------------------|\n| Assessment & Planning | 4 weeks | Week 1 | Week 4 | Current state analysis, migration strategy, resource allocation |\n| Environment Setup | 3 weeks | Week 3 | Week 5 | Unity Catalog workspace setup, security configuration |\n| Pilot Migration | 4 weeks | Week 6 | Week 9 | Migrate 2-3 critical datasets, validate processes |\n| Bulk Migration Phase 1 | 6 weeks | Week 10 | Week 15 | Migrate 60% of datasets and workflows |\n| Bulk Migration Phase 2 | 6 weeks | Week 16 | Week 21 | Complete remaining migrations |\n| Testing & Validation | 3 weeks | Week 22 | Week 24 | End-to-end testing, performance validation |\n| Go-Live & Support | 2 weeks | Week 25 | Week 26 | Production cutover, hypercare support |\n\n## Resource Requirements\n\n| Role | FTE | Duration | Responsibilities |\n|------|-----|----------|------------------|\n| Migration Lead | 1.0 | 26 weeks | Overall project coordination, stakeholder management |\n| Databricks Architect | 1.0 | 26 weeks | Unity Catalog design, security implementation |\n| Data Engineers | 3.0 | 20 weeks | ETL migration, data pipeline conversion |\n| DevOps Engineer | 0.5 | 16 weeks | Infrastructure setup, CI/CD pipeline configuration |\n| Data Governance Specialist | 0.5 | 26 weeks | Metadata management, compliance validation |\n| QA/Testing Specialist | 1.0 | 12 weeks | Migration validation, performance testing |\n| Business Analysts | 2.0 | 8 weeks | Requirements gathering, user acceptance testing |\n\n## Migration Phases\n\n| Phase | Scope | Key Activities | Success Criteria |\n|-------|-------|----------------|------------------|\n| **Phase 1: Foundation** | Infrastructure & Governance | • Unity Catalog workspace setup<br>• Security model implementation<br>• Metadata catalog creation<br>• Access control configuration | • UC workspace operational<br>• Security policies implemented<br>• Metadata framework established |\n| **Phase 2: Pilot** | Critical datasets (10-15%) | • Select high-impact, low-risk datasets<br>• Migrate schemas and data<br>• Convert ETL processes<br>• User training and feedback | • Pilot datasets fully functional<br>• Performance benchmarks met<br>• User acceptance achieved |\n| **Phase 3: Bulk Migration** | Remaining datasets (85-90%) | • Automated migration tools deployment<br>• Batch data migration<br>• Workflow conversion<br>• Incremental testing | • All datasets migrated<br>• Data integrity validated<br>• Workflows operational |\n| **Phase 4: Optimization** | Performance & Adoption | • Performance tuning<br>• User training completion<br>• Documentation finalization<br>• Decommission legacy systems | • Performance targets met<br>• Users fully trained<br>• Legacy systems retired |\n\n## Risk Assessment\n\n| Risk Category | Risk Level | Impact | Mitigation Strategy |\n|---------------|------------|--------|-------------------|\n| **Data Loss/Corruption** | High | Critical | • Implement comprehensive backup strategy<br>• Parallel run validation<br>• Incremental migration approach |\n| **Performance Degradation** | Medium | High | • Conduct performance benchmarking<br>• Optimize cluster configurations<br>• Implement caching strategies |\n| **Security Compliance** | Medium | Critical | • Early security review and approval<br>• Implement least privilege access<br>• Regular security audits |\n| **User Adoption** | Medium | Medium | • Comprehensive training program<br>• Change management support<br>• Gradual rollout approach |\n| **Timeline Delays** | Medium | Medium | • Buffer time in critical phases<br>• Parallel workstream execution<br>• Regular milestone reviews |\n| **Integration Issues** | Low | High | • Early integration testing<br>• API compatibility validation<br>• Fallback procedures |\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:37:42 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"[[ ## re...er_specific_fields=None), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n  RISK ASSESSMENT:\n==================================================\n**CRITICAL RISKS:**\n\n**1. Regulatory Compliance & Data Sovereignty (HIGH RISK - CRITICAL IMPACT)**\n- **Risk**: Energy sector regulations (GDPR, industry-specific compliance) may have specific data residency and governance requirements that Unity Catalog configuration might not initially meet\n- **Likelihood**: Medium-High (energy companies face strict regulatory oversight)\n- **Mitigation**: \n  - Conduct early regulatory compliance assessment with legal/compliance teams\n  - Engage Databricks compliance specialists for energy sector requirements\n  - Implement data classification and lineage tracking from day one\n  - Plan for regulatory audit trails and data retention policies\n\n**2. Business Continuity During Migration (HIGH RISK - CRITICAL IMPACT)**\n- **Risk**: Centrica's energy operations likely run 24/7 with real-time data needs; any disruption could impact energy supply or trading operations\n- **Likelihood**: Medium\n- **Mitigation**:\n  - Implement zero-downtime migration strategy with real-time data replication\n  - Establish rollback procedures for each migration phase\n  - Create detailed business continuity plans with energy operations teams\n  - Schedule migrations during low-impact periods (if any exist)\n\n**3. Data Volume & Complexity Underestimation (MEDIUM-HIGH RISK - HIGH IMPACT)**\n- **Risk**: Energy companies typically have massive datasets (smart meter data, trading data, operational data) that may exceed migration timeline estimates\n- **Likelihood**: High\n- **Mitigation**:\n  - Conduct thorough data volume assessment including growth projections\n  - Implement data archiving strategy for historical data\n  - Use parallel processing and optimize network bandwidth\n  - Consider phased data migration with most recent data prioritized\n\n**4. Integration with Energy Trading Systems (HIGH RISK - CRITICAL IMPACT)**\n- **Risk**: Real-time trading systems and market data feeds may have complex integration requirements that could fail during migration\n- **Likelihood**: Medium\n- **Mitigation**:\n  - Map all external system dependencies early in assessment phase\n  - Conduct integration testing with trading system vendors\n  - Maintain parallel systems during critical trading periods\n  - Establish emergency procedures for trading system failures\n\n**ADDITIONAL RISKS:**\n\n**5. Skills Gap in Energy Domain Knowledge (MEDIUM RISK - MEDIUM IMPACT)**\n- **Risk**: Migration team may lack understanding of energy industry data patterns and business logic\n- **Mitigation**:\n  - Include energy domain experts in migration team\n  - Conduct knowledge transfer sessions with business users\n  - Document energy-specific data transformation rules\n\n**6. Performance Impact on Real-time Analytics (MEDIUM RISK - HIGH IMPACT)**\n- **Risk**: Energy forecasting and grid management systems require real-time performance that may be affected\n- **Mitigation**:\n  - Establish performance SLAs specific to energy operations\n  - Implement monitoring for real-time data pipelines\n  - Create performance optimization playbooks\n\n**7. Vendor Lock-in and Exit Strategy (LOW-MEDIUM RISK - MEDIUM IMPACT)**\n- **Risk**: Heavy dependency on Databricks Unity Catalog without clear exit strategy\n- **Mitigation**:\n  - Document data export procedures\n  - Maintain metadata in portable formats\n  - Negotiate favorable contract terms with Databricks\n\n**ENHANCED RECOMMENDATIONS:**\n- Extend assessment phase to 6 weeks to accommodate energy sector complexity\n- Add dedicated energy domain expert to the team (0.5 FTE)\n- Implement 24/7 monitoring and support during migration phases\n- Create specific rollback procedures for each critical energy system\n- Establish direct communication channels with Centrica's energy operations control room\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'project_plan': '## Migration Timeline\\n\\n| Phase | Duration | Start Week | End Week | Key Deliverables |\\n|-------|----------|------------|----------|------------------|\\n| Assessment & Planning | 4 weeks | Week 1 | Week 4 | Current state analysis, migration strategy, resource allocation |\\n| Environment Setup | 3 weeks | Week 3 | Week 5 | Unity Catalog workspace setup, security configuration |\\n| Pilot Migration | 4 weeks | Week 6 | Week 9 | Migrate 2-3 critical datasets, validate processes |\\n| Bulk Migration Phase 1 | 6 weeks | Week 10 | Week 15 | Migrate 60% of datasets and workflows |\\n| Bulk Migration Phase 2 | 6 weeks | Week 16 | Week 21 | Complete remaining migrations |\\n| Testing & Validation | 3 weeks | Week 22 | Week 24 | End-to-end testing, performance validation |\\n| Go-Live & Support | 2 weeks | Week 25 | Week 26 | Production cutover, hypercare support |\\n\\n## Resource Requirements\\n\\n| Role | FTE | Duration | Responsibilities |\\n|------|-----|----------|------------------|\\n| Migration Lead | 1.0 | 26 weeks | Overall project coordination, stakeholder management |\\n| Databricks Architect | 1.0 | 26 weeks | Unity Catalog design, security implementation |\\n| Data Engineers | 3.0 | 20 weeks | ETL migration, data pipeline conversion |\\n| DevOps Engineer | 0.5 | 16 weeks | Infrastructure setup, CI/CD pipeline configuration |\\n| Data Governance Specialist | 0.5 | 26 weeks | Metadata management, compliance validation |\\n| QA/Testing Specialist | 1.0 | 12 weeks | Migration validation, performance testing |\\n| Business Analysts | 2.0 | 8 weeks | Requirements gathering, user acceptance testing |\\n\\n## Migration Phases\\n\\n| Phase | Scope | Key Activities | Success Criteria |\\n|-------|-------|----------------|------------------|\\n| **Phase 1: Foundation** | Infrastructure & Governance | • Unity Catalog workspace setup<br>• Security model implementation<br>• Metadata catalog creation<br>• Access control configuration | • UC workspace operational<br>• Security policies implemented<br>• Metadata framework established |\\n| **Phase 2: Pilot** | Critical datasets (10-15%) | • Select high-impact, low-risk datasets<br>• Migrate schemas and data<br>• Convert ETL processes<br>• User training and feedback | • Pilot datasets fully functional<br>• Performance benchmarks met<br>• User acceptance achieved |\\n| **Phase 3: Bulk Migration** | Remaining datasets (85-90%) | • Automated migration tools deployment<br>• Batch data migration<br>• Workflow conversion<br>• Incremental testing | • All datasets migrated<br>• Data integrity validated<br>• Workflows operational |\\n| **Phase 4: Optimization** | Performance & Adoption | • Performance tuning<br>• User training completion<br>• Documentation finalization<br>• Decommission legacy systems | • Performance targets met<br>• Users fully trained<br>• Legacy systems retired |\\n\\n## Risk Assessment\\n\\n| Risk Category | Risk Level | Impact | Mitigation Strategy |\\n|---------------|------------|--------|-------------------|\\n| **Data Loss/Corruption** | High | Critical | • Implement comprehensive backup strategy<br>• Parallel run validation<br>• Incremental migration approach |\\n| **Performance Degradation** | Medium | High | • Conduct performance benchmarking<br>• Optimize cluster configurations<br>• Implement caching strategies |\\n| **Security Compliance** | Medium | Critical | • Early security review and approval<br>• Implement least privilege access<br>• Regular security audits |\\n| **User Adoption** | Medium | Medium | • Comprehensive training program<br>• Change management support<br>• Gradual rollout approach |\\n| **Timeline Delays** | Medium | Medium | • Buffer time in critical phases<br>• Parallel workstream execution<br>• Regular milestone reviews |\\n| **Integration Issues** | Low | High | • Early integration testing<br>• API compatibility validation<br>• Fallback procedures |',\n",
       " 'risk_assessment': \"**CRITICAL RISKS:**\\n\\n**1. Regulatory Compliance & Data Sovereignty (HIGH RISK - CRITICAL IMPACT)**\\n- **Risk**: Energy sector regulations (GDPR, industry-specific compliance) may have specific data residency and governance requirements that Unity Catalog configuration might not initially meet\\n- **Likelihood**: Medium-High (energy companies face strict regulatory oversight)\\n- **Mitigation**: \\n  - Conduct early regulatory compliance assessment with legal/compliance teams\\n  - Engage Databricks compliance specialists for energy sector requirements\\n  - Implement data classification and lineage tracking from day one\\n  - Plan for regulatory audit trails and data retention policies\\n\\n**2. Business Continuity During Migration (HIGH RISK - CRITICAL IMPACT)**\\n- **Risk**: Centrica's energy operations likely run 24/7 with real-time data needs; any disruption could impact energy supply or trading operations\\n- **Likelihood**: Medium\\n- **Mitigation**:\\n  - Implement zero-downtime migration strategy with real-time data replication\\n  - Establish rollback procedures for each migration phase\\n  - Create detailed business continuity plans with energy operations teams\\n  - Schedule migrations during low-impact periods (if any exist)\\n\\n**3. Data Volume & Complexity Underestimation (MEDIUM-HIGH RISK - HIGH IMPACT)**\\n- **Risk**: Energy companies typically have massive datasets (smart meter data, trading data, operational data) that may exceed migration timeline estimates\\n- **Likelihood**: High\\n- **Mitigation**:\\n  - Conduct thorough data volume assessment including growth projections\\n  - Implement data archiving strategy for historical data\\n  - Use parallel processing and optimize network bandwidth\\n  - Consider phased data migration with most recent data prioritized\\n\\n**4. Integration with Energy Trading Systems (HIGH RISK - CRITICAL IMPACT)**\\n- **Risk**: Real-time trading systems and market data feeds may have complex integration requirements that could fail during migration\\n- **Likelihood**: Medium\\n- **Mitigation**:\\n  - Map all external system dependencies early in assessment phase\\n  - Conduct integration testing with trading system vendors\\n  - Maintain parallel systems during critical trading periods\\n  - Establish emergency procedures for trading system failures\\n\\n**ADDITIONAL RISKS:**\\n\\n**5. Skills Gap in Energy Domain Knowledge (MEDIUM RISK - MEDIUM IMPACT)**\\n- **Risk**: Migration team may lack understanding of energy industry data patterns and business logic\\n- **Mitigation**:\\n  - Include energy domain experts in migration team\\n  - Conduct knowledge transfer sessions with business users\\n  - Document energy-specific data transformation rules\\n\\n**6. Performance Impact on Real-time Analytics (MEDIUM RISK - HIGH IMPACT)**\\n- **Risk**: Energy forecasting and grid management systems require real-time performance that may be affected\\n- **Mitigation**:\\n  - Establish performance SLAs specific to energy operations\\n  - Implement monitoring for real-time data pipelines\\n  - Create performance optimization playbooks\\n\\n**7. Vendor Lock-in and Exit Strategy (LOW-MEDIUM RISK - MEDIUM IMPACT)**\\n- **Risk**: Heavy dependency on Databricks Unity Catalog without clear exit strategy\\n- **Mitigation**:\\n  - Document data export procedures\\n  - Maintain metadata in portable formats\\n  - Negotiate favorable contract terms with Databricks\\n\\n**ENHANCED RECOMMENDATIONS:**\\n- Extend assessment phase to 6 weeks to accommodate energy sector complexity\\n- Add dedicated energy domain expert to the team (0.5 FTE)\\n- Implement 24/7 monitoring and support during migration phases\\n- Create specific rollback procedures for each critical energy system\\n- Establish direct communication channels with Centrica's energy operations control room\",\n",
       " 'conversation_history': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-7c461b1c9caa328c0899f2d88ac3e25f\", \"tr-92b50ce91a51d9867e08cbd2ac0188c8\", \"tr-d7fd6f7089bb781b15e27b7a18ce17af\"]",
      "text/plain": [
       "[Trace(trace_id=tr-7c461b1c9caa328c0899f2d88ac3e25f), Trace(trace_id=tr-92b50ce91a51d9867e08cbd2ac0188c8), Trace(trace_id=tr-d7fd6f7089bb781b15e27b7a18ce17af)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_planning_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fad7dbe-dc28-4508-b653-2f2c50c8f8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 09:37:43 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://adb-984752964297111.11.azuredatabricks.net/ml/experiments/3013595711630001/models/m-e7aa2164dc0b41a487fedc83e20fd365?o=984752964297111\n2025/09/16 09:38:08 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-e7aa2164dc0b41a487fedc83e20fd365\n2025/09/16 09:38:08 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Processing Answer:\nQ: [{'role': 'user', 'content': 'How many team members are there for the Databricks migration? Category: Resource. Context: We have 5 team members.'}]\nA: \n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<head>\n",
       "  <link\n",
       "    rel=\"stylesheet\"\n",
       "    href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/xcode.min.css\"\n",
       "  />\n",
       "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js\"></script>\n",
       "  <script>\n",
       "    hljs.highlightAll();\n",
       "  </script>\n",
       "  <style>\n",
       "    body {\n",
       "      margin: 0;\n",
       "      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto,\n",
       "        Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji,\n",
       "        Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;\n",
       "      -webkit-tap-highlight-color: rgba(0, 0, 0, 0);\n",
       "      margin: 0;\n",
       "      font-weight: 400;\n",
       "      font-size: 13px;\n",
       "      line-height: 18px;\n",
       "      color: rgb(17, 23, 28);\n",
       "    }\n",
       "    code {\n",
       "      line-height: 18px;\n",
       "      font-size: 11px;\n",
       "      background: rgb(250, 250, 250) !important;\n",
       "    }\n",
       "    pre {\n",
       "      background: rgb(250, 250, 250);\n",
       "      margin: 0;\n",
       "      display: none;\n",
       "    }\n",
       "    pre.active {\n",
       "      display: unset;\n",
       "    }\n",
       "    button {\n",
       "      white-space: nowrap;\n",
       "      text-align: center;\n",
       "      position: relative;\n",
       "      cursor: pointer;\n",
       "      background: rgba(34, 114, 180, 0) !important;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      border-color: rgba(34, 114, 180, 0) !important;\n",
       "      padding: 4px 6px !important;\n",
       "      text-decoration: none !important;\n",
       "      line-height: 20px !important;\n",
       "      box-shadow: none !important;\n",
       "      height: 32px !important;\n",
       "      display: inline-flex !important;\n",
       "      -webkit-box-align: center !important;\n",
       "      align-items: center !important;\n",
       "      -webkit-box-pack: center !important;\n",
       "      justify-content: center !important;\n",
       "      vertical-align: middle !important;\n",
       "    }\n",
       "    p {\n",
       "      margin: 0;\n",
       "      padding: 0;\n",
       "    }\n",
       "    button:hover {\n",
       "      background: rgba(34, 114, 180, 0.08) !important;\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    button:active {\n",
       "      background: rgba(34, 114, 180, 0.16) !important;\n",
       "      color: rgb(4, 53, 93) !important;\n",
       "    }\n",
       "    h1 {\n",
       "      margin-top: 4px;\n",
       "      font-size: 22px;\n",
       "    }\n",
       "    .info {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      line-height: 16px;\n",
       "      color: rgb(95, 114, 129);\n",
       "    }\n",
       "    .tabs {\n",
       "      margin-top: 10px;\n",
       "      border-bottom: 1px solid rgb(209, 217, 225) !important;\n",
       "      display: flex;\n",
       "      line-height: 24px;\n",
       "    }\n",
       "    .tab {\n",
       "      font-size: 13px;\n",
       "      font-weight: 600 !important;\n",
       "      cursor: pointer;\n",
       "      margin: 0 24px 0 2px;\n",
       "      padding-left: 2px;\n",
       "    }\n",
       "    .tab:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .tab.active {\n",
       "      border-bottom: 3px solid rgb(34, 114, 180) !important;\n",
       "    }\n",
       "    .link {\n",
       "      margin-left: 12px;\n",
       "      display: inline-block;\n",
       "      text-decoration: none;\n",
       "      color: rgb(34, 114, 180) !important;\n",
       "      font-size: 13px;\n",
       "      font-weight: 400;\n",
       "    }\n",
       "    .link:hover {\n",
       "      color: rgb(14, 83, 139) !important;\n",
       "    }\n",
       "    .link-content {\n",
       "      display: flex;\n",
       "      gap: 6px;\n",
       "      align-items: center;\n",
       "    }\n",
       "    .caret-up {\n",
       "      transform: rotate(180deg);\n",
       "    }\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "  <div style=\"display: flex; align-items: center\">\n",
       "    The logged model is compatible with the Mosaic AI Agent Framework.\n",
       "    <button onclick=\"toggleCode()\">\n",
       "      See how to evaluate the model&nbsp;\n",
       "      <span\n",
       "        role=\"img\"\n",
       "        id=\"caret\"\n",
       "        aria-hidden=\"true\"\n",
       "        class=\"anticon css-6xix1i\"\n",
       "        style=\"font-size: 14px\"\n",
       "        ><svg\n",
       "          xmlns=\"http://www.w3.org/2000/svg\"\n",
       "          width=\"1em\"\n",
       "          height=\"1em\"\n",
       "          fill=\"none\"\n",
       "          viewBox=\"0 0 16 16\"\n",
       "          aria-hidden=\"true\"\n",
       "          focusable=\"false\"\n",
       "          class=\"\"\n",
       "        >\n",
       "          <path\n",
       "            fill=\"currentColor\"\n",
       "            fill-rule=\"evenodd\"\n",
       "            d=\"M8 8.917 10.947 6 12 7.042 8 11 4 7.042 5.053 6z\"\n",
       "            clip-rule=\"evenodd\"\n",
       "          ></path>\n",
       "        </svg>\n",
       "      </span>\n",
       "    </button>\n",
       "  </div>\n",
       "  <div id=\"code\" style=\"display: none\">\n",
       "    <h1>\n",
       "      Agent evaluation\n",
       "      <a\n",
       "        class=\"link\"\n",
       "        href=\"https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html?utm_source=mlflow.log_model&utm_medium=notebook\"\n",
       "        target=\"_blank\"\n",
       "      >\n",
       "        <span class=\"link-content\">\n",
       "          Learn more\n",
       "          <span role=\"img\" aria-hidden=\"true\" class=\"anticon css-6xix1i\"\n",
       "            ><svg\n",
       "              xmlns=\"http://www.w3.org/2000/svg\"\n",
       "              width=\"1em\"\n",
       "              height=\"1em\"\n",
       "              fill=\"none\"\n",
       "              viewBox=\"0 0 16 16\"\n",
       "              aria-hidden=\"true\"\n",
       "              focusable=\"false\"\n",
       "              class=\"\"\n",
       "            >\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"\n",
       "              ></path>\n",
       "              <path\n",
       "                fill=\"currentColor\"\n",
       "                d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"\n",
       "              ></path></svg></span></span\n",
       "      ></a>\n",
       "    </h1>\n",
       "    <p class=\"info\">\n",
       "      Copy the following code snippet in a notebook cell (right click → copy)\n",
       "    </p>\n",
       "    <div class=\"tabs\">\n",
       "      <div class=\"tab active\" onclick=\"tabClicked(0)\">Using synthetic data</div>\n",
       "      <div class=\"tab\" onclick=\"tabClicked(1)\">Using your own dataset</div>\n",
       "    </div>\n",
       "    <div style=\"height: 472px\">\n",
       "      <pre\n",
       "        class=\"active\"\n",
       "      ><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "from databricks.agents.evals import generate_evals_df\n",
       "import mlflow\n",
       "\n",
       "agent_description = &quot;A chatbot that answers questions about Databricks.&quot;\n",
       "question_guidelines = &quot;&quot;&quot;\n",
       "# User personas\n",
       "- A developer new to the Databricks platform\n",
       "# Example questions\n",
       "- What API lets me parallelize operations over rows of a delta table?\n",
       "&quot;&quot;&quot;\n",
       "# TODO: Spark/Pandas DataFrame with &quot;content&quot; and &quot;doc_uri&quot; columns.\n",
       "docs = spark.table(&quot;catalog.schema.my_table_of_docs&quot;)\n",
       "evals = generate_evals_df(\n",
       "    docs=docs,\n",
       "    num_evals=25,\n",
       "    agent_description=agent_description,\n",
       "    question_guidelines=question_guidelines,\n",
       ")\n",
       "eval_result = mlflow.evaluate(data=evals, model=&quot;models:/m-e7aa2164dc0b41a487fedc83e20fd365&quot;, model_type=&quot;databricks-agent&quot;)\n",
       "</code></pre>\n",
       "\n",
       "      <pre><code class=\"language-python\">%pip install -U databricks-agents\n",
       "dbutils.library.restartPython()\n",
       "## Run the above in a separate cell ##\n",
       "\n",
       "import pandas as pd\n",
       "import mlflow\n",
       "\n",
       "evals = [\n",
       "    {\n",
       "        &quot;request&quot;: {\n",
       "            &quot;messages&quot;: [\n",
       "                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I convert a Spark DataFrame to Pandas?&quot;}\n",
       "            ],\n",
       "        },\n",
       "        # Optional, needed for judging correctness.\n",
       "        &quot;expected_facts&quot;: [\n",
       "            &quot;To convert a Spark DataFrame to Pandas, you can use the toPandas() method.&quot;\n",
       "        ],\n",
       "    }\n",
       "]\n",
       "eval_result = mlflow.evaluate(\n",
       "    data=pd.DataFrame.from_records(evals), model=&quot;models:/m-e7aa2164dc0b41a487fedc83e20fd365&quot;, model_type=&quot;databricks-agent&quot;\n",
       ")\n",
       "</code></pre>\n",
       "    </div>\n",
       "  </div>\n",
       "  <script>\n",
       "    var codeShown = false;\n",
       "    function clip(el) {\n",
       "      var range = document.createRange();\n",
       "      range.selectNodeContents(el);\n",
       "      var sel = window.getSelection();\n",
       "      sel.removeAllRanges();\n",
       "      sel.addRange(range);\n",
       "    }\n",
       "\n",
       "    function toggleCode() {\n",
       "      if (codeShown) {\n",
       "        document.getElementById(\"code\").style.display = \"none\";\n",
       "        codeShown = false;\n",
       "      } else {\n",
       "        document.getElementById(\"code\").style.display = \"block\";\n",
       "        clip(document.querySelector(\"pre.active\"));\n",
       "        codeShown = true;\n",
       "      }\n",
       "      document.getElementById(\"caret\").classList.toggle(\"caret-up\");\n",
       "    }\n",
       "\n",
       "    function tabClicked(tabIndex) {\n",
       "      document.querySelectorAll(\".tab\").forEach((tab, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          tab.classList.add(\"active\");\n",
       "        } else {\n",
       "          tab.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      document.querySelectorAll(\"pre\").forEach((pre, index) => {\n",
       "        if (index === tabIndex) {\n",
       "          pre.classList.add(\"active\");\n",
       "        } else {\n",
       "          pre.classList.remove(\"active\");\n",
       "        }\n",
       "      });\n",
       "      clip(document.querySelector(\"pre.active\"));\n",
       "    }\n",
       "  </script>\n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Agent logged to MLflow: models:/m-e7aa2164dc0b41a487fedc83e20fd365\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRestException\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8479387074310181>, line 66\u001B[0m\n",
       "\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Register the agent in Unity Catalog\u001B[39;00m\n",
       "\u001B[1;32m     65\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.delivery-planning-agent\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 66\u001B[0m uc_model_info \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mregister_model(model_uri\u001B[38;5;241m=\u001B[39mlogged_agent_info\u001B[38;5;241m.\u001B[39mmodel_uri, name\u001B[38;5;241m=\u001B[39mmodel_name)\n",
       "\u001B[1;32m     68\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Agent registered in Unity Catalog: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muc_model_info\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     69\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m   Version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muc_model_info\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/fluent.py:129\u001B[0m, in \u001B[0;36mregister_model\u001B[0;34m(model_uri, name, await_registration_for, tags, env_pack)\u001B[0m\n",
       "\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregister_model\u001B[39m(\n",
       "\u001B[1;32m     62\u001B[0m     model_uri,\n",
       "\u001B[1;32m     63\u001B[0m     name,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     67\u001B[0m     env_pack: EnvPackType \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m     68\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ModelVersion:\n",
       "\u001B[1;32m     69\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a new model version in model registry for the model files specified by ``model_uri``.\u001B[39;00m\n",
       "\u001B[1;32m     70\u001B[0m \n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    Note that this method assumes the model registry backend URI is the same as that of the\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    127\u001B[0m \u001B[38;5;124;03m        Version: 1\u001B[39;00m\n",
       "\u001B[1;32m    128\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _register_model(\n",
       "\u001B[1;32m    130\u001B[0m         model_uri\u001B[38;5;241m=\u001B[39mmodel_uri,\n",
       "\u001B[1;32m    131\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n",
       "\u001B[1;32m    132\u001B[0m         await_registration_for\u001B[38;5;241m=\u001B[39mawait_registration_for,\n",
       "\u001B[1;32m    133\u001B[0m         tags\u001B[38;5;241m=\u001B[39mtags,\n",
       "\u001B[1;32m    134\u001B[0m         env_pack\u001B[38;5;241m=\u001B[39menv_pack,\n",
       "\u001B[1;32m    135\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/fluent.py:160\u001B[0m, in \u001B[0;36m_register_model\u001B[0;34m(model_uri, name, await_registration_for, tags, local_model_path, env_pack)\u001B[0m\n",
       "\u001B[1;32m    156\u001B[0m         eprint(\n",
       "\u001B[1;32m    157\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRegistered model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m already exists. Creating a new version of this model...\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    158\u001B[0m         )\n",
       "\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 160\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    162\u001B[0m run_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    163\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/fluent.py:149\u001B[0m, in \u001B[0;36m_register_model\u001B[0;34m(model_uri, name, await_registration_for, tags, local_model_path, env_pack)\u001B[0m\n",
       "\u001B[1;32m    147\u001B[0m client \u001B[38;5;241m=\u001B[39m MlflowClient()\n",
       "\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 149\u001B[0m     create_model_response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mcreate_registered_model(name)\n",
       "\u001B[1;32m    150\u001B[0m     eprint(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccessfully registered model \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcreate_model_response\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MlflowException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/client.py:3656\u001B[0m, in \u001B[0;36mMlflowClient.create_registered_model\u001B[0;34m(self, name, tags, description, deployment_job_id)\u001B[0m\n",
       "\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_prompt_tag(tags):\n",
       "\u001B[1;32m   3654\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException\u001B[38;5;241m.\u001B[39minvalid_parameter_value(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrompts cannot be registered as models.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m-> 3656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_registry_client()\u001B[38;5;241m.\u001B[39mcreate_registered_model(\n",
       "\u001B[1;32m   3657\u001B[0m     name, tags, description, deployment_job_id\n",
       "\u001B[1;32m   3658\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/telemetry/track.py:23\u001B[0m, in \u001B[0;36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m R:\n",
       "\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_telemetry_disabled() \u001B[38;5;129;01mor\u001B[39;00m _is_telemetry_disabled_for_event(event):\n",
       "\u001B[0;32m---> 23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     25\u001B[0m     success \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     26\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/client.py:95\u001B[0m, in \u001B[0;36mModelRegistryClient.create_registered_model\u001B[0;34m(self, name, tags, description, deployment_job_id)\u001B[0m\n",
       "\u001B[1;32m     93\u001B[0m tags \u001B[38;5;241m=\u001B[39m tags \u001B[38;5;28;01mif\u001B[39;00m tags \u001B[38;5;28;01melse\u001B[39;00m {}\n",
       "\u001B[1;32m     94\u001B[0m tags \u001B[38;5;241m=\u001B[39m [RegisteredModelTag(key, \u001B[38;5;28mstr\u001B[39m(value)) \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m tags\u001B[38;5;241m.\u001B[39mitems()]\n",
       "\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstore\u001B[38;5;241m.\u001B[39mcreate_registered_model(name, tags, description, deployment_job_id)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/store/_unity_catalog/registry/rest_store.py:461\u001B[0m, in \u001B[0;36mUcModelRegistryStore.create_registered_model\u001B[0;34m(self, name, tags, description, deployment_job_id)\u001B[0m\n",
       "\u001B[1;32m    452\u001B[0m req_body \u001B[38;5;241m=\u001B[39m message_to_json(\n",
       "\u001B[1;32m    453\u001B[0m     CreateRegisteredModelRequest(\n",
       "\u001B[1;32m    454\u001B[0m         name\u001B[38;5;241m=\u001B[39mfull_name,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    458\u001B[0m     )\n",
       "\u001B[1;32m    459\u001B[0m )\n",
       "\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 461\u001B[0m     response_proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_endpoint(CreateRegisteredModelRequest, req_body)\n",
       "\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    464\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreraise_with_legacy_hint\u001B[39m(exception, legacy_hint):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/store/model_registry/base_rest_store.py:42\u001B[0m, in \u001B[0;36mBaseRestStore._call_endpoint\u001B[0;34m(self, api, json_body, call_all_endpoints, extra_headers)\u001B[0m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     41\u001B[0m     endpoint, method \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_endpoint_from_method(api)\n",
       "\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m call_endpoint(\n",
       "\u001B[1;32m     43\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_host_creds(), endpoint, method, json_body, response_proto, extra_headers\n",
       "\u001B[1;32m     44\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:554\u001B[0m, in \u001B[0;36mcall_endpoint\u001B[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001B[0m\n",
       "\u001B[1;32m    551\u001B[0m     call_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m json_body\n",
       "\u001B[1;32m    552\u001B[0m     response \u001B[38;5;241m=\u001B[39m http_request(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcall_kwargs)\n",
       "\u001B[0;32m--> 554\u001B[0m response \u001B[38;5;241m=\u001B[39m verify_rest_response(response, endpoint)\n",
       "\u001B[1;32m    555\u001B[0m response_to_parse \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\n",
       "\u001B[1;32m    556\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:308\u001B[0m, in \u001B[0;36mverify_rest_response\u001B[0;34m(response, endpoint)\u001B[0m\n",
       "\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n",
       "\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _can_parse_as_json_object(response\u001B[38;5;241m.\u001B[39mtext):\n",
       "\u001B[0;32m--> 308\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m RestException(json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mtext))\n",
       "\u001B[1;32m    309\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    310\u001B[0m         base_msg \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    311\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI request to endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    312\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfailed with error code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != 200\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    313\u001B[0m         )\n",
       "\n",
       "\u001B[0;31mRestException\u001B[0m: PERMISSION_DENIED: User does not have CREATE MODEL on Schema 'vbdemos.usecase_agent'. Config: host=https://eastus2.azuredatabricks.net, auth_type=runtime, retry_timeout_seconds=500"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RestException",
        "evalue": "PERMISSION_DENIED: User does not have CREATE MODEL on Schema 'vbdemos.usecase_agent'. Config: host=https://eastus2.azuredatabricks.net, auth_type=runtime, retry_timeout_seconds=500"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>RestException</span>: PERMISSION_DENIED: User does not have CREATE MODEL on Schema 'vbdemos.usecase_agent'. Config: host=https://eastus2.azuredatabricks.net, auth_type=runtime, retry_timeout_seconds=500"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mRestException\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-8479387074310181>, line 66\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Register the agent in Unity Catalog\u001B[39;00m\n\u001B[1;32m     65\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.delivery-planning-agent\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 66\u001B[0m uc_model_info \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mregister_model(model_uri\u001B[38;5;241m=\u001B[39mlogged_agent_info\u001B[38;5;241m.\u001B[39mmodel_uri, name\u001B[38;5;241m=\u001B[39mmodel_name)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Agent registered in Unity Catalog: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muc_model_info\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m   Version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muc_model_info\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/fluent.py:129\u001B[0m, in \u001B[0;36mregister_model\u001B[0;34m(model_uri, name, await_registration_for, tags, env_pack)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregister_model\u001B[39m(\n\u001B[1;32m     62\u001B[0m     model_uri,\n\u001B[1;32m     63\u001B[0m     name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     67\u001B[0m     env_pack: EnvPackType \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     68\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ModelVersion:\n\u001B[1;32m     69\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a new model version in model registry for the model files specified by ``model_uri``.\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    Note that this method assumes the model registry backend URI is the same as that of the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;124;03m        Version: 1\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _register_model(\n\u001B[1;32m    130\u001B[0m         model_uri\u001B[38;5;241m=\u001B[39mmodel_uri,\n\u001B[1;32m    131\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m    132\u001B[0m         await_registration_for\u001B[38;5;241m=\u001B[39mawait_registration_for,\n\u001B[1;32m    133\u001B[0m         tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[1;32m    134\u001B[0m         env_pack\u001B[38;5;241m=\u001B[39menv_pack,\n\u001B[1;32m    135\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/fluent.py:160\u001B[0m, in \u001B[0;36m_register_model\u001B[0;34m(model_uri, name, await_registration_for, tags, local_model_path, env_pack)\u001B[0m\n\u001B[1;32m    156\u001B[0m         eprint(\n\u001B[1;32m    157\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRegistered model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m already exists. Creating a new version of this model...\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    158\u001B[0m         )\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    162\u001B[0m run_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    163\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/fluent.py:149\u001B[0m, in \u001B[0;36m_register_model\u001B[0;34m(model_uri, name, await_registration_for, tags, local_model_path, env_pack)\u001B[0m\n\u001B[1;32m    147\u001B[0m client \u001B[38;5;241m=\u001B[39m MlflowClient()\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 149\u001B[0m     create_model_response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mcreate_registered_model(name)\n\u001B[1;32m    150\u001B[0m     eprint(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccessfully registered model \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcreate_model_response\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MlflowException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/client.py:3656\u001B[0m, in \u001B[0;36mMlflowClient.create_registered_model\u001B[0;34m(self, name, tags, description, deployment_job_id)\u001B[0m\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_prompt_tag(tags):\n\u001B[1;32m   3654\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException\u001B[38;5;241m.\u001B[39minvalid_parameter_value(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrompts cannot be registered as models.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 3656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_registry_client()\u001B[38;5;241m.\u001B[39mcreate_registered_model(\n\u001B[1;32m   3657\u001B[0m     name, tags, description, deployment_job_id\n\u001B[1;32m   3658\u001B[0m )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/telemetry/track.py:23\u001B[0m, in \u001B[0;36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m R:\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_telemetry_disabled() \u001B[38;5;129;01mor\u001B[39;00m _is_telemetry_disabled_for_event(event):\n\u001B[0;32m---> 23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     25\u001B[0m     success \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/tracking/_model_registry/client.py:95\u001B[0m, in \u001B[0;36mModelRegistryClient.create_registered_model\u001B[0;34m(self, name, tags, description, deployment_job_id)\u001B[0m\n\u001B[1;32m     93\u001B[0m tags \u001B[38;5;241m=\u001B[39m tags \u001B[38;5;28;01mif\u001B[39;00m tags \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m     94\u001B[0m tags \u001B[38;5;241m=\u001B[39m [RegisteredModelTag(key, \u001B[38;5;28mstr\u001B[39m(value)) \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m tags\u001B[38;5;241m.\u001B[39mitems()]\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstore\u001B[38;5;241m.\u001B[39mcreate_registered_model(name, tags, description, deployment_job_id)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/store/_unity_catalog/registry/rest_store.py:461\u001B[0m, in \u001B[0;36mUcModelRegistryStore.create_registered_model\u001B[0;34m(self, name, tags, description, deployment_job_id)\u001B[0m\n\u001B[1;32m    452\u001B[0m req_body \u001B[38;5;241m=\u001B[39m message_to_json(\n\u001B[1;32m    453\u001B[0m     CreateRegisteredModelRequest(\n\u001B[1;32m    454\u001B[0m         name\u001B[38;5;241m=\u001B[39mfull_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    458\u001B[0m     )\n\u001B[1;32m    459\u001B[0m )\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 461\u001B[0m     response_proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_endpoint(CreateRegisteredModelRequest, req_body)\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreraise_with_legacy_hint\u001B[39m(exception, legacy_hint):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/store/model_registry/base_rest_store.py:42\u001B[0m, in \u001B[0;36mBaseRestStore._call_endpoint\u001B[0;34m(self, api, json_body, call_all_endpoints, extra_headers)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     41\u001B[0m     endpoint, method \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_endpoint_from_method(api)\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m call_endpoint(\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_host_creds(), endpoint, method, json_body, response_proto, extra_headers\n\u001B[1;32m     44\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:554\u001B[0m, in \u001B[0;36mcall_endpoint\u001B[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001B[0m\n\u001B[1;32m    551\u001B[0m     call_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m json_body\n\u001B[1;32m    552\u001B[0m     response \u001B[38;5;241m=\u001B[39m http_request(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcall_kwargs)\n\u001B[0;32m--> 554\u001B[0m response \u001B[38;5;241m=\u001B[39m verify_rest_response(response, endpoint)\n\u001B[1;32m    555\u001B[0m response_to_parse \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    556\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-955dad53-167e-4691-b82c-fb179c6805b2/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:308\u001B[0m, in \u001B[0;36mverify_rest_response\u001B[0;34m(response, endpoint)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _can_parse_as_json_object(response\u001B[38;5;241m.\u001B[39mtext):\n\u001B[0;32m--> 308\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m RestException(json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mtext))\n\u001B[1;32m    309\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    310\u001B[0m         base_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    311\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI request to endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    312\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfailed with error code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != 200\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    313\u001B[0m         )\n",
        "\u001B[0;31mRestException\u001B[0m: PERMISSION_DENIED: User does not have CREATE MODEL on Schema 'vbdemos.usecase_agent'. Config: host=https://eastus2.azuredatabricks.net, auth_type=runtime, retry_timeout_seconds=500"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deploy Agent using DSPy MLflow Integration\n",
    "import mlflow\n",
    "import mlflow.dspy\n",
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n",
    "\n",
    "# Set registry URI for Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Input example for the model\n",
    "input_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many team members are there for the Databricks migration? Category: Resource. Context: We have 5 team members.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DSPy-compatible wrapper for the agent\n",
    "class DeliveryPlanningAgentDSPy(dspy.Module):\n",
    "    \"\"\"DSPy-compatible wrapper for the Delivery Planning Agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.agent = None\n",
    "        \n",
    "    def forward(self, question, category=\"General\", context=\"\"):\n",
    "        \"\"\"Forward method for DSPy.\"\"\"\n",
    "        try:\n",
    "            if not self.agent:\n",
    "                # Initialize the agent if not already done\n",
    "                self.agent = DeliveryPlanningAgent(\n",
    "                    vector_search_endpoint=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "                    vector_search_index=VECTOR_SEARCH_INDEX_NAME\n",
    "                )\n",
    "            \n",
    "            result = self.agent.answer_question(question, context, category)\n",
    "            return dspy.Prediction(answer=result[\"answer\"])\n",
    "        except Exception as e:\n",
    "            return dspy.Prediction(answer=f\"Error: {str(e)}\")\n",
    "\n",
    "# Create the DSPy model instance\n",
    "dspy_agent = DeliveryPlanningAgentDSPy()\n",
    "\n",
    "# Log the agent using DSPy MLflow integration\n",
    "with mlflow.start_run() as run:\n",
    "    logged_agent_info = mlflow.dspy.log_model(\n",
    "        dspy_model=dspy_agent,\n",
    "        artifact_path=\"delivery-planning-agent\",\n",
    "        input_example=input_example,\n",
    "        task=\"llm/v1/chat\",\n",
    "        model_config={\n",
    "            \"vector_search_endpoint\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "            \"vector_search_index\": VECTOR_SEARCH_INDEX_NAME\n",
    "        },\n",
    "        resources=[\n",
    "            DatabricksVectorSearchIndex(index_name=VECTOR_SEARCH_INDEX_NAME),\n",
    "            DatabricksServingEndpoint(endpoint_name=\"databricks-claude-sonnet-4\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\" Agent logged to MLflow: {logged_agent_info.model_uri}\")\n",
    "\n",
    "# Register the agent in Unity Catalog\n",
    "model_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.delivery-planning-agent\"\n",
    "uc_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=model_name)\n",
    "\n",
    "print(f\" Agent registered in Unity Catalog: {uc_model_info.name}\")\n",
    "print(f\"   Version: {uc_model_info.version}\")\n",
    "\n",
    "print(f\" Agent ready for deployment!\")\n",
    "print(f\" Model URI: {logged_agent_info.model_uri}\")\n",
    "print(f\"  Registered as: {uc_model_info.name}\")\n",
    "print(f\" Next step: Deploy via Databricks Model Serving UI or API\")\n",
    "print(f\" Use the model URI to create a serving endpoint\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "usecase_delivery_planning_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}