{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Parse Document Processor and Chunker for Vector Indexing\n",
        "\n",
        "This notebook uses Databricks AI parse functions to process PDF files from a Unity Catalog volume, extracts content using AI, chunks the text, and stores the results in a Delta table for vector indexing.\n",
        "\n",
        "## Features:\n",
        "- Uses AI parse functions for intelligent document processing\n",
        "- Processes PDF files from Unity Catalog volume\n",
        "- Extracts text and structure using AI\n",
        "- Chunks text for optimal vector indexing\n",
        "- Stores results in Delta table with metadata\n",
        "- Handles multiple files in batch processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "SOURCE_VOLUME_PATH = \"/Volumes/vbdemos/dbdemos_autoloader/raw_data/usecase-planning-agent-pdf/\"\n",
        "DESTINATION_TABLE_NAME = \"vbdemos.usecase_agent.usecase_planning_agent_pdf_parsed\"\n",
        "CHUNKS_TABLE_NAME = \"vbdemos.usecase_agent.pdf_chunks\"\n",
        "\n",
        "# AI Parse parameters\n",
        "PARSE_EXTENSIONS = ['.pdf', '.jpg', '.jpeg', '.png']\n",
        "LIMIT = 1000  # Maximum number of files to process\n",
        "PARTITION_COUNT = 4  # Number of partitions for processing\n",
        "\n",
        "# Processing options\n",
        "ENABLE_CHUNKING = False  # Set to False if using models with large context windows (e.g., 272k tokens)\n",
        "CHUNK_SIZE = 500  # characters per chunk (only used if ENABLE_CHUNKING=True)\n",
        "CHUNK_OVERLAP = 50  # overlap between chunks (only used if ENABLE_CHUNKING=True)\n",
        "\n",
        "print(f\"Source volume path: {SOURCE_VOLUME_PATH}\")\n",
        "print(f\"Destination table: {DESTINATION_TABLE_NAME}\")\n",
        "print(f\"Chunks table: {CHUNKS_TABLE_NAME}\")\n",
        "print(f\"Chunking enabled: {ENABLE_CHUNKING}\")\n",
        "if ENABLE_CHUNKING:\n",
        "    print(f\"Chunk size: {CHUNK_SIZE} characters\")\n",
        "else:\n",
        "    print(\"Using full documents (no chunking) - suitable for large context window models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. AI Parse Document Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create the AI parse query\n",
        "ai_parse_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE {DESTINATION_TABLE_NAME} AS (\n",
        "-- Parse documents with ai_parse\n",
        "WITH all_files AS (\n",
        "  SELECT\n",
        "    path,\n",
        "    content\n",
        "  FROM\n",
        "    READ_FILES('{SOURCE_VOLUME_PATH}', format => 'binaryFile')\n",
        "  ORDER BY\n",
        "    path ASC\n",
        "  LIMIT {LIMIT}\n",
        "),\n",
        "repartitioned_files AS (\n",
        "  SELECT *\n",
        "  FROM all_files\n",
        "  -- Force Spark to split into partitions\n",
        "  DISTRIBUTE BY crc32(path) % {PARTITION_COUNT}\n",
        "),\n",
        "-- Parse the files using ai_parse document\n",
        "parsed_documents AS (\n",
        "  SELECT\n",
        "    path,\n",
        "    ai_parse_document(content) as parsed\n",
        "  FROM\n",
        "    repartitioned_files\n",
        "  WHERE array_contains(array{tuple(PARSE_EXTENSIONS)}, lower(regexp_extract(path, r'(\\\\.[^.]+)$', 1)))\n",
        "),\n",
        "raw_documents AS (\n",
        "  SELECT\n",
        "    path,\n",
        "    null as raw_parsed,\n",
        "    decode(content, \"utf-8\") as text\n",
        "  FROM \n",
        "    repartitioned_files\n",
        "  WHERE NOT array_contains(array{tuple(PARSE_EXTENSIONS)}, lower(regexp_extract(path, r'(\\\\.[^.]+)$', 1)))\n",
        "),\n",
        "-- Extract page markdowns from ai_parse output\n",
        "sorted_page_contents AS (\n",
        "  SELECT\n",
        "    path,\n",
        "    page:content AS content\n",
        "  FROM\n",
        "    (\n",
        "      SELECT\n",
        "        path,\n",
        "        posexplode(try_cast(parsed:document:pages AS ARRAY<VARIANT>)) AS (page_idx, page)\n",
        "      FROM\n",
        "        parsed_documents\n",
        "      WHERE\n",
        "        parsed:document:pages IS NOT NULL\n",
        "        AND CAST(parsed:error_status AS STRING) IS NULL\n",
        "    )\n",
        "  ORDER BY\n",
        "    page_idx\n",
        "),\n",
        "-- Concatenate so we have 1 row per document\n",
        "concatenated AS (\n",
        "    SELECT\n",
        "        path,\n",
        "        concat_ws('\\\\n\\\\n', collect_list(content)) AS full_content\n",
        "    FROM\n",
        "        sorted_page_contents\n",
        "    GROUP BY\n",
        "        path\n",
        "),\n",
        "-- Bring back the raw parsing since it could be useful for other downstream uses\n",
        "with_raw AS (\n",
        "    SELECT\n",
        "        a.path,\n",
        "        b.parsed as raw_parsed,\n",
        "        a.full_content as text\n",
        "    FROM concatenated a\n",
        "    JOIN parsed_documents b ON a.path = b.path\n",
        ")\n",
        "-- Recombine raw text documents with parsed documents\n",
        "SELECT *  FROM with_raw\n",
        "UNION ALL \n",
        "SELECT * FROM raw_documents\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "print(\"AI Parse query created. Executing...\")\n",
        "spark.sql(ai_parse_query)\n",
        "\n",
        "print(f\"‚úÖ AI Parse processing completed!\")\n",
        "print(f\"üìä Table created: {DESTINATION_TABLE_NAME}\")\n",
        "\n",
        "# Display a sample from the table\n",
        "print(\"\\nüìù Sample data from parsed documents:\")\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        path,\n",
        "        LEFT(text, 200) as text_preview,\n",
        "        LENGTH(text) as text_length\n",
        "    FROM {DESTINATION_TABLE_NAME}\n",
        "    LIMIT 5\n",
        "\"\"\").show(truncate=False)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
